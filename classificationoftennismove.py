# -*- coding: utf-8 -*-
"""Makale.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K68scpKmypTGZlJfNA-cYbIhp5Bkd3j7

# EÄÄ°TÄ°MLERE BAÅLAMADAN Ã–NCE GEREKLÄ° VERÄ° HAZIRLIKLARI
"""

!pip install mediapipe

!pip install opencv-python mediapipe pandas

import cv2
import mediapipe as mp
import os
import pandas as pd
import numpy as np

# MediaPipe Pose setup
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False)

# Video dizinleri
video_paths = [
    "/content/drive/MyDrive/makale tuÌˆm dosyalar/makale pdfleri/dataset-main/VIDEO_RGB"
]

# Ã‡Ä±ktÄ± dosyasÄ±
output_csv_path = "/content/drive/MyDrive/Sskeleton_keypoints.csv"
output_npy_path = "/content/drive/MyDrive/Sskeleton_5d_keypoints.npy"  # 5 boyutlu veriyi kaydedeceÄŸimiz dosya

# Ã‡Ä±ktÄ±lar iÃ§in bir liste
all_keypoints = []

# Her video dizini iÃ§in iÅŸlemler
for path in video_paths:
    for root, dirs, files in os.walk(path):
        for file in files:
            if file.endswith(".avi"):  # Sadece video dosyalarÄ±nÄ± iÅŸler
                video_path = os.path.join(root, file)
                print(f"Processing video: {video_path}")

                cap = cv2.VideoCapture(video_path)

                frame_index = 0
                video_keypoints = []

                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break

                    # YalnÄ±zca her 10. kareyi iÅŸle
                    if frame_index % 10 == 0:
                        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        results = pose.process(rgb_frame)

                        if results.pose_landmarks:
                            keypoints = []
                            for lm in results.pose_landmarks.landmark:
                                keypoints.append((lm.x, lm.y, lm.z))
                            video_keypoints.append({
                                "video_path": video_path,
                                "frame": frame_index,
                                "keypoints": keypoints
                            })

                    frame_index += 1

                cap.release()
                all_keypoints.extend(video_keypoints)

# Veriyi DataFrame'e Ã§evir ve kaydet
df = pd.DataFrame(all_keypoints)
df.to_csv(output_csv_path, index=False)
print(f"Keypoints saved to: {output_csv_path}")

# 5. AdÄ±m: 5 Boyutlu Veriyi HazÄ±rlama

# Veriyi NumPy array'e dÃ¶nÃ¼ÅŸtÃ¼rme
num_keypoints = 33  # MediaPipe'den gelen 33 anahtar nokta
num_coordinates = 3  # x, y, z koordinatlarÄ±
time_steps = 10  # Her 10. kareyi birleÅŸtiriyoruz

# TÃ¼m keypoint'leri numpy array'e dÃ¶nÃ¼ÅŸtÃ¼rme
X = np.array([video['keypoints'] for video in all_keypoints])  # all_keypoints'ten keypoint verisini al

# X array'inin boyutunu kontrol edelim
print("Original X shape:", X.shape)

# X array'inin boyutunu kontrol ettikten sonra zaman adÄ±mlarÄ±nÄ± uyarlayalÄ±m
num_frames = X.shape[0]  # Toplam kare sayÄ±sÄ±

# Zaman adÄ±mlarÄ±nÄ± ve kareleri kontrol et
time_steps = 10  # Ä°stediÄŸiniz zaman adÄ±mÄ± sayÄ±sÄ±nÄ± buradan deÄŸiÅŸtirebilirsiniz
frames_per_time_step = num_frames // time_steps

# EÄŸer zaman adÄ±mlarÄ±na gÃ¶re tam bir bÃ¶lÃ¼nme yoksa, eksik olan kÄ±smÄ± keselim
X = X[:frames_per_time_step * time_steps]  # Zaman adÄ±mlarÄ±nÄ± tam hale getirmek iÃ§in kesiyoruz

# X'i yeniden ÅŸekillendirelim
X_reshaped = X.reshape(-1, time_steps, num_keypoints, num_coordinates)

# 5 boyutlu veriyi kaydediyoruz
np.save(output_npy_path, X_reshaped)
print(f"5D Keypoints saved to: {output_npy_path}")


# 4. AdÄ±m: 5 Boyutlu Veriyi Ä°ÅŸlemek ve Etiketlemek

# CSV dosyasÄ±nÄ± yÃ¼kleyelim
data = pd.read_csv(output_csv_path)

# 1. 'keypoints' sÃ¼tunundaki verilerin iÅŸlenmesi
def expand_keypoints(row):
    try:
        keypoints = ast.literal_eval(row)  # Keypoints sÃ¼tununu bir listeye dÃ¶nÃ¼ÅŸtÃ¼r
        flat_keypoints = [coord for point in keypoints for coord in point]  # x, y, z'yi birleÅŸtir
        return flat_keypoints
    except Exception as e:
        print(f"Hata oluÅŸtu: {e} -> {row}")  # Hata mesajÄ±nÄ± daha ayrÄ±ntÄ±lÄ± ÅŸekilde yazdÄ±ralÄ±m
        return [np.nan] * (num_keypoints * 3)  # Hata durumunda NaN deÄŸerleri dÃ¶ndÃ¼r

# keypoints sÃ¼tununun Ã¶rnek uzunluÄŸunu (keypoint sayÄ±sÄ±nÄ±) bulalÄ±m
num_keypoints = len(ast.literal_eval(str(data['keypoints'].iloc[0])))  # ilk satÄ±rdan Ã¶rnek al

# Anahtar noktalarÄ± sÃ¼tunlara ayÄ±rmak iÃ§in yeni sÃ¼tun isimleri oluÅŸturalÄ±m
keypoint_columns = [f"kp{i}_{axis}" for i in range(num_keypoints) for axis in ["x", "y", "z"]]

# 'keypoints' sÃ¼tununu dÃ¶nÃ¼ÅŸtÃ¼rÃ¼p yeni sÃ¼tunlar ekleyelim
expanded_keypoints = data['keypoints'].apply(expand_keypoints)

# Yeni sÃ¼tunlarÄ± oluÅŸturup, 'keypoints' sÃ¼tununu kaldÄ±rarak veriyi geniÅŸletelim
keypoints_df = pd.DataFrame(expanded_keypoints.tolist(), columns=keypoint_columns)
data_expanded = pd.concat([data.drop(columns=['keypoints']), keypoints_df], axis=1)

# Verinin tamamÄ±nÄ± kaydedelim
output_full_path = "/content/drive/MyDrive/Sskeleton_data_expanded_full.csv"
data_expanded.to_csv(output_full_path, index=False)

print(f"Veri baÅŸarÄ±yla kaydedildi: {output_full_path}")

# 5. AdÄ±m: Etiketleme

def label_data(file_path):
    # CSV dosyasÄ±nÄ± oku
    df = pd.read_csv(file_path)

    # 'video_path' sÃ¼tununda 'backhand2h', 'serflat' ve 'foreflat' anahtar kelimelerine gÃ¶re etiketleme yap
    df['label'] = df['video_path'].apply(lambda x: 0 if 'backhand2h' in x else (1 if 'serflat' in x else (2 if 'foreflat' in x else None)))

    # 'video_path' ve 'frame' sÃ¼tunlarÄ±nÄ± kaldÄ±r
    df.drop(columns=['video_path', 'frame'], inplace=True)

    # Yeni dosyayÄ± kaydet
    df.to_csv('/content/drive/MyDrive/Sskeleton_last_labeled_data.csv', index=False)
    print("Yeni etiketli dosya 'labeled_data.csv' olarak kaydedildi.")

# Dosya yolunu belirterek fonksiyonu Ã§aÄŸÄ±r
label_data('/content/drive/MyDrive/Sskeleton_data_expanded_full.csv')

"""# Veri ArttÄ±rma"""

import pandas as pd

file_path = "/content/drive/MyDrive/Sskeleton_last_labeled_data.csv"
df = pd.read_csv(file_path)

print(df.columns)

df.columns = df.columns.str.strip()  # SÃ¼tun isimlerindeki boÅŸluklarÄ± temizler
print(df.columns)

matching_columns = [col for col in df.columns if 'key' in col.lower()]
print(matching_columns)

import pandas as pd
import numpy as np

# DosyanÄ±n yÃ¼klenmesi
file_path = "/content/drive/MyDrive/Sskeleton_last_labeled_data.csv"
df = pd.read_csv(file_path)

# Veri artÄ±rÄ±mÄ± iÃ§in fonksiyonlar
def add_noise(data, noise_level=0.02):
    """Veriye rastgele gÃ¼rÃ¼ltÃ¼ ekler."""
    noise = np.random.normal(0, noise_level, data.shape)
    return data + noise

def translate(data, shift=5):
    """Veriyi belirli bir miktarda Ã¶teleyerek artÄ±rÄ±r."""
    return data + shift

def flip_horizontal(data):
    """X koordinatlarÄ±nÄ± ters Ã§evirerek yatay eksende aynalama yapar."""
    flipped_data = data.copy()
    flipped_data.iloc[:, ::2] = -flipped_data.iloc[:, ::2]  # X koordinatlarÄ±nÄ± ters Ã§evir
    return flipped_data

# SayÄ±sal veri sÃ¼tunlarÄ±nÄ± al (etiket sÃ¼tunu hariÃ§)
data_columns = df.columns[:-1]
label_column = df.columns[-1]

# Orijinal veriyi kopyala
augmented_data = df.copy()

# ArtÄ±rma sayÄ±sÄ±nÄ± 10 kat yapacak ÅŸekilde ayarla
augmentation_count = 9  # Ä°lk veri orijinal olduÄŸu iÃ§in 9 kez daha artÄ±racaÄŸÄ±z

# Veri artÄ±rÄ±mÄ± iÅŸlemini bir dÃ¶ngÃ¼de yap
for _ in range(augmentation_count):
    # GÃ¼rÃ¼ltÃ¼ ekleme
    noisy_data = df[data_columns].apply(add_noise)
    noisy_data[label_column] = df[label_column]  # Etiketleri koru

    # Ã–teleme
    translated_data = df[data_columns].apply(translate)
    translated_data[label_column] = df[label_column]

    # Aynalama
    flipped_data = flip_horizontal(df[data_columns])
    flipped_data[label_column] = df[label_column]

    # Yeni verileri birleÅŸtir
    augmented_data = pd.concat([augmented_data, noisy_data, translated_data, flipped_data], axis=0).reset_index(drop=True)

# Yeni veri setini kaydet
augmented_file_path = "/content/drive/MyDrive/Sskeleton_augmented_data.csv"
augmented_data.to_csv(augmented_file_path, index=False)

# Veri setinin yeni boyutunu yazdÄ±r
print(f"Veri artÄ±rÄ±mÄ± tamamlandÄ±. Yeni veri setinin satÄ±r sayÄ±sÄ±: {augmented_data.shape[0]}")
print(f"Yeni dosya kaydedildi: {augmented_file_path}")

import pandas as pd

# Orijinal ve artÄ±rÄ±lmÄ±ÅŸ veri setlerinin yollarÄ±
original_file_path = "/content/drive/MyDrive/Sskeleton_last_labeled_data.csv"
augmented_file_path = "/content/drive/MyDrive/Sskeleton_augmented_data.csv"

# Veri setlerini yÃ¼kleyelim
original_df = pd.read_csv(original_file_path)
augmented_df = pd.read_csv(augmented_file_path)

# Etiket sÃ¼tununun adÄ±nÄ± belirleyelim
label_column = original_df.columns[-1]  # Son sÃ¼tunun etiket olduÄŸunu varsayÄ±yoruz

# Orijinal veri setindeki etiket daÄŸÄ±lÄ±mÄ±
original_label_counts = original_df[label_column].value_counts().sort_index()

# ArtÄ±rÄ±lmÄ±ÅŸ veri setindeki etiket daÄŸÄ±lÄ±mÄ±
augmented_label_counts = augmented_df[label_column].value_counts().sort_index()

# Etiketlerin oranlarÄ±nÄ± hesapla
original_ratios = original_label_counts / len(original_df)
augmented_ratios = augmented_label_counts / len(augmented_df)

# SonuÃ§larÄ± yazdÄ±r
print("Orijinal veri setindeki etiket daÄŸÄ±lÄ±mÄ±:")
print(original_label_counts)
print("\nArtÄ±rÄ±lmÄ±ÅŸ veri setindeki etiket daÄŸÄ±lÄ±mÄ±:")
print(augmented_label_counts)

print("\nOrijinal veri setindeki etiket oranlarÄ±:")
print(original_ratios)
print("\nArtÄ±rÄ±lmÄ±ÅŸ veri setindeki etiket oranlarÄ±:")
print(augmented_ratios)

# FarklarÄ± analiz et
difference = augmented_ratios - original_ratios
print("\nEtiket daÄŸÄ±lÄ±m farklarÄ±:")
print(difference)

import pandas as pd


# CSV dosyasÄ±nÄ± yÃ¼kleme
file_path = "/content/drive/MyDrive/Sskeleton_balanced_scaled_data.csv"
df = pd.read_csv(file_path)

#  Veri tiplerini yazdÄ±r
print("ğŸ”¹ Veri Tipleri:")
print(df.dtypes)

#  Eksik veri sayÄ±sÄ±
print("\nğŸ”¹ Eksik Veri SayÄ±sÄ±:")
print(df.isnull().sum())

#  SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± yazdÄ±r (Son sÃ¼tunun 'label' olduÄŸunu varsayarak)
if 'label' in df.columns:
    print("\nğŸ”¹ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:")
    print(df['label'].value_counts())
else:
    print("\n 'label' sÃ¼tunu bulunamadÄ±!")

import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import MinMaxScaler

#  1. DosyanÄ±n YÃ¼klenmesi
file_path = "/content/drive/MyDrive/Sskeleton_augmented_data.csv"
df = pd.read_csv(file_path)

#  2. Ã–zellik ve Etiketleri AyÄ±rma
X = df.iloc[:, :-1].values  # Son sÃ¼tun hariÃ§ tÃ¼m Ã¶zellikler
y = df.iloc[:, -1].values   # Son sÃ¼tun (etiket)

#  3. SMOTE ile Veri Dengeleme
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

#  4. Veriyi Ã–lÃ§eklendirme (Etiketleri HariÃ§ Tut)
scaler = MinMaxScaler()
X_resampled_scaled = scaler.fit_transform(X_resampled)

#  5. DengelenmiÅŸ ve Ã–lÃ§eklendirilmiÅŸ Veriyi DataFrame'e Ã‡evirme
df_balanced = pd.DataFrame(X_resampled_scaled, columns=df.columns[:-1])
df_balanced["label"] = y_resampled  # Etiket sÃ¼tununu ekle

# 6. DengelenmiÅŸ Veriyi Kaydetme
balanced_file_path = "/content/drive/MyDrive/Sskeleton_balanced_data.csv"
df_balanced.to_csv(balanced_file_path, index=False)

print(f" SMOTE ile dengelenmiÅŸ ve Ã¶lÃ§eklendirilmiÅŸ veri baÅŸarÄ±yla kaydedildi: {balanced_file_path}")

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

#  1. Veriyi YÃ¼kleme
file_path = "/content/drive/MyDrive/Sskeleton_balanced_data.csv"
df = pd.read_csv(file_path)

#  2. Ã–zellik ve Etiketleri AyÄ±rma
X = df.iloc[:, :-1].values  # Son sÃ¼tun hariÃ§ tÃ¼m Ã¶zellikler
y = df.iloc[:, -1].values   # Son sÃ¼tun (etiket)

#  3. MinMaxScaler ile Ã–lÃ§eklendirme
scaler = MinMaxScaler(feature_range=(0, 1))
X_scaled = scaler.fit_transform(X)

#  4. Ã–lÃ§eklendirilmiÅŸ Veriyi DataFrame'e Ã‡evirme
df_scaled = pd.DataFrame(X_scaled, columns=df.columns[:-1])
df_scaled["label"] = y  # Etiket sÃ¼tununu ekle

#  5. Ã–lÃ§eklendirilmiÅŸ Veriyi Kaydetme
scaled_file_path = "/content/drive/MyDrive/Sskeleton_balanced_scaled_data.csv"
df_scaled.to_csv(scaled_file_path, index=False)

print(f" Ã–lÃ§eklendirilmiÅŸ veri baÅŸarÄ±yla kaydedildi: {scaled_file_path}")

import numpy as np
import pandas as pd
from google.colab import drive


#  2. Veri Setinin Yolunu TanÄ±mlama
file_path = "/content/drive/MyDrive/Sskeleton_balanced_scaled_data.csv"

#  3. CSV DosyasÄ±nÄ± Okuma
df = pd.read_csv(file_path)

#  4. Pencere Boyutu ve Ã–zellik SayÄ±sÄ±
window_size = 33
num_joints = 33  # 33 eklem noktasÄ±
num_features = num_joints * 3  # Her eklem noktasÄ± iÃ§in (x, y, z)

#  5. Veriyi NumPy Dizisine Ã‡evirme
data = df.iloc[:, :-1].values  # Son sÃ¼tun label, onu Ã§Ä±kardÄ±k
labels = df["label"].values  # Hareket etiketleri

#  6. Pencereleme (33 karelik pencere)
X_sequences = []
y_sequences = []

for i in range(len(data) - window_size):
    X_sequences.append(data[i : i + window_size])  # 33 karelik pencere
    y_sequences.append(labels[i + window_size - 1])  # Son kare etiketi

#  7. NumPy Dizilerine Ã‡evirme
X_sequences = np.array(X_sequences)  # (num_samples, 33, 99)
y_sequences = np.array(y_sequences)  # (num_samples,)

#  8. 3D CNN iÃ§in Yeniden Åekillendirme (num_samples, 33, 33, 3)
X_3D_CNN = X_sequences.reshape(-1, window_size, num_joints, 3)

#  9. Veriyi `.csv` formatÄ±nda kaydetme
df_reshaped = X_3D_CNN.reshape(X_3D_CNN.shape[0], -1)  # 2D hale getir
df_final = pd.DataFrame(df_reshaped)
df_final["label"] = y_sequences  # Etiketleri ekle

# Google Drive iÃ§ine kaydetme
csv_output_path = "/content/drive/MyDrive/Sskeleton_balanced_scaled_data_33frames.csv"
df_final.to_csv(csv_output_path, index=False)
print(f" CSV dosyasÄ± kaydedildi: {csv_output_path}")

#  10. Veriyi `.npy` formatÄ±nda kaydetme
npy_output_X = "/content/drive/MyDrive/X_3D_CNN.npy"
npy_output_y = "/content/drive/MyDrive/y_labels.npy"

np.save(npy_output_X, X_3D_CNN)
np.save(npy_output_y, y_sequences)

print(f" NPY dosyalarÄ± kaydedildi:\n{npy_output_X}\n{npy_output_y}")

#  11. Ä°ÅŸlemin DoÄŸru YapÄ±ldÄ±ÄŸÄ±nÄ± Kontrol Etme

# 1. Pencereleme ve etiketlerin eÅŸleÅŸip eÅŸleÅŸmediÄŸini kontrol etme
print(f"Pencere sayÄ±sÄ±: {len(X_sequences)}")
print(f"Etiket sayÄ±sÄ±: {len(y_sequences)}")
assert len(X_sequences) == len(y_sequences), "Pencere sayÄ±sÄ± ile etiket sayÄ±sÄ± eÅŸleÅŸmiyor!"

# 2. 3D CNN iÃ§in verinin doÄŸru ÅŸekilde ÅŸekillendirildiÄŸini kontrol etme
print(f"X_3D_CNN ÅŸekli: {X_3D_CNN.shape}")
assert X_3D_CNN.shape[1] == window_size, f"Pencere boyutu yanlÄ±ÅŸ, beklenen: {window_size}, mevcut: {X_3D_CNN.shape[1]}"
assert X_3D_CNN.shape[2] == num_joints, f"Eklem sayÄ±sÄ± yanlÄ±ÅŸ, beklenen: {num_joints}, mevcut: {X_3D_CNN.shape[2]}"
assert X_3D_CNN.shape[3] == 3, "Koordinat sayÄ±sÄ± (x, y, z) yanlÄ±ÅŸ, beklenen: 3, mevcut: {X_3D_CNN.shape[3]}"

# 3. Verinin belirli bir Ã¶rneÄŸini kontrol etme
print(f"Ã–rnek bir pencere (ilk pencere):\n{X_3D_CNN[0]}")  # Ä°lk pencereyi yazdÄ±r
print(f"Ä°lk etiket: {y_sequences[0]}")

# 4. CSV dosyasÄ±nÄ±n doÄŸru ÅŸekilde kaydedildiÄŸini kontrol etme
loaded_csv = pd.read_csv(csv_output_path)
print(f"CSV dosyasÄ±nÄ±n ilk satÄ±rlarÄ±:\n{loaded_csv.head()}")

# 5. NPY dosyasÄ±nÄ±n yÃ¼klenip doÄŸru kaydedildiÄŸini kontrol etme
loaded_X = np.load(npy_output_X)
loaded_y = np.load(npy_output_y)
print(f"YÃ¼klenen X_3D_CNN ÅŸekli: {loaded_X.shape}")
print(f"YÃ¼klenen etiket sayÄ±sÄ±: {loaded_y.shape[0]}")

import numpy as np
import pandas as pd
from collections import Counter

# Etiketleri yÃ¼kle
npy_output_y = "/content/drive/MyDrive/y_labels.npy"
y_sequences = np.load(npy_output_y)

# SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± yazdÄ±r
label_counts = Counter(y_sequences)
print(" SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:", label_counts)

import numpy as np

# Veriyi yÃ¼kleme
X = np.load("/content/drive/MyDrive/X_3D_CNN_reshaped.npy")
y = np.load("/content/drive/MyDrive/y_labels_reshaped.npy")

# Verinin ÅŸekline bakalÄ±m
print(f"X shape: {X.shape}")  # (num_samples, 33, 33, 3)
print(f"y shape: {y.shape}")  # (num_samples,)

import numpy as np

# Veriyi yÃ¼kleme
X = np.load("/content/drive/MyDrive/X_3D_CNN.npy")

# Mevcut boyutu al
current_shape = X.shape
expected_dims = ["batch_size", "time_steps", "height", "width", "channels"]

# Beklenen 5 boyuttan hangisinin eksik olduÄŸunu kontrol et
if len(current_shape) == 4:
    print(f"Mevcut veri boyutu: {current_shape} (4D)")
    print("Eksik boyut: 'time_steps' (Zaman adÄ±mÄ±)")

elif len(current_shape) == 5:
    print(f"Mevcut veri boyutu: {current_shape} (5D)")
    print("Veri zaten 3D CNN iÃ§in uygun!")

else:
    print(f"Beklenmeyen veri boyutu: {current_shape}!")

import numpy as np

#  Veriyi yÃ¼kle
X = np.load("/content/drive/MyDrive/X_3D_CNN.npy")
y = np.load("/content/drive/MyDrive/y_labels.npy")

#  Time steps deÄŸeri
time_steps = 10  # Her 10 kare bir zaman dizisi olarak gruplandÄ±rÄ±lacak

#  NumPy dizisinin boyutunu kontrol et
num_samples = X.shape[0]  # 16935
height, width, channels = X.shape[1], X.shape[2], X.shape[3]  # (33, 33, 3)

#  Time steps ile tam bÃ¶lÃ¼nÃ¼p bÃ¶lÃ¼nmediÄŸini kontrol et
if num_samples % time_steps != 0:
    new_num_samples = (num_samples // time_steps) * time_steps
    X = X[:new_num_samples]  # Fazla olan kareleri at
    y = y[:new_num_samples]  # Etiketleri de aynÄ± ÅŸekilde dÃ¼zenle

#  5D forma dÃ¶nÃ¼ÅŸtÃ¼rme (batch_size, time_steps, height, width, channels)
X_reshaped = X.reshape(-1, time_steps, height, width, channels)
y_reshaped = y.reshape(-1, time_steps)  # Etiketleri de zaman serisi halinde dÃ¼zenle

#  DÃ¼zenlenmiÅŸ verilerin boyutunu yazdÄ±r
print(f"Yeni X shape: {X_reshaped.shape}")  # (num_samples/time_steps, time_steps, 33, 33, 3)
print(f"Yeni y shape: {y_reshaped.shape}")  # (num_samples/time_steps, time_steps)

#  Verileri kaydet
np.save("/content/drive/MyDrive/X_3D_CNN_reshaped.npy", X_reshaped)
np.save("/content/drive/MyDrive/y_labels_reshaped.npy", y_reshaped)

print(" DÃ¼zenlenmiÅŸ veriler baÅŸarÄ±yla kaydedildi!")

"""# Hibrit model eÄŸitimi"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Input, Conv3D, MaxPooling3D, BatchNormalization,
                                     Flatten, LSTM, Dense, Dropout, Reshape)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import KFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, confusion_matrix)
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# =============================================================================
# 1. VERÄ°LERÄ°N YÃœKLENMESÄ° VE HAZIRLANMASI
# =============================================================================

# X verisini yÃ¼kleyin: Beklenen boyut: (num_groups, time_steps, height, width, channels)
X = np.load("/content/drive/MyDrive/X_3D_CNN_reshaped.npy")
# y verisini yÃ¼kleyin: Ä°lk hali (num_groups, time_steps)
y = np.load("/content/drive/MyDrive/y_labels_reshaped.npy")

# Orijinal veri boyutlarÄ±nÄ± yazdÄ±r
print("Orijinal X boyutlarÄ±:", X.shape)  # Ã–rnek Ã§Ä±ktÄ±: (1693, 10, 33, 33, 3)
print("Orijinal y boyutlarÄ±:", y.shape)  # Ã–rnek Ã§Ä±ktÄ±: (1693, 10)

# EÄŸer y verisi her sekans iÃ§in 10 etiket iÃ§eriyorsa, her sekansÄ±n
# son etiketini alarak y verisini (num_groups,) boyutuna indiriyoruz.
if y.ndim == 2 and y.shape[1] > 1:
    y = y[:, -1]
print("DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ y boyutlarÄ±:", y.shape)  # Beklenen Ã§Ä±ktÄ±: (1693,)

# =============================================================================
# 2. 3D MODEL MÄ°MARÄ°SÄ°NÄ°N OLUÅTURULMASI
# =============================================================================

def create_3d_model(input_shape, num_classes):
    """
    Bu fonksiyon, verilen input_shape (time_steps, height, width, channels)
    ve num_classes (Ã¶rneÄŸin, 3) parametrelerine gÃ¶re 3D bir model oluÅŸturur.
    Model, her zaman adÄ±mÄ±ndaki (frame) gÃ¶rÃ¼ntÃ¼den 3D Ã¶zellik Ã§Ä±karÄ±p,
    sonrasÄ±nda LSTM ile zamansal iliÅŸkileri modelleyerek her sekans iÃ§in
    tek bir tahmin (output shape: (None, num_classes)) Ã¼retir.
    """
    model = Sequential()

    # Ä°lk katman olarak Input eklenir
    model.add(Input(shape=input_shape))

    # 3D konvolÃ¼syon iÅŸlemleri yapÄ±lÄ±r.
    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    # 5D Ã§Ä±ktÄ±yÄ± 3D'ye dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in Reshape ekleniyor.
    model.add(Reshape((-1, 128)))  # Bu katman, veriyi 3D hale getirir.

    # LSTM katmanÄ±
    model.add(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))

    # Ã‡Ä±kÄ±ÅŸ katmanÄ±: softmax aktivasyon ile num_classes tahmini yapar.
    model.add(Dense(num_classes, activation='softmax'))

    # TamsayÄ± etiketler kullandÄ±ÄŸÄ±mÄ±z iÃ§in sparse_categorical_crossentropy kullanÄ±lÄ±r.
    model.compile(optimizer=Adam(learning_rate=0.00001),  # Ã–ÄŸrenme oranÄ±nÄ± dÃ¼ÅŸÃ¼rdÃ¼k
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# =============================================================================
# 3. K-FOLD CROSS VALIDATION VE MODEL EÄÄ°TÄ°MÄ°
# =============================================================================

kf = KFold(n_splits=10, shuffle=True, random_state=42)

metrics = {
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': [],
    'auc_roc': [],
}

y_true_all, y_pred_all = [], []

fold_no = 1
for train_index, val_index in kf.split(X):
    print(f'\nFold {fold_no} iÅŸleniyor...')

    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    class_weights = {0: 1.0, 1: 1.0, 2: 1.5}

    # Model her fold iÃ§in yeniden oluÅŸturulur.
    model = create_3d_model(input_shape=X_train.shape[1:], num_classes=3)

    # EarlyStopping ve ModelCheckpoint kullanÄ±mÄ±
    #early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/fold_1_best_model.keras',
                                   monitor='val_loss',
                                   save_best_only=True,
                                   verbose=1)


    # Modeli eÄŸitiyoruz.
    history = model.fit(
        X_train, y_train,
        epochs=15,
        batch_size=32,
        validation_data=(X_val, y_val),
        class_weight=class_weights,
        callbacks=[ model_checkpoint],
        verbose=1
    )

    # DoÄŸrulama seti Ã¼zerinde tahmin yapÄ±yoruz.
    y_pred = model.predict(X_val)
    y_pred_classes = np.argmax(y_pred, axis=1)

    auc = None
    try:
        y_val_onehot = np.eye(3)[y_val]
        if len(np.unique(y_val)) > 1:
            auc = roc_auc_score(y_val_onehot, y_pred, multi_class='ovr', average='macro')
        else:
            print("UyarÄ±: YalnÄ±zca bir sÄ±nÄ±f bulundu. AUC hesaplanamaz.")
    except ValueError as e:
        print(f"UyarÄ±: AUC hesaplanamadÄ±: {e}")

    prec = precision_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    rec = recall_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)

    metrics['accuracy'].append(accuracy_score(y_val, y_pred_classes))
    metrics['precision'].append(prec)
    metrics['recall'].append(rec)
    metrics['f1'].append(f1)
    if auc is not None:
        metrics['auc_roc'].append(auc)

    y_true_all.extend(y_val)
    y_pred_all.extend(y_pred_classes)

    print(f"Fold {fold_no} - Accuracy: {metrics['accuracy'][-1]:.4f}, "
          f"Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}, "
          f"AUC-ROC: {auc if auc is not None else 'NaN'}")

    fold_no += 1

# =============================================================================
# 4. ORTALAMA METRÄ°KLERÄ°N HESAPLANMASI VE SONUÃ‡LARIN KAYDEDÄ°LMESÄ°
# =============================================================================

avg_metrics = {metric: np.mean(values) for metric, values in metrics.items()}
print("\nOrtalama SonuÃ§lar:")
for metric, value in avg_metrics.items():
    print(f"{metric}: {value:.4f}")

df_results = pd.DataFrame(metrics)
df_results.to_csv("/content/drive/MyDrive/NO_3D_CNN_LSTM_kfold_results.csv", index=False)

# =============================================================================
# 5. KARMAÅIKLIK MATRÄ°SÄ°NÄ°N HESAPLANMASI VE GÃ–RSELLEÅTÄ°RÄ°LMESÄ°
# =============================================================================

conf_matrix = confusion_matrix(y_true_all, y_pred_all)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Backhand", "Forehand", "Serve"],
            yticklabels=["Backhand", "Forehand", "Serve"])
plt.title("KarmaÅŸÄ±klÄ±k Matrisi")
plt.xlabel("Tahmin Edilen SÄ±nÄ±f")
plt.ylabel("GerÃ§ek SÄ±nÄ±f")
plt.show()

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Input, Conv3D, MaxPooling3D, BatchNormalization,
                                     Flatten, LSTM, Dense, Dropout, Reshape)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import KFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, confusion_matrix)
from sklearn.decomposition import PCA
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# =============================================================================
# 1. VERÄ°LERÄ°N YÃœKLENMESÄ° VE HAZIRLANMASI
# =============================================================================

# X verisini yÃ¼kleyin: Beklenen boyut: (num_groups, time_steps, height, width, channels)
X = np.load("/content/drive/MyDrive/X_3D_CNN_reshaped.npy")
# y verisini yÃ¼kleyin: Ä°lk hali (num_groups, time_steps)
y = np.load("/content/drive/MyDrive/y_labels_reshaped.npy")

# Orijinal veri boyutlarÄ±nÄ± yazdÄ±r
print("Orijinal X boyutlarÄ±:", X.shape)  # Ã–rnek Ã§Ä±ktÄ±: (1693, 10, 33, 33, 3)
print("Orijinal y boyutlarÄ±:", y.shape)  # Ã–rnek Ã§Ä±ktÄ±: (1693, 10)

# EÄŸer y verisi her sekans iÃ§in 10 etiket iÃ§eriyorsa, her sekansÄ±n
# son etiketini alarak y verisini (num_groups,) boyutuna indiriyoruz.
if y.ndim == 2 and y.shape[1] > 1:
    y = y[:, -1]
print("DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ y boyutlarÄ±:", y.shape)  # Beklenen Ã§Ä±ktÄ±: (1693,)

# =============================================================================
# 2. MODEL MÄ°MARÄ°SÄ°NÄ°N OLUÅTURULMASI
# =============================================================================

def create_3d_model(input_shape, num_classes):
    """
    Bu fonksiyon, verilen input_shape (time_steps, height, width, channels)
    ve num_classes (Ã¶rneÄŸin, 3) parametrelerine gÃ¶re 3D CNN - LSTM hibrit modeli oluÅŸturur.
    """
    model = Sequential()
    model.add(Input(shape=input_shape))

    # 3D konvolÃ¼syon iÅŸlemleri
    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    # 5D Ã§Ä±ktÄ±yÄ± 3D'ye dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in Reshape katmanÄ±.
    model.add(Reshape((-1, 128)))

    # LSTM katmanÄ±
    model.add(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))

    # Ã‡Ä±kÄ±ÅŸ katmanÄ±: softmax aktivasyon ile num_classes tahmini yapar.
    model.add(Dense(num_classes, activation='softmax'))

    # TamsayÄ± etiketler kullandÄ±ÄŸÄ±mÄ±z iÃ§in sparse_categorical_crossentropy kullanÄ±lÄ±r.
    model.compile(optimizer=Adam(learning_rate=1e-5),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# =============================================================================
# 3. K-FOLD CROSS VALIDATION, PCA Ä°LE Ã–Z NÄ°TELÄ°K SEÃ‡Ä°MÄ° VE MODEL EÄÄ°TÄ°MÄ°
# =============================================================================

kf = KFold(n_splits=10, shuffle=True, random_state=42)

metrics = {
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': [],
    'auc_roc': [],
}

y_true_all, y_pred_all = [], []

# PCA parametreleri:
# Her frame orijinal boyutu: (33, 33, 3) => 33*33*3 = 3267
# PCA sonrasÄ±nda n_components kadar Ã¶zellik seÃ§iyoruz, Ã¶rneÄŸin 64.
# Bu Ã¶rnekte 64 bileÅŸeni kare (8x8) formatÄ±nda geri yerleÅŸtireceÄŸiz.
n_components = 128
pca_target_shape = (8, 16, 1)  # 8 * 16 * 1 = 128


fold_no = 1
for train_index, val_index in kf.split(X):
    print(f'\nFold {fold_no} iÅŸleniyor...')

    # BÃ¶lÃ¼nmÃ¼ÅŸ veriler
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # --- PCA Ä°le Ã–Z NÄ°TELÄ°K SEÃ‡Ä°MÄ° ---
    # Her frame'i flatten edip PCAâ€™ya sokacaÄŸÄ±z.
    num_train, time_steps, h, w, c = X_train.shape
    num_val = X_val.shape[0]

    # EÄŸitim verisini (num_train*time_steps, h*w*c) ÅŸeklinde yeniden ÅŸekillendir.
    X_train_reshaped = X_train.reshape(-1, h*w*c)
    # PCAâ€™yÄ± eÄŸitim verisi Ã¼zerinde fit ediyoruz.
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train_reshaped)
    # PCA Ã§Ä±ktÄ±sÄ±nÄ± (num_train, time_steps, pca_target_shape) ÅŸeklinde dÃ¼zenliyoruz.
    X_train_pca = X_train_pca.reshape(num_train, time_steps, *pca_target_shape)

    # DoÄŸrulama verisi iÃ§in de aynÄ± dÃ¶nÃ¼ÅŸÃ¼mÃ¼ PCA parametreleri kullanarak uyguluyoruz.
    X_val_reshaped = X_val.reshape(-1, h*w*c)
    X_val_pca = pca.transform(X_val_reshaped)
    X_val_pca = X_val_pca.reshape(num_val, time_steps, *pca_target_shape)

    # Modelin giriÅŸ boyutu artÄ±k (time_steps, 8, 8, 1) ÅŸeklindedir.
    input_shape = X_train_pca.shape[1:]
    num_classes = 3  # SÄ±nÄ±f sayÄ±sÄ±
    model = create_3d_model(input_shape=input_shape, num_classes=num_classes)

    # AÄŸÄ±rlÄ±klandÄ±rma (Ã¶rnekte sÄ±nÄ±f 2 iÃ§in hafif daha yÃ¼ksek aÄŸÄ±rlÄ±k)
    class_weights = {0: 1.0, 1: 1.0, 2: 1.5}

    # Erken durdurma ve model kaydetme callback'leri (ModelCheckpoint Ã¶rneÄŸinde fold numarasÄ±nÄ± kullanabilirsiniz)
    # early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    model_checkpoint = ModelCheckpoint(f'/content/drive/MyDrive/fold_{fold_no}_best_model.keras',
                                         monitor='val_loss',
                                         save_best_only=True,
                                         verbose=1)

    # Modeli eÄŸitiyoruz.
    history = model.fit(
        X_train_pca, y_train,
        epochs=15,
        batch_size=32,
        validation_data=(X_val_pca, y_val),
        class_weight=class_weights,
        callbacks=[model_checkpoint],
        verbose=1
    )

    # DoÄŸrulama seti Ã¼zerinde tahmin yapÄ±yoruz.
    y_pred = model.predict(X_val_pca)
    y_pred_classes = np.argmax(y_pred, axis=1)

    # AUC-ROC hesaplama (sadece birden fazla sÄ±nÄ±f varsa)
    auc = None
    try:
        y_val_onehot = np.eye(num_classes)[y_val]
        if len(np.unique(y_val)) > 1:
            auc = roc_auc_score(y_val_onehot, y_pred, multi_class='ovr', average='macro')
        else:
            print("UyarÄ±: YalnÄ±zca bir sÄ±nÄ±f bulundu. AUC hesaplanamaz.")
    except ValueError as e:
        print(f"UyarÄ±: AUC hesaplanamadÄ±: {e}")

    prec = precision_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    rec = recall_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)

    metrics['accuracy'].append(accuracy_score(y_val, y_pred_classes))
    metrics['precision'].append(prec)
    metrics['recall'].append(rec)
    metrics['f1'].append(f1)
    if auc is not None:
        metrics['auc_roc'].append(auc)

    y_true_all.extend(y_val)
    y_pred_all.extend(y_pred_classes)

    print(f"Fold {fold_no} - Accuracy: {metrics['accuracy'][-1]:.4f}, "
          f"Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}, "
          f"AUC-ROC: {auc if auc is not None else 'NaN'}")

    fold_no += 1

# =============================================================================
# 4. ORTALAMA METRÄ°KLERÄ°N HESAPLANMASI VE SONUÃ‡LARIN KAYDEDÄ°LMESÄ°
# =============================================================================

avg_metrics = {metric: np.mean(values) for metric, values in metrics.items()}
print("\nOrtalama SonuÃ§lar:")
for metric, value in avg_metrics.items():
    print(f"{metric}: {value:.4f}")

df_results = pd.DataFrame(metrics)
df_results.to_csv("/content/drive/MyDrive/PCA_3D_CNN_LSTM_kfold_results.csv", index=False)

# =============================================================================
# 5. KARMAÅIKLIK MATRÄ°SÄ°NÄ°N HESAPLANMASI VE GÃ–RSELLEÅTÄ°RÄ°LMESÄ°
# =============================================================================

conf_matrix = confusion_matrix(y_true_all, y_pred_all)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Backhand", "Forehand", "Serve"],
            yticklabels=["Backhand", "Forehand", "Serve"])
plt.title("KarmaÅŸÄ±klÄ±k Matrisi")
plt.xlabel("Tahmin Edilen SÄ±nÄ±f")
plt.ylabel("GerÃ§ek SÄ±nÄ±f")
plt.show()

!pip uninstall -y pandas
!pip install --no-cache-dir --upgrade pandas numpy tensorflow

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import KFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, confusion_matrix)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# =============================================================================
# 1. VERÄ°LERÄ°N YÃœKLENMESÄ° VE HAZIRLANMASI
# =============================================================================

# X verisini yÃ¼kleyin: Beklenen boyut: (num_groups, time_steps, height, width, channels)
X = np.load("/content/drive/MyDrive/X_3D_CNN_reshaped.npy")
# y verisini yÃ¼kleyin: Ä°lk hali (num_groups, time_steps)
y = np.load("/content/drive/MyDrive/y_labels_reshaped.npy")

# Orijinal veri boyutlarÄ±nÄ± yazdÄ±r
print("Orijinal X boyutlarÄ±:", X.shape)  # Ã–rnek Ã§Ä±ktÄ±: (1693, 10, 33, 33, 3)
print("Orijinal y boyutlarÄ±:", y.shape)  # Ã–rnek Ã§Ä±ktÄ±: (1693, 10)

# EÄŸer y verisi her sekans iÃ§in 10 etiket iÃ§eriyorsa, her sekansÄ±n son etiketini alÄ±yoruz.
if y.ndim == 2 and y.shape[1] > 1:
    y = y[:, -1]
print("DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ y boyutlarÄ±:", y.shape)  # Beklenen Ã§Ä±ktÄ±: (1693,)

# (Ä°steÄŸe baÄŸlÄ±) Gerekliyse verileri normalize edebilirsiniz.
# Ã–rneÄŸin, eÄŸer gÃ¶rÃ¼ntÃ¼ deÄŸerleriniz 0-255 arasÄ±ndaysa:
X = X.astype('float32') / 255.0

# =============================================================================
# 2. MODEL MÄ°MARÄ°SÄ°NÄ°N OLUÅTURULMASI (LSTM tabanlÄ± model)
# =============================================================================
def create_lda_model(input_shape, num_classes):
    """
    Bu fonksiyon, LDA ile indirgenmiÅŸ Ã¶zellikleri (time_steps, n_features) giriÅŸ alan,
    LSTM kullanarak sÄ±nÄ±flandÄ±rma yapan basit bir modeli oluÅŸturur.
    """
    model = Sequential()
    model.add(Input(shape=input_shape))

    # LSTM katmanÄ±: time_steps boyunca iliÅŸkileri Ã¶ÄŸrenir.
    model.add(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))

    # Ã‡Ä±kÄ±ÅŸ katmanÄ±: softmax ile sÄ±nÄ±f tahmini.
    model.add(Dense(num_classes, activation='softmax'))

    # Derleme: TamsayÄ± etiketler iÃ§in sparse_categorical_crossentropy kullanÄ±lÄ±yor.
    model.compile(optimizer=Adam(learning_rate=1e-5),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# =============================================================================
# 3. K-FOLD CROSS VALIDATION, LDA Ä°LE Ã–Z NÄ°TELÄ°K SEÃ‡Ä°MÄ° VE MODEL EÄÄ°TÄ°MÄ°
# =============================================================================

kf = KFold(n_splits=10, shuffle=True, random_state=42)

metrics = {
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': [],
    'auc_roc': [],
}

y_true_all, y_pred_all = [], []

# LDA iÃ§in ayarlar:
# Orijinal her frame boyutu: (33, 33, 3) => 33*33*3 = 3267 Ã¶zellik
# LDA maksimum (n_classes - 1) bileÅŸen Ã¼retebilir. 3 sÄ±nÄ±f iÃ§in n_components = 2.
n_components = 2

fold_no = 1
for train_index, val_index in kf.split(X):
    print(f'\nFold {fold_no} iÅŸleniyor...')

    # BÃ¶lÃ¼nmÃ¼ÅŸ veriler
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # --- LDA ile Ã–z Nitelik SeÃ§imi ---
    # X_train: (num_train, time_steps, h, w, c)
    num_train, time_steps, h, w, c = X_train.shape
    num_val = X_val.shape[0]

    # Her frame'i flatten ediyoruz: (num_train*time_steps, h*w*c)
    X_train_reshaped = X_train.reshape(-1, h * w * c)
    # Her frame'in etiketi, video etiketi ile aynÄ± kabul ediliyor:
    y_train_repeated = np.repeat(y_train, time_steps)

    # LDA'yÄ± eÄŸitim verisi Ã¼zerinde fit ediyoruz.
    lda = LDA(n_components=n_components)
    X_train_lda = lda.fit_transform(X_train_reshaped, y_train_repeated)

    # LDA Ã§Ä±ktÄ±sÄ±: (num_train*time_steps, n_components)
    # Her video iÃ§in tekrar ÅŸekillendiriyoruz: (num_train, time_steps, n_components)
    X_train_lda = X_train_lda.reshape(num_train, time_steps, n_components)

    # DoÄŸrulama verisi iÃ§in aynÄ± dÃ¶nÃ¼ÅŸÃ¼mÃ¼ uyguluyoruz.
    X_val_reshaped = X_val.reshape(-1, h * w * c)
    X_val_lda = lda.transform(X_val_reshaped)
    X_val_lda = X_val_lda.reshape(num_val, time_steps, n_components)

    # Modelin giriÅŸ boyutu artÄ±k (time_steps, n_components) yani (10, 2) olacaktÄ±r.
    input_shape = X_train_lda.shape[1:]
    num_classes = 3  # SÄ±nÄ±f sayÄ±sÄ±
    model = create_lda_model(input_shape=input_shape, num_classes=num_classes)

    # SÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ± (Ã¶rnekte, sÄ±nÄ±f 2 iÃ§in hafif daha yÃ¼ksek aÄŸÄ±rlÄ±k veriliyor)
    class_weights = {0: 1.0, 1: 1.0, 2: 1.5}

    # Callback: Erken durdurma veya model kaydetme isteÄŸe baÄŸlÄ± eklenebilir.
    model_checkpoint = ModelCheckpoint(f'/content/drive/MyDrive/fold_{fold_no}_best_model.keras',
                                         monitor='val_loss',
                                         save_best_only=True,
                                         verbose=1)

    # Modeli eÄŸitiyoruz.
    history = model.fit(
        X_train_lda, y_train,
        epochs=15,
        batch_size=32,
        validation_data=(X_val_lda, y_val),
        class_weight=class_weights,
        callbacks=[model_checkpoint],
        verbose=1
    )

    # DoÄŸrulama seti Ã¼zerinde tahmin yapÄ±yoruz.
    y_pred = model.predict(X_val_lda)
    y_pred_classes = np.argmax(y_pred, axis=1)

    # AUC-ROC hesaplama (birden fazla sÄ±nÄ±f varsa)
    auc = None
    try:
        y_val_onehot = np.eye(num_classes)[y_val]
        if len(np.unique(y_val)) > 1:
            auc = roc_auc_score(y_val_onehot, y_pred, multi_class='ovr', average='macro')
        else:
            print("UyarÄ±: YalnÄ±zca bir sÄ±nÄ±f bulundu. AUC hesaplanamaz.")
    except ValueError as e:
        print(f"UyarÄ±: AUC hesaplanamadÄ±: {e}")

    prec = precision_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    rec = recall_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)

    metrics['accuracy'].append(accuracy_score(y_val, y_pred_classes))
    metrics['precision'].append(prec)
    metrics['recall'].append(rec)
    metrics['f1'].append(f1)
    if auc is not None:
        metrics['auc_roc'].append(auc)

    y_true_all.extend(y_val)
    y_pred_all.extend(y_pred_classes)

    print(f"Fold {fold_no} - Accuracy: {metrics['accuracy'][-1]:.4f}, "
          f"Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}, "
          f"AUC-ROC: {auc if auc is not None else 'NaN'}")

    fold_no += 1

# =============================================================================
# 4. ORTALAMA METRÄ°KLERÄ°N HESAPLANMASI VE SONUÃ‡LARIN KAYDEDÄ°LMESÄ°
# =============================================================================

avg_metrics = {metric: np.mean(values) for metric, values in metrics.items()}
print("\nOrtalama SonuÃ§lar:")
for metric, value in avg_metrics.items():
    print(f"{metric}: {value:.4f}")

df_results = pd.DataFrame(metrics)
df_results.to_csv("/content/drive/MyDrive/LDA_3D_CNN_LSTM_kfold_results.csv", index=False)

# =============================================================================
# 5. KARMAÅIKLIK MATRÄ°SÄ°NÄ°N HESAPLANMASI VE GÃ–RSELLEÅTÄ°RÄ°LMESÄ°
# =============================================================================

conf_matrix = confusion_matrix(y_true_all, y_pred_all)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Backhand", "Forehand", "Serve"],
            yticklabels=["Backhand", "Forehand", "Serve"])
plt.title("KarmaÅŸÄ±klÄ±k Matrisi")
plt.xlabel("Tahmin Edilen SÄ±nÄ±f")
plt.ylabel("GerÃ§ek SÄ±nÄ±f")
plt.show()

"""# MAKÄ°NE Ã–ÄRENÄ°MÄ°

"""

import numpy as np
import pandas as pd

# Makine Ã¶ÄŸrenmesi modÃ¼lleri
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

# Multioutput iÃ§in
from sklearn.multioutput import MultiOutputClassifier

# Metrik modÃ¼lleri
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score

# Model kopyalamak iÃ§in
from sklearn.base import clone

# -----------------------------
# Veri YÃ¼kleme ve HazÄ±rlama
# -----------------------------
x_path = "/content/drive/MyDrive/X_3D_CNN_reshaped.npy"
y_path = "/content/drive/MyDrive/y_labels_reshaped.npy"

# Verileri yÃ¼kleyelim
X = np.load(x_path)
y = np.load(y_path)

# EÄŸer veriler 3 boyutlu ise ve modellerin kabul edeceÄŸi 2B formata getirmek gerekiyorsa:
X = X.reshape(X.shape[0], -1)

# -----------------------------
# Modellerin TanÄ±mlanmasÄ±
# -----------------------------
models = {
    "SVM": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "GBM": GradientBoostingClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Uygulanacak dÃ¶nÃ¼ÅŸÃ¼m stratejileri:
# "None": hiÃ§bir boyut indirgeme uygulanmaz,
# "PCA": PCA ile, verinin %95 varyansÄ± korunarak boyut indirgeme,
# "LDA": LDA ile, etiket bilgisini kullanarak boyut indirgeme
transformations = {
    "None": None,
    "PCA": PCA(n_components=0.95, random_state=42),
    "LDA": LDA()  # LDA, sÄ±nÄ±f sayÄ±sÄ±na baÄŸlÄ± olarak n_components belirler.
}

# -----------------------------
# YardÄ±mcÄ± Fonksiyonlar
# -----------------------------
def compute_confusion_metrics(y_true, y_pred):
    """
    Ä°kili sÄ±nÄ±flandÄ±rmada confusion matrix'den sensitivity (TPR) ve specificity (TNR) hesaplar.
    VarsayÄ±m: y_true ve y_pred binary deÄŸerler iÃ§eriyor.
    """
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    return sensitivity, specificity

def evaluate_metrics(y_true, y_pred, y_prob=None):
    """
    EÄŸer y_true tek boyutlu ise doÄŸrudan metrikleri hesaplar.
    Ã‡oklu Ã§Ä±ktÄ± (multioutput) durumunda, her kolon iÃ§in metrik hesaplanÄ±p ortalamasÄ± alÄ±nÄ±r.
    """
    # EÄŸer tek Ã§Ä±ktÄ±ysa:
    if y_true.ndim == 1:
        acc = accuracy_score(y_true, y_pred)
        sensitivity, specificity = compute_confusion_metrics(y_true, y_pred)
        f_measure = f1_score(y_true, y_pred)
        auc = None
        if y_prob is not None:
            try:
                auc = roc_auc_score(y_true, y_prob)
            except Exception:
                auc = None
        return acc, sensitivity, specificity, f_measure, auc
    else:
        n_outputs = y_true.shape[1]
        acc_list, sens_list, spec_list, f1_list, auc_list = [], [], [], [], []
        for i in range(n_outputs):
            acc_list.append(accuracy_score(y_true[:, i], y_pred[:, i]))
            sens, spec = compute_confusion_metrics(y_true[:, i], y_pred[:, i])
            sens_list.append(sens)
            spec_list.append(spec)
            f1_list.append(f1_score(y_true[:, i], y_pred[:, i]))
            # AUC ROC: yalnÄ±zca y_prob saÄŸlanÄ±rsa hesaplanÄ±r.
            if y_prob is not None:
                try:
                    auc_i = roc_auc_score(y_true[:, i], y_prob[i][:, 1])
                except Exception:
                    auc_i = None
                if auc_i is not None:
                    auc_list.append(auc_i)
        avg_auc = np.mean(auc_list) if auc_list else None
        return np.mean(acc_list), np.mean(sens_list), np.mean(spec_list), np.mean(f1_list), avg_auc

# -----------------------------
# Ana EÄŸitim & DeÄŸerlendirme DÃ¶ngÃ¼sÃ¼
# -----------------------------
results = {}  # SonuÃ§larÄ± saklamak iÃ§in

# Ã‡oklu etiket (multioutput) problemleri iÃ§in, stratifiye yapÄ±lamadÄ±ÄŸÄ± iÃ§in KFold kullanÄ±yoruz.
cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Her dÃ¶nÃ¼ÅŸÃ¼m stratejisi iÃ§in:
for trans_name, transformer in transformations.items():
    results[trans_name] = {}

    # Her model iÃ§in:
    for model_name, model in models.items():
        fold_accuracies = []
        fold_sensitivities = []
        fold_specificities = []
        fold_f_measures = []
        fold_auc_rocs = []

        # Her fold iÃ§in:
        for train_index, test_index in cv.split(X):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # Ã–lÃ§eklendirme (StandardScaler)
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)

            # Uygulanan dÃ¶nÃ¼ÅŸÃ¼m varsa (PCA veya LDA):
            if transformer is not None:
                # Her fold iÃ§in transformer'Ä± yeniden oluÅŸturmak daha doÄŸru.
                if trans_name == "PCA":
                    trans = PCA(n_components=0.95, random_state=42)
                    X_train = trans.fit_transform(X_train)
                    X_test = trans.transform(X_test)
                elif trans_name == "LDA":
                    trans = LDA()
                    # LDA, etiket bilgisine ihtiyaÃ§ duyar.
                    X_train = trans.fit_transform(X_train, y_train if y_train.ndim==1 else y_train[:,0])
                    X_test = trans.transform(X_test)

            # EÄŸer Ã§oklu etiket (multioutput) varsa, bazÄ± modellerin bu desteÄŸi yoktur.
            # Bu durumda, tÃ¼m modelleri MultiOutputClassifier ile sarmalÄ±yoruz.
            if y.ndim > 1 and y.shape[1] > 1:
                clf = MultiOutputClassifier(clone(model))
            else:
                clf = clone(model)

            # Modeli eÄŸitelim
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)

            # AUC ROC iÃ§in tahmin olasÄ±lÄ±klarÄ±nÄ± almaya Ã§alÄ±ÅŸalÄ±m (varsa)
            y_prob = None
            try:
                # EÄŸer multioutput ise, MultiOutputClassifier.predict_proba dÃ¶nen liste verir.
                if hasattr(clf, "predict_proba"):
                    if y.ndim > 1 and y.shape[1] > 1:
                        y_prob = clf.predict_proba(X_test)  # Bu, her Ã§Ä±ktÄ± iÃ§in bir liste dÃ¶ner.
                    else:
                        y_prob = clf.predict_proba(X_test)[:, 1]
                elif hasattr(clf, "decision_function"):
                    y_prob = clf.decision_function(X_test)
            except Exception:
                y_prob = None

            # EÄŸer y_test Ã§oklu etiketliyse, y_pred'nin de aynÄ± forma sahip olduÄŸundan emin olalÄ±m.
            # BazÄ± durumlarda predict metodu 1D dÃ¶nebilir.
            if y.ndim > 1 and y_pred.ndim == 1:
                y_pred = y_pred.reshape(-1, 1)

            # Metrikleri hesaplayalÄ±m
            acc, sens, spec, f_measure, auc = evaluate_metrics(y_test, y_pred, y_prob)
            fold_accuracies.append(acc)
            fold_sensitivities.append(sens)
            fold_specificities.append(spec)
            fold_f_measures.append(f_measure)
            if auc is not None:
                fold_auc_rocs.append(auc)

        avg_auc = np.mean(fold_auc_rocs) if fold_auc_rocs else None
        results[trans_name][model_name] = {
            'Accuracy': np.mean(fold_accuracies),
            'Sensitivity': np.mean(fold_sensitivities),
            'Specificity': np.mean(fold_specificities),
            'F-Measure': np.mean(fold_f_measures),
            'AUC ROC': avg_auc
        }

# -----------------------------
# SonuÃ§larÄ±n CSV'ye Kaydedilmesi
# -----------------------------
all_results = []
for trans_name, models_dict in results.items():
    for model_name, metrics in models_dict.items():
        row = {"Transformation": trans_name, "Model": model_name}
        row.update(metrics)
        all_results.append(row)

df_results = pd.DataFrame(all_results)
output_file = "/content/drive/MyDrive/model_evaluation_results.csv"
df_results.to_csv(output_file, index=False)

print(f"SonuÃ§lar '{output_file}' dosyasÄ±na kaydedildi.")

import numpy as np
import pandas as pd

# Makine Ã¶ÄŸrenmesi modÃ¼lleri
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

# Metrik modÃ¼lleri
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score

# Model kopyalamak iÃ§in
from sklearn.base import clone

# Verilerin yolu
x_path = "/content/drive/MyDrive/X_3D_CNN_reshaped.npy"
y_path = "/content/drive/MyDrive/y_labels_reshaped.npy"

# Veriyi yÃ¼kleyelim
X = np.load(x_path)
y = np.load(y_path)

# EÄŸer veriler 3 boyutlu ise (Ã¶rneÄŸin; gÃ¶rÃ¼ntÃ¼, video vb.) ve modellerin kabul edeceÄŸi 2B formata dÃ¶nÃ¼ÅŸtÃ¼rmek gerekiyorsa,
# aÅŸaÄŸÄ±daki satÄ±r ile reshape edebilirsiniz:
# X = X.reshape(X.shape[0], -1)

# Modelleri tanÄ±mlÄ±yoruz
models = {
    "SVM": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "GBM": GradientBoostingClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Uygulanacak Ã¶n iÅŸleme stratejileri
# 'None': Boyut indirgeme yapmadan,
# 'PCA': PCA ile boyut indirgeme (aÃ§Ä±klanan varyansÄ±n %95â€™i korunur),
# 'LDA': LDA ile boyut indirgeme (etiket bilgisine baÄŸlÄ±)
transformations = {
    "None": None,
    "PCA": PCA(n_components=0.95, random_state=42),
    "LDA": LDA()  # LDA, sÄ±nÄ±f sayÄ±sÄ±na baÄŸlÄ± olarak n_components ayarlar (ikili sÄ±nÄ±flandÄ±rmada n_components=1 olur)
}

# SonuÃ§larÄ± saklamak iÃ§in bir sÃ¶zlÃ¼k oluÅŸturuyoruz
results = {}

# 5-fold stratifiye Ã§apraz doÄŸrulama
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Her bir dÃ¶nÃ¼ÅŸÃ¼m stratejisi iÃ§in:
for trans_name, transformer in transformations.items():
    results[trans_name] = {}  # Ã–rneÄŸin: results["PCA"] = { ... }

    # Her model iÃ§in:
    for model_name, model in models.items():
        fold_accuracies = []
        fold_sensitivities = []
        fold_specificities = []
        fold_f_measures = []
        fold_auc_rocs = []

        # Ã‡apraz doÄŸrulama foldâ€™larÄ±
        for train_index, test_index in cv.split(X, y):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # Ã–ncelikle StandardScaler ile Ã¶lÃ§eklendirme (hem PCA hem de LDA daha iyi sonuÃ§ verir)
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)

            # EÄŸer bir dÃ¶nÃ¼ÅŸÃ¼m (PCA veya LDA) uygulanacaksa:
            if transformer is not None:
                # Her foldâ€™da transformerâ€™Ä± yeniden tanÄ±mlamak en temiz yaklaÅŸÄ±mdÄ±r.
                if trans_name == "PCA":
                    trans = PCA(n_components=0.95, random_state=42)
                    X_train = trans.fit_transform(X_train)
                    X_test = trans.transform(X_test)
                elif trans_name == "LDA":
                    trans = LDA()
                    # LDA fit ederken etiket bilgisine ihtiyaÃ§ duyar.
                    X_train = trans.fit_transform(X_train, y_train)
                    X_test = trans.transform(X_test)

            # Modeli fold iÃ§in yeniden oluÅŸturuyoruz (clone kullanarak global nesnenin etkilenmesini Ã¶nleriz)
            clf = clone(model)
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)

            # Accuracy
            acc = accuracy_score(y_test, y_pred)
            fold_accuracies.append(acc)

            # Confusion Matrix'den sensitivity (TPR) ve specificity (TNR) hesaplama (ikili sÄ±nÄ±flandÄ±rma varsayÄ±mÄ±yla)
            # Confusion Matrix formatÄ±: [[TN, FP], [FN, TP]]
            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
            fold_sensitivities.append(sensitivity)
            fold_specificities.append(specificity)

            # F-Measure (F1 Score)
            f1 = f1_score(y_test, y_pred)
            fold_f_measures.append(f1)

            # AUC ROC hesabÄ± (modelin probability veya decision_function metoduna baÄŸlÄ± olarak)
            auc = None
            try:
                if hasattr(clf, "predict_proba"):
                    y_prob = clf.predict_proba(X_test)[:, 1]
                elif hasattr(clf, "decision_function"):
                    y_prob = clf.decision_function(X_test)
                auc = roc_auc_score(y_test, y_prob)
            except Exception as e:
                pass  # AUC hesaplanamÄ±yorsa None kalÄ±r.
            if auc is not None:
                fold_auc_rocs.append(auc)

        # Foldâ€™lardan elde edilen metriklerin ortalamasÄ±nÄ± hesaplÄ±yoruz
        avg_auc = np.mean(fold_auc_rocs) if len(fold_auc_rocs) > 0 else None
        results[trans_name][model_name] = {
            'Accuracy': np.mean(fold_accuracies),
            'Sensitivity': np.mean(fold_sensitivities),
            'Specificity': np.mean(fold_specificities),
            'F-Measure': np.mean(fold_f_measures),
            'AUC ROC': avg_auc
        }

# SonuÃ§larÄ± bir DataFrameâ€™e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼p CSV olarak kaydediyoruz.
all_results = []
for trans_name, models_dict in results.items():
    for model_name, metrics in models_dict.items():
        row = {"Transformation": trans_name, "Model": model_name}
        row.update(metrics)
        all_results.append(row)

df_results = pd.DataFrame(all_results)
output_file = "/content/drive/MyDrive/makine_model_evaluation_results.csv"
df_results.to_csv(output_file, index=False)

print(f"SonuÃ§lar '{output_file}' dosyasÄ±na kaydedildi.")

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# Veri dosyalarÄ±
pca_path = "/content/drive/MyDrive/data_with_pca_normalized.csv"
lda_train_path = "/content/drive/MyDrive/data_with_lda_train.csv"
lda_test_path = "/content/drive/MyDrive/data_with_lda_test.csv"
ham_data_path = "/content/drive/MyDrive/5_data_balanced.csv"

# 1. PCA uygulanmÄ±ÅŸ veriyi yÃ¼kleme
data_pca = pd.read_csv(pca_path)
X_pca = data_pca.drop(columns=['label'])
y_pca = data_pca['label'].astype(int)  # SÄ±nÄ±flarÄ± 0, 1, 2 olarak kullanma

# 2. LDA uygulanmÄ±ÅŸ veriyi yÃ¼kleme
lda_train = pd.read_csv(lda_train_path)
lda_test = pd.read_csv(lda_test_path)
X_lda = pd.concat([lda_train.drop(columns=['label']), lda_test.drop(columns=['label'])])
y_lda = pd.concat([lda_train['label'], lda_test['label']]).astype(int)  # SÄ±nÄ±flarÄ± 0, 1, 2 olarak kullanma

# 3. PCA ve LDA uygulanmamÄ±ÅŸ veriyi yÃ¼kleme
ham_data = pd.read_csv(ham_data_path)
X_ham = ham_data.drop(columns=['label'])
y_ham = ham_data['label'].astype(int)  # SÄ±nÄ±flarÄ± 0, 1, 2 olarak kullanma

# Modeller
models = {
    "SVM": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "GBM": GradientBoostingClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Performans metrikleri hesaplama fonksiyonu
def evaluate_model(X, y, model_name):
    results = []
    cm_dict = {name: [] for name in models.keys()}  # Her model iÃ§in ayrÄ± karmaÅŸÄ±klÄ±k matrisi saklama
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    for name, model in models.items():
        fold_accuracies = []
        fold_sensitivities = []
        fold_specificities = []
        fold_f_measures = []
        fold_auc_rocs = []

        for train_index, val_index in cv.split(X, y):
            X_train, X_val = X.iloc[train_index], X.iloc[val_index]
            y_train, y_val = y.iloc[train_index], y.iloc[val_index]

            # Ã–zellik standardizasyonu
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_val = scaler.transform(X_val)

            # Modeli eÄŸitme
            model.fit(X_train, y_train)

            # Tahmin yapma
            y_pred = model.predict(X_val)
            y_prob = model.predict_proba(X_val) if hasattr(model, 'predict_proba') else None

            # Performans metrikleri
            cm = confusion_matrix(y_val, y_pred, labels=[0, 1, 2])
            sensitivity = np.mean([cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0 for i in range(3)])
            specificity = np.mean([(cm.sum() - cm[:, i].sum() - cm[i].sum() + cm[i, i]) / (cm.sum() - cm[:, i].sum()) if cm.sum() - cm[:, i].sum() > 0 else 0 for i in range(3)])
            f_measure = f1_score(y_val, y_pred, average='weighted')
            auc_roc = roc_auc_score(y_val, y_prob, multi_class='ovr') if y_prob is not None else None

            fold_accuracies.append(accuracy_score(y_val, y_pred))
            fold_sensitivities.append(sensitivity)
            fold_specificities.append(specificity)
            fold_f_measures.append(f_measure)
            if auc_roc is not None:
                fold_auc_rocs.append(auc_roc)

            cm_dict[name].append(cm)

        # Ortalama sonuÃ§lar
        results.append({
            'Model': name,
            'Accuracy': np.mean(fold_accuracies),
            'Sensitivity': np.mean(fold_sensitivities),
            'Specificity': np.mean(fold_specificities),
            'F-Measure': np.mean(fold_f_measures),
            'AUC ROC': np.mean(fold_auc_rocs) if fold_auc_rocs else None,
            'Dataset': model_name
        })

    return results, cm_dict

# TÃ¼m veri setleri iÃ§in Ã§apraz doÄŸrulama ve karmaÅŸÄ±klÄ±k matrislerini alma
results_pca, cm_pca_dict = evaluate_model(X_pca, y_pca, "PCA")
results_lda, cm_lda_dict = evaluate_model(X_lda, y_lda, "LDA")
results_ham, cm_ham_dict = evaluate_model(X_ham, y_ham, "Ham")

# SonuÃ§larÄ± birleÅŸtirme
all_results = pd.DataFrame(results_pca + results_lda + results_ham)

# SonuÃ§larÄ± CSV olarak kaydetme
all_results.to_csv('/content/drive/MyDrive/ml_All_model_results.csv', index=False)

# Her model iÃ§in karmaÅŸÄ±klÄ±k matrislerini gÃ¶rselleÅŸtirme
for model_name, cm_list in {**cm_pca_dict, **cm_lda_dict, **cm_ham_dict}.items():
    total_cm = np.sum(cm_list, axis=0)
    plt.figure(figsize=(7, 7))
    sns.heatmap(total_cm, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=['Class 0', 'Class 1', 'Class 2'],
                yticklabels=['Class 0', 'Class 1', 'Class 2'])
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# 8. SonuÃ§larÄ± gÃ¶rselleÅŸtirme (Ã¼Ã§ ayrÄ± grafik iÃ§in dÃ¼zenleme)

# Bar geniÅŸliÄŸi
bar_width = 0.15
x = np.arange(len(models))  # Modellerin sayÄ±sÄ± kadar bar yerleÅŸtirilecek

# Metrikler
metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'F-Measure', 'AUC ROC']
metric_labels = ['Accuracy', 'Sensitivity', 'Specificity', 'F-Measure', 'AUC ROC']

# Renk paleti (her metrik iÃ§in farklÄ± renk)
colors = sns.color_palette("Set2", len(metrics))

# 1. PCA verisi iÃ§in grafik
plt.figure(figsize=(14, 8))
for i, metric in enumerate(metrics):
    for j, model_name in enumerate(models.keys()):
        # PCA verisi iÃ§in metrik deÄŸeri
        metric_data = all_results[(all_results['Model'] == model_name) & (all_results['Dataset'] == 'PCA')]
        metric_value = metric_data[metric].values[0]
        offset = bar_width * i  # Her metriÄŸi yatayda sÄ±ralamak iÃ§in ofset
        plt.bar(x[j] + offset, metric_value, bar_width, color=colors[i], label=metric if j == 0 else "")

plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Performance Metrics for PCA')
plt.xticks(x + bar_width * 2, models.keys())
plt.legend(title='Metrics', loc='upper right')
plt.tight_layout()
plt.show()

# 2. LDA verisi iÃ§in grafik
plt.figure(figsize=(14, 8))
for i, metric in enumerate(metrics):
    for j, model_name in enumerate(models.keys()):
        # LDA verisi iÃ§in metrik deÄŸeri
        metric_data = all_results[(all_results['Model'] == model_name) & (all_results['Dataset'] == 'LDA')]
        metric_value = metric_data[metric].values[0]
        offset = bar_width * i  # Her metriÄŸi yatayda sÄ±ralamak iÃ§in ofset
        plt.bar(x[j] + offset, metric_value, bar_width, color=colors[i], label=metric if j == 0 else "")

plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Performance Metrics for LDA')
plt.xticks(x + bar_width * 2, models.keys())
plt.legend(title='Metrics', loc='upper right')
plt.tight_layout()
plt.show()

# 3. Ham veri seti iÃ§in grafik
plt.figure(figsize=(14, 8))
for i, metric in enumerate(metrics):
    for j, model_name in enumerate(models.keys()):
        # Ham veri seti iÃ§in metrik deÄŸeri
        metric_data = all_results[(all_results['Model'] == model_name) & (all_results['Dataset'] == 'Ham')]
        metric_value = metric_data[metric].values[0]
        offset = bar_width * i  # Her metriÄŸi yatayda sÄ±ralamak iÃ§in ofset
        plt.bar(x[j] + offset, metric_value, bar_width, color=colors[i], label=metric if j == 0 else "")

plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Performance Metrics for Raw (Ham) Data')
plt.xticks(x + bar_width * 2, models.keys())
plt.legend(title='Metrics', loc='upper right')
plt.tight_layout()
plt.show()

"""# DERÄ°N Ã–ÄRENME

"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Dosya yollarÄ±
lda_file = '/content/drive/MyDrive/dl_lda_model_results_kfold.csv'
no_method_file = '/content/drive/MyDrive/dl_model_performance_results_kfold.csv'
pca_file = '/content/drive/MyDrive/dl_pca_model_performance_results_kfold.csv'

# DosyalarÄ± yÃ¼kle
lda_results = pd.read_csv(lda_file)
no_method_results = pd.read_csv(no_method_file)
pca_results = pd.read_csv(pca_file)

# SÃ¼tun adlarÄ±nÄ± kontrol et
print("LDA Columns:", lda_results.columns)
print("PCA Columns:", pca_results.columns)
print("No Method Columns:", no_method_results.columns)

# Performans metriklerini kontrol et
lda_metrics = ['accuracy', 'f1', 'sensitivity', 'specificity', 'auc_roc']
pca_metrics = ['accuracy', 'f1', 'sensitivity', 'specificity', 'auc_roc']
no_method_metrics = ['accuracy', 'f1', 'sensitivity', 'specificity', 'auc_roc']

# Verilerin eksik olup olmadÄ±ÄŸÄ±nÄ± kontrol et ve NaN yerine 0 ile doldur
lda_results = lda_results.fillna(0)
pca_results = pca_results.fillna(0)
no_method_results = no_method_results.fillna(0)

# OrtalamalarÄ± hesapla
lda_means = [lda_results[metric].mean() for metric in lda_metrics]
pca_means = [pca_results[metric].mean() for metric in pca_metrics]
no_method_means = [no_method_results[metric].mean() for metric in no_method_metrics]

# Metriklerin konumlarÄ±nÄ± belirle
x = np.arange(len(lda_metrics))  # Metriklerin konumu
width = 0.25  # BarlarÄ±n geniÅŸliÄŸi

# Grafik oluÅŸturma
fig, ax = plt.subplots(figsize=(10, 6))

# Barlar
ax.bar(x - width, pca_means, width, label='PCA')
ax.bar(x, lda_means, width, label='LDA')
ax.bar(x + width, no_method_means, width, label='No Method')

# GrafiÄŸi Ã¶zelleÅŸtirme
ax.set_xlabel('Performance Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of Performance Metrics for Deep Learning PCA, LDA, and No Method')
ax.set_xticks(x)
ax.set_xticklabels(lda_metrics)
ax.legend()

# GrafiÄŸi gÃ¶sterme
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Dropout, Flatten
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli
def create_dbn_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Autoencoder Modeli
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli EÄŸitme ve Test Etme
def train_and_predict_kfold(features, labels, model_name, input_dim, input_shape=None, n_splits=5):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    total_cm = None

    for train_index, test_index in skf.split(features, labels):
        X_train, X_test = features[train_index], features[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        y_train = to_categorical(y_train)
        y_test = to_categorical(y_test)

        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

        y_pred = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_test_classes = np.argmax(y_test, axis=1)

        cm = confusion_matrix(y_test_classes, y_pred_classes)
        if total_cm is None:
            total_cm = cm
        else:
            total_cm += cm

    return total_cm

# KarmaÅŸÄ±klÄ±k Matrisini GÃ¶rselleÅŸtirme
def plot_confusion_matrix(cm, model_name, dataset_name):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(cm.shape[0]), yticklabels=np.arange(cm.shape[0]))
    plt.title(f'Confusion Matrix - {model_name} on {dataset_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

def main():
    # Veri yollarÄ±
    paths = [
        "/content/drive/MyDrive/data_with_lda_train.csv",
        "/content/drive/MyDrive/data_with_lda_test.csv",
        "/content/drive/MyDrive/5_data_balanced.csv",
        "/content/drive/MyDrive/data_with_pca_normalized.csv"
    ]

    # Her model iÃ§in karmaÅŸÄ±klÄ±k matrislerini toplamak
    confusion_matrices = {model: None for model in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]}

    # Her veri seti ile iÅŸlem yapÄ±lacak
    for path in paths:
        data = pd.read_csv(path)

        # Etiketler ve Ã–zellikler
        labels = data.iloc[:, -1].values
        features = data.iloc[:, :-1].values

        print(f"Veri seti: {path}")
        print(f"Etiketlerin ilk 5 deÄŸeri: {labels[:5]}")
        print(f"Ã–zelliklerin boyutu: {features.shape}")

        # Her model iÃ§in eÄŸitim ve karmaÅŸÄ±klÄ±k matrisi toplama
        for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
            cm = train_and_predict_kfold(features, labels, model_name, features.shape[1])
            if confusion_matrices[model_name] is None:
                confusion_matrices[model_name] = cm
            else:
                confusion_matrices[model_name] += cm

    # KarmaÅŸÄ±klÄ±k Matrislerinin GÃ¶rselleÅŸtirilmesi
    for model_name, cm in confusion_matrices.items():
        print(f"{model_name} Modeli iÃ§in Toplam KarmaÅŸÄ±klÄ±k Matrisi:")
        plot_confusion_matrix(cm, model_name, "All Datasets")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, roc_auc_score
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, LSTM, SimpleRNN, Dropout
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical
import seaborn as sns

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli (Deep Belief Network)
def create_dbn_model(input_dim, num_classes):
    model = create_mlp_model(input_dim, num_classes)
    return model

# Autoencoder Modeli (SÄ±nÄ±flandÄ±rma iÃ§in)
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))  # SÄ±nÄ±flandÄ±rma amaÃ§lÄ± Ã§Ä±kÄ±ÅŸ katmanÄ±
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli EÄŸitme ve Test Etme
def train_and_predict(features, labels, model_name, input_dim, input_shape=None, kfold_splits=5):
    # K-Fold Ã§apraz doÄŸrulama
    kfold = KFold(n_splits=kfold_splits, shuffle=True, random_state=42)

    accuracies = []
    f1_scores = []
    sensitivities = []
    specificities = []
    auc_roc_scores = []
    cm_list = []

    for train_index, test_index in kfold.split(features):
        X_train, X_test = features[train_index], features[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        # Verilerin Ã¶lÃ§eklendirilmesi
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Etiketlerin one-hot encoding formatÄ±na Ã§evrilmesi
        y_train = to_categorical(y_train)
        y_test = to_categorical(y_test)

        # Modelin oluÅŸturulmasÄ±
        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # X_train iÃ§in 3D ÅŸekil
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)  # X_test iÃ§in 3D ÅŸekil
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        # Modeli eÄŸitme
        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)

        # Tahmin yapma
        y_pred = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_test_classes = np.argmax(y_test, axis=1)

        # BaÅŸarÄ±yÄ± hesaplama
        accuracy = accuracy_score(y_test_classes, y_pred_classes)
        accuracies.append(accuracy)

        # DiÄŸer metrikleri hesaplama
        f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
        f1_scores.append(f1)

        sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
        sensitivities.append(sensitivity)

        # Specificity hesaplama (True Negatives / (True Negatives + False Positives))
        cm = confusion_matrix(y_test_classes, y_pred_classes)
        specificity = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0
        specificities.append(specificity)

        # AUC ROC hesaplama
        auc_roc = roc_auc_score(y_test, y_pred, multi_class='ovr')
        auc_roc_scores.append(auc_roc)

        cm_list.append(cm)

    # Ortalama metrikleri hesaplama
    metrics = {
        'accuracy': np.mean(accuracies),
        'f1': np.mean(f1_scores),
        'sensitivity': np.mean(sensitivities),
        'specificity': np.mean(specificities),
        'auc_roc': np.mean(auc_roc_scores),
        'confusion_matrix': cm_list
    }

    return metrics

# KarmaÅŸÄ±klÄ±k Matrisini GÃ¶rselleÅŸtirme
def plot_confusion_matrix(cm, model_name, save_path=None):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(cm.shape[0]), yticklabels=np.arange(cm.shape[0]))
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    # KarmaÅŸÄ±klÄ±k matrisini kaydetme
    if save_path:
        plt.savefig(save_path)
    else:
        plt.show()

# Model PerformanslarÄ±nÄ± GÃ¶rselleÅŸtirme
def plot_model_performance(metrics):
    model_names = list(metrics.keys())
    accuracy_values = [metrics[model]['accuracy'] for model in model_names]
    f1_values = [metrics[model]['f1'] for model in model_names]
    sensitivity_values = [metrics[model]['sensitivity'] for model in model_names]
    specificity_values = [metrics[model]['specificity'] for model in model_names]
    auc_roc_values = [metrics[model]['auc_roc'] for model in model_names]

    # Performans metriklerini bar grafiÄŸiyle gÃ¶rselleÅŸtirme
    bar_width = 0.15
    x = np.arange(len(model_names))

    plt.figure(figsize=(10, 6))

    plt.bar(x - 2*bar_width, accuracy_values, bar_width, label='Accuracy', color='b')
    plt.bar(x - bar_width, f1_values, bar_width, label='F1 Score', color='g')
    plt.bar(x, sensitivity_values, bar_width, label='Sensitivity', color='r')
    plt.bar(x + bar_width, specificity_values, bar_width, label='Specificity', color='y')
    plt.bar(x + 2*bar_width, auc_roc_values, bar_width, label='AUC ROC', color='c')

    plt.xlabel('Model')
    plt.ylabel('Score')
    plt.title('Model Performance Comparison')
    plt.xticks(x, model_names)
    plt.legend()
    plt.show()

def main():
    # Veri setini yÃ¼kleme
    file_path = "/content/drive/MyDrive/5_data_balanced.csv"
    data = pd.read_csv(file_path)

    # Etiketler ve Ã¶zelliklerin ayrÄ±lmasÄ±
    labels = data.iloc[:, -1].values
    features = data.iloc[:, :-1].values

    print(f"Etiketlerin ilk 5 deÄŸeri: {labels[:5]}")
    print(f"Ã–zelliklerin boyutu: {features.shape}")

    # Model performanslarÄ±nÄ± saklayacak bir sÃ¶zlÃ¼k
    metrics = {}

    # Modelleri eÄŸitme ve doÄŸruluklarÄ±nÄ± saklama
    for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
        metrics[model_name] = train_and_predict(features, labels, model_name, features.shape[1])

        print(f"{model_name} model doÄŸruluÄŸu: {metrics[model_name]['accuracy']:.4f}")

        # KarmaÅŸÄ±klÄ±k matrisini gÃ¶rselleÅŸtirme ve kaydetme
        plot_confusion_matrix(metrics[model_name]['confusion_matrix'][0], model_name, save_path=f'{model_name}_confusion_matrix.png')

    # Modellerin performanslarÄ±nÄ± gÃ¶rselleÅŸtirme
    plot_model_performance(metrics)

    # SonuÃ§larÄ± bir CSV dosyasÄ±na kaydetme
    results_df = pd.DataFrame(metrics).T
    results_df.to_csv("/content/drive/MyDrive/dl_model_performance_results_kfold.csv")
    print("SonuÃ§lar 'dl_model_performance_results_kfold.csv' dosyasÄ±na kaydedildi.")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, roc_auc_score
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, LSTM, SimpleRNN, Dropout
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical
import seaborn as sns

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli (Deep Belief Network)
def create_dbn_model(input_dim, num_classes):
    model = create_mlp_model(input_dim, num_classes)
    return model

# Autoencoder Modeli (SÄ±nÄ±flandÄ±rma iÃ§in)
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))  # SÄ±nÄ±flandÄ±rma amaÃ§lÄ± Ã§Ä±kÄ±ÅŸ katmanÄ±
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli EÄŸitme ve Test Etme
def train_and_predict(features, labels, model_name, input_dim, input_shape=None, kfold_splits=5):
    # K-Fold Ã§apraz doÄŸrulama
    kfold = KFold(n_splits=kfold_splits, shuffle=True, random_state=42)

    accuracies = []
    f1_scores = []
    sensitivities = []
    specificities = []
    auc_roc_scores = []
    cm_list = []

    for train_index, test_index in kfold.split(features):
        X_train, X_test = features[train_index], features[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        # Verilerin Ã¶lÃ§eklendirilmesi
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Etiketlerin one-hot encoding formatÄ±na Ã§evrilmesi
        y_train = to_categorical(y_train)
        y_test = to_categorical(y_test)

        # Modelin oluÅŸturulmasÄ±
        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # X_train iÃ§in 3D ÅŸekil
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)  # X_test iÃ§in 3D ÅŸekil
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        # Modeli eÄŸitme
        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)

        # Tahmin yapma
        y_pred = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_test_classes = np.argmax(y_test, axis=1)

        # BaÅŸarÄ±yÄ± hesaplama
        accuracy = accuracy_score(y_test_classes, y_pred_classes)
        accuracies.append(accuracy)

        # DiÄŸer metrikleri hesaplama
        f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
        f1_scores.append(f1)

        sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
        sensitivities.append(sensitivity)

        # Specificity hesaplama (True Negatives / (True Negatives + False Positives))
        cm = confusion_matrix(y_test_classes, y_pred_classes)
        specificity = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0
        specificities.append(specificity)

        # AUC ROC hesaplama
        auc_roc = roc_auc_score(y_test, y_pred, multi_class='ovr')
        auc_roc_scores.append(auc_roc)

        cm_list.append(cm)

    # Ortalama metrikleri hesaplama
    metrics = {
        'accuracy': np.mean(accuracies),
        'f1': np.mean(f1_scores),
        'sensitivity': np.mean(sensitivities),
        'specificity': np.mean(specificities),
        'auc_roc': np.mean(auc_roc_scores),
        'confusion_matrix': cm_list
    }

    return metrics

# KarmaÅŸÄ±klÄ±k Matrisini GÃ¶rselleÅŸtirme
def plot_confusion_matrix(cm, model_name, save_path=None):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(cm.shape[0]), yticklabels=np.arange(cm.shape[0]))
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    # KarmaÅŸÄ±klÄ±k matrisini kaydetme
    if save_path:
        plt.savefig(save_path)
    else:
        plt.show()

# Model PerformanslarÄ±nÄ± GÃ¶rselleÅŸtirme
def plot_model_performance(metrics):
    model_names = list(metrics.keys())
    accuracy_values = [metrics[model]['accuracy'] for model in model_names]
    f1_values = [metrics[model]['f1'] for model in model_names]
    sensitivity_values = [metrics[model]['sensitivity'] for model in model_names]
    specificity_values = [metrics[model]['specificity'] for model in model_names]
    auc_roc_values = [metrics[model]['auc_roc'] for model in model_names]

    # Performans metriklerini bar grafiÄŸiyle gÃ¶rselleÅŸtirme
    bar_width = 0.15
    x = np.arange(len(model_names))

    plt.figure(figsize=(10, 6))

    plt.bar(x - 2*bar_width, accuracy_values, bar_width, label='Accuracy', color='b')
    plt.bar(x - bar_width, f1_values, bar_width, label='F1 Score', color='g')
    plt.bar(x, sensitivity_values, bar_width, label='Sensitivity', color='r')
    plt.bar(x + bar_width, specificity_values, bar_width, label='Specificity', color='y')
    plt.bar(x + 2*bar_width, auc_roc_values, bar_width, label='AUC ROC', color='c')

    plt.xlabel('Model')
    plt.ylabel('Score')
    plt.title('Model Performance Comparison')
    plt.xticks(x, model_names)
    plt.legend()
    plt.show()

def main():
    # Veri setini yÃ¼kleme
    file_path = "/content/drive/MyDrive/data_with_pca_normalized.csv"
    data = pd.read_csv(file_path)

    # Etiketler ve Ã¶zelliklerin ayrÄ±lmasÄ±
    labels = data.iloc[:, -1].values
    features = data.iloc[:, :-1].values

    print(f"Etiketlerin ilk 5 deÄŸeri: {labels[:5]}")
    print(f"Ã–zelliklerin boyutu: {features.shape}")

    # Model performanslarÄ±nÄ± saklayacak bir sÃ¶zlÃ¼k
    metrics = {}

    # Modelleri eÄŸitme ve doÄŸruluklarÄ±nÄ± saklama
    for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
        metrics[model_name] = train_and_predict(features, labels, model_name, features.shape[1])

        print(f"{model_name} model doÄŸruluÄŸu: {metrics[model_name]['accuracy']:.4f}")

        # KarmaÅŸÄ±klÄ±k matrisini gÃ¶rselleÅŸtirme ve kaydetme
        plot_confusion_matrix(metrics[model_name]['confusion_matrix'][0], model_name, save_path=f'{model_name}_confusion_matrix.png')

    # Modellerin performanslarÄ±nÄ± gÃ¶rselleÅŸtirme
    plot_model_performance(metrics)

    # SonuÃ§larÄ± bir CSV dosyasÄ±na kaydetme
    results_df = pd.DataFrame(metrics).T
    results_df.to_csv("/content/drive/MyDrive/dl_pca_model_performance_results_kfold.csv")
    print("SonuÃ§lar 'dl_pca_model_performance_results_kfold.csv' dosyasÄ±na kaydedildi.")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, roc_auc_score
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Dropout, Flatten
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical
import seaborn as sns
from keras.layers import GlobalAveragePooling1D
from sklearn.model_selection import KFold

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(GlobalAveragePooling1D())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli
def create_dbn_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Autoencoder Modeli
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def train_and_predict_with_kfold(features, labels, model_name, input_dim, input_shape=None, k=5):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    fold_metrics = []

    for train_index, val_index in kf.split(features):
        X_train, X_val = features[train_index], features[val_index]
        y_train, y_val = labels[train_index], labels[val_index]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_val = scaler.transform(X_val)

        y_train = to_categorical(y_train)
        y_val = to_categorical(y_val)

        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)

        y_pred = model.predict(X_val)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_val_classes = np.argmax(y_val, axis=1)

        accuracy = accuracy_score(y_val_classes, y_pred_classes)
        f1 = f1_score(y_val_classes, y_pred_classes, average='weighted')
        sensitivity = recall_score(y_val_classes, y_pred_classes, average='weighted')

        cm = confusion_matrix(y_val_classes, y_pred_classes)
        specificity = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0

        auc_roc = roc_auc_score(y_val, y_pred, multi_class='ovr')

        fold_metrics.append({
            'accuracy': accuracy,
            'f1': f1,
            'sensitivity': sensitivity,
            'specificity': specificity,
            'auc_roc': auc_roc,
            'confusion_matrix': cm
        })

    # Ortalama metrikleri hesapla
    avg_metrics = {
        'accuracy': np.mean([fold['accuracy'] for fold in fold_metrics]),
        'f1': np.mean([fold['f1'] for fold in fold_metrics]),
        'sensitivity': np.mean([fold['sensitivity'] for fold in fold_metrics]),
        'specificity': np.mean([fold['specificity'] for fold in fold_metrics]),
        'auc_roc': np.mean([fold['auc_roc'] for fold in fold_metrics])
    }

    return avg_metrics, fold_metrics

# SonuÃ§larÄ± Kaydetme ve GÃ¶rselleÅŸtirme
def save_results_to_csv(metrics, file_name="/content/drive/MyDrive/dl_lda_model_results_kfold.csv"):
    results_df = pd.DataFrame.from_dict(metrics, orient='index')
    results_df.to_csv(file_name)
    print(f"SonuÃ§lar {file_name} dosyasÄ±na kaydedildi.")

def plot_model_performance(metrics):
    models = list(metrics.keys())
    avg_accuracy = [metrics[model]['accuracy'] for model in models]
    avg_f1 = [metrics[model]['f1'] for model in models]
    avg_sensitivity = [metrics[model]['sensitivity'] for model in models]
    avg_specificity = [metrics[model]['specificity'] for model in models]
    avg_auc_roc = [metrics[model]['auc_roc'] for model in models]

    plt.figure(figsize=(12, 8))

    x = np.arange(len(models))
    width = 0.15  # Bar width

    fig, ax = plt.subplots(figsize=(10, 6))

    ax.bar(x - 2*width, avg_accuracy, width, label='Accuracy')
    ax.bar(x - width, avg_f1, width, label='F1 Score')
    ax.bar(x, avg_sensitivity, width, label='Sensitivity')
    ax.bar(x + width, avg_specificity, width, label='Specificity')
    ax.bar(x + 2*width, avg_auc_roc, width, label='AUC ROC')

    ax.set_xlabel('Models')
    ax.set_ylabel('Metrics')
    ax.set_title('Model Performance Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(models)
    ax.legend()

    plt.tight_layout()
    plt.show()

def main():
    train_file_path = "/content/drive/MyDrive/data_with_lda_train.csv"
    test_file_path = "/content/drive/MyDrive/data_with_lda_test.csv"

    train_data = pd.read_csv(train_file_path)
    test_data = pd.read_csv(test_file_path)

    train_labels = train_data.iloc[:, -1].values
    train_features = train_data.iloc[:, :-1].values

    test_labels = test_data.iloc[:, -1].values
    test_features = test_data.iloc[:, :-1].values

    print(f"Etiketlerin ilk 5 deÄŸeri (EÄŸitim): {train_labels[:5]}")
    print(f"Ã–zelliklerin boyutu (EÄŸitim): {train_features.shape}")
    print(f"Etiketlerin ilk 5 deÄŸeri (Test): {test_labels[:5]}")
    print(f"Ã–zelliklerin boyutu (Test): {test_features.shape}")

    metrics = {}

    for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
        avg_metrics, fold_metrics = train_and_predict_with_kfold(train_features, train_labels, model_name, train_features.shape[1])

        metrics[model_name] = avg_metrics

        print(f"{model_name} modelinin K-Fold sonuÃ§larÄ±:")
        print(f"Ortalama DoÄŸruluk: {avg_metrics['accuracy']:.4f}")
        print(f"Ortalama F1 Skoru: {avg_metrics['f1']:.4f}")
        print(f"Ortalama Hassasiyet: {avg_metrics['sensitivity']:.4f}")
        print(f"Ortalama Spesifite: {avg_metrics['specificity']:.4f}")
        print(f"Ortalama AUC ROC: {avg_metrics['auc_roc']:.4f}")

    plot_model_performance(metrics)
    save_results_to_csv(metrics)

if __name__ == "__main__":
    main()

"""# HÄ°BRÄ°T MODEL EÄÄ°TÄ°M VE PERFORMANSI
 Hibrit model PCA ,LDA ve NO_PCA_LDA iÃ§in her bir foldun performans metriklerinin hesaplanmasÄ± ve kaydedilmesi ardÄ±ndan tÃ¼m sonuÃ§larÄ± bar grafiÄŸinde gÃ¶sterilmek iÃ§in birleÅŸtirilmesini, karmaÅŸÄ±klÄ±k matrisini iÃ§erir.
"""

pip install tensorflow

#HER BÄ°R FOLD UN METÄ°KLERÄ°NÄ° AYRI AYARI HESAPLAYIP YAZDIRIYOR.
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape
import matplotlib.pyplot as plt
import seaborn as sns

# Veri YÃ¼kleme
balanced_data = pd.read_csv("/content/drive/MyDrive/5_data_balanced.csv")

# Veriyi 3D CNN iÃ§in Yeniden Åekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    if num_features % time_steps != 0:
        # FazlalÄ±k Ã¶zellikleri atmak iÃ§in boyutu ayarla
        X = X[:, :(num_features // time_steps) * time_steps]
    features_per_step = X.shape[1] // time_steps
    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model TanÄ±mlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same',
               input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(2, 2, 1)),
        Flatten(),
        Reshape((time_steps, -1)),
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# DeÄŸerlendirme Fonksiyonu
def evaluate_model(y_test_classes, y_pred_classes, y_test, y_pred_prob):
    accuracy = accuracy_score(y_test_classes, y_pred_classes)
    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
    sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
    cm = confusion_matrix(y_test_classes, y_pred_classes)
    specificity = cm[1, 1] / (cm[1, 1] + cm[0, 1]) if (cm[1, 1] + cm[0, 1]) > 0 else 0
    try:
        auc_roc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='macro')
    except ValueError:
        auc_roc = None
    try:
        precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=1)
    except ValueError:
        precision = None
    return accuracy, f1, sensitivity, specificity, precision, auc_roc, cm

# K-Fold Ã‡apraz DoÄŸrulama
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
X = balanced_data.iloc[:, :-1].values
y = balanced_data.iloc[:, -1].values
time_steps = 10

fold = 1
results = []
for train_idx, test_idx in kfold.split(X, y):
    print(f"Fold {fold} BaÅŸlÄ±yor...")

    # EÄŸitim ve Test Verilerini AyÄ±rma
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Veriyi Yeniden Åekillendirme
    X_train = reshape_for_3d_cnn(X_train, time_steps)
    X_test = reshape_for_3d_cnn(X_test, time_steps)

    # Modeli OluÅŸturma
    model = create_hybrid_model(time_steps, X_train.shape[2])

    # Modeli EÄŸitme
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    # Tahminler
    y_pred_classes = np.argmax(model.predict(X_test), axis=1)
    y_pred_prob = model.predict(X_test)

    # DeÄŸerlendirme
    accuracy, f1, sensitivity, specificity, precision, auc_roc, cm = evaluate_model(y_test, y_pred_classes, y_test, y_pred_prob)
    results.append({
        "Fold": fold,
        "Accuracy": accuracy,
        "F1 Score": f1,
        "Sensitivity": sensitivity,
        "Specificity": specificity,
        "Precision": precision,
        "AUC ROC": auc_roc
    })

    print(f"Fold {fold} SonuÃ§larÄ±:")
    print(f"Accuracy: {accuracy}")
    print(f"F1 Score: {f1}")
    print(f"Sensitivity: {sensitivity}")
    print(f"Specificity: {specificity}")
    print(f"Precision: {precision}")
    print(f"AUC ROC: {auc_roc}")
    print(f"Confusion Matrix: \n{cm}")
    fold += 1

# TÃ¼m Fold SonuÃ§larÄ±nÄ± Kaydetme
results_df = pd.DataFrame(results)
results_df.to_csv("/content/drive/MyDrive/hibrit_no_kfold_results.csv", index=False)

# Ortalama SonuÃ§larÄ± YazdÄ±rma
print("\nK-Fold OrtalamalarÄ±:")
print(results_df.mean())

#HER BÄ°R FOLD UN METRÄ°KLERÄ°NÄ° HESAPLAYIP YAZDIRIYOR
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape
import matplotlib.pyplot as plt
import seaborn as sns

# Veri YÃ¼kleme
balanced_data = pd.read_csv("/content/drive/MyDrive/data_with_pca_normalized.csv")

# Veriyi 3D CNN iÃ§in Yeniden Åekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    if num_features % time_steps != 0:
        # FazlalÄ±k Ã¶zellikleri atmak iÃ§in boyutu ayarla
        X = X[:, :(num_features // time_steps) * time_steps]
    features_per_step = X.shape[1] // time_steps
    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model TanÄ±mlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same', input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(1, 1, 1)),  # Havuzlama boyutunu giriÅŸe uyarladÄ±k
        Flatten(),
        Reshape((time_steps, -1)),  # Veriyi zaman adÄ±mlarÄ±na gÃ¶re yeniden ÅŸekillendir
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model


# DeÄŸerlendirme Fonksiyonu
def evaluate_model(y_test_classes, y_pred_classes, y_test, y_pred_prob):
    accuracy = accuracy_score(y_test_classes, y_pred_classes)
    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
    sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
    cm = confusion_matrix(y_test_classes, y_pred_classes)
    specificity = cm[1, 1] / (cm[1, 1] + cm[0, 1]) if (cm[1, 1] + cm[0, 1]) > 0 else 0
    try:
        auc_roc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='macro')
    except ValueError:
        auc_roc = None
    try:
        precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=1)
    except ValueError:
        precision = None
    return accuracy, f1, sensitivity, specificity, precision, auc_roc, cm

# K-Fold Ã‡apraz DoÄŸrulama
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
X = balanced_data.iloc[:, :-1].values
y = balanced_data.iloc[:, -1].values
time_steps = 10

fold = 1
results = []
for train_idx, test_idx in kfold.split(X, y):
    print(f"Fold {fold} BaÅŸlÄ±yor...")

    # EÄŸitim ve Test Verilerini AyÄ±rma
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Veriyi Yeniden Åekillendirme
    X_train = reshape_for_3d_cnn(X_train, time_steps)
    X_test = reshape_for_3d_cnn(X_test, time_steps)

    # Modeli OluÅŸturma
    model = create_hybrid_model(time_steps, X_train.shape[2])

    # Modeli EÄŸitme
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    # Tahminler
    y_pred_classes = np.argmax(model.predict(X_test), axis=1)
    y_pred_prob = model.predict(X_test)

    # DeÄŸerlendirme
    accuracy, f1, sensitivity, specificity, precision, auc_roc, cm = evaluate_model(y_test, y_pred_classes, y_test, y_pred_prob)
    results.append({
        "Fold": fold,
        "Accuracy": accuracy,
        "F1 Score": f1,
        "Sensitivity": sensitivity,
        "Specificity": specificity,
        "Precision": precision,
        "AUC ROC": auc_roc
    })

    print(f"Fold {fold} SonuÃ§larÄ±:")
    print(f"Accuracy: {accuracy}")
    print(f"F1 Score: {f1}")
    print(f"Sensitivity: {sensitivity}")
    print(f"Specificity: {specificity}")
    print(f"Precision: {precision}")
    print(f"AUC ROC: {auc_roc}")
    print(f"Confusion Matrix: \n{cm}")
    fold += 1

# TÃ¼m Fold SonuÃ§larÄ±nÄ± Kaydetme
results_df = pd.DataFrame(results)
results_df.to_csv("/content/drive/MyDrive/hibrit_pca_kfold_results.csv", index=False)

# Ortalama SonuÃ§larÄ± YazdÄ±rma
print("\nK-Fold OrtalamalarÄ±:")
print(results_df.mean())

#HER BÄ°R K FOLD Ä°Ã‡Ä°N AYRI AYRI METRÄ°KLERÄ° Ä°Ã‡ERÄ°R.
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape
import matplotlib.pyplot as plt
import seaborn as sns

# Veri YÃ¼kleme
train_data = pd.read_csv("/content/drive/MyDrive/data_with_lda_train.csv")
test_data = pd.read_csv("/content/drive/MyDrive/data_with_lda_test.csv")

# Veriyi 3D CNN iÃ§in Yeniden Åekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    if num_features % time_steps != 0:
        # FazlalÄ±k Ã¶zellikleri atmak iÃ§in boyutu ayarla
        X = X[:, :(num_features // time_steps) * time_steps]
    features_per_step = X.shape[1] // time_steps
    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model TanÄ±mlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same', input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(1, 1, 1)),  # Havuzlama boyutunu giriÅŸe uyarladÄ±k
        Flatten(),
        Reshape((time_steps, -1)),  # Veriyi zaman adÄ±mlarÄ±na gÃ¶re yeniden ÅŸekillendir
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# DeÄŸerlendirme Fonksiyonu
def evaluate_model(y_test_classes, y_pred_classes, y_test, y_pred_prob):
    accuracy = accuracy_score(y_test_classes, y_pred_classes)
    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
    sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
    cm = confusion_matrix(y_test_classes, y_pred_classes)
    specificity = cm[1, 1] / (cm[1, 1] + cm[0, 1]) if (cm[1, 1] + cm[0, 1]) > 0 else 0
    try:
        auc_roc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='macro')
    except ValueError:
        auc_roc = None
    try:
        precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=1)
    except ValueError:
        precision = None
    return accuracy, f1, sensitivity, specificity, precision, auc_roc, cm

# K-Fold Ã‡apraz DoÄŸrulama
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
X = balanced_data.iloc[:, :-1].values
y = balanced_data.iloc[:, -1].values
time_steps = 10

fold = 1
results = []
for train_idx, test_idx in kfold.split(X, y):
    print(f"Fold {fold} BaÅŸlÄ±yor...")

    # EÄŸitim ve Test Verilerini AyÄ±rma
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Veriyi Yeniden Åekillendirme
    X_train = reshape_for_3d_cnn(X_train, time_steps)
    X_test = reshape_for_3d_cnn(X_test, time_steps)

    # Modeli OluÅŸturma
    model = create_hybrid_model(time_steps, X_train.shape[2])

    # Modeli EÄŸitme
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    # Tahminler
    y_pred_classes = np.argmax(model.predict(X_test), axis=1)
    y_pred_prob = model.predict(X_test)

    # DeÄŸerlendirme
    accuracy, f1, sensitivity, specificity, precision, auc_roc, cm = evaluate_model(y_test, y_pred_classes, y_test, y_pred_prob)
    results.append({
        "Fold": fold,
        "Accuracy": accuracy,
        "F1 Score": f1,
        "Sensitivity": sensitivity,
        "Specificity": specificity,
        "Precision": precision,
        "AUC ROC": auc_roc
    })

    print(f"Fold {fold} SonuÃ§larÄ±:")
    print(f"Accuracy: {accuracy}")
    print(f"F1 Score: {f1}")
    print(f"Sensitivity: {sensitivity}")
    print(f"Specificity: {specificity}")
    print(f"Precision: {precision}")
    print(f"AUC ROC: {auc_roc}")
    print(f"Confusion Matrix: \n{cm}")
    fold += 1

# TÃ¼m Fold SonuÃ§larÄ±nÄ± Kaydetme
results_df = pd.DataFrame(results)
results_df.to_csv("/content/drive/MyDrive/hibrit_lda_kfold_results.csv", index=False)

# Ortalama SonuÃ§larÄ± YazdÄ±rma
print("\nK-Fold OrtalamalarÄ±:")
print(results_df.mean())

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Flatten, Conv3D, MaxPooling3D
from keras.utils import to_categorical
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# 3D CNN+LSTM Modeli
def create_3d_cnn_lstm_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))
    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same'))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli EÄŸitme ve Test Etme
def train_and_predict(features, labels, model_name, input_shape=None):
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    if model_name == "3D_CNN_LSTM":
        time_steps = 1
        n_features = X_train.shape[1]
        X_train = X_train.reshape(X_train.shape[0], time_steps, n_features // time_steps, 1, 1)
        X_test = X_test.reshape(X_test.shape[0], time_steps, n_features // time_steps, 1, 1)

        model = create_3d_cnn_lstm_model((time_steps, X_train.shape[2], 1, 1), y_train.shape[1])

    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_test_classes = np.argmax(y_test, axis=1)

    cm = confusion_matrix(y_test_classes, y_pred_classes)

    return cm

# KarmaÅŸÄ±klÄ±k Matrisini GÃ¶rselleÅŸtirme
def plot_confusion_matrix(cm, model_name, dataset_name):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(cm.shape[0]), yticklabels=np.arange(cm.shape[0]))
    plt.title(f'Confusion Matrix - {model_name} on {dataset_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

def main():
    # Veri yollarÄ±
    paths = [
        "/content/drive/MyDrive/data_with_lda_train.csv",
        "/content/drive/MyDrive/data_with_lda_test.csv",
        "/content/drive/MyDrive/5_data_balanced.csv",
        "/content/drive/MyDrive/data_with_pca_normalized.csv"
    ]

    # Her model iÃ§in karmaÅŸÄ±klÄ±k matrislerini toplamak
    confusion_matrices = {model: None for model in ["3D_CNN_LSTM"]}

    # Her veri seti ile iÅŸlem yapÄ±lacak
    for path in paths:
        data = pd.read_csv(path)

        # Etiketler ve Ã–zellikler
        labels = data.iloc[:, -1].values
        features = data.iloc[:, :-1].values

        print(f"Veri seti: {path}")
        print(f"Etiketlerin ilk 5 deÄŸeri: {labels[:5]}")
        print(f"Ã–zelliklerin boyutu: {features.shape}")

        # K-Fold Ã‡apraz DoÄŸrulama iÃ§in StratifiedKFold
        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        fold = 1
        for train_idx, test_idx in kfold.split(features, labels):
            print(f"Fold {fold} BaÅŸlÄ±yor...")

            # EÄŸitim ve Test Verilerini AyÄ±rma
            X_train, X_test = features[train_idx], features[test_idx]
            y_train, y_test = labels[train_idx], labels[test_idx]

            # Veriyi StandartlaÅŸtÄ±rma
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)

            y_train = to_categorical(y_train)
            y_test = to_categorical(y_test)

            # Zaman adÄ±mlarÄ±na gÃ¶re ÅŸekillendirme
            time_steps = 1
            n_features = X_train.shape[1]
            X_train = X_train.reshape(X_train.shape[0], time_steps, n_features // time_steps, 1, 1)
            X_test = X_test.reshape(X_test.shape[0], time_steps, n_features // time_steps, 1, 1)

            # Modeli oluÅŸturma
            model = create_3d_cnn_lstm_model((time_steps, X_train.shape[2], 1, 1), y_train.shape[1])

            # Modeli eÄŸitme
            model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

            # Tahminler
            y_pred = model.predict(X_test)
            y_pred_classes = np.argmax(y_pred, axis=1)
            y_test_classes = np.argmax(y_test, axis=1)

            # KarmaÅŸÄ±klÄ±k Matrisi
            cm = confusion_matrix(y_test_classes, y_pred_classes)

            if confusion_matrices["3D_CNN_LSTM"] is None:
                confusion_matrices["3D_CNN_LSTM"] = cm
            else:
                confusion_matrices["3D_CNN_LSTM"] += cm

            fold += 1

    # KarmaÅŸÄ±klÄ±k Matrislerinin GÃ¶rselleÅŸtirilmesi
    for model_name, cm in confusion_matrices.items():
        print(f"{model_name} Modeli iÃ§in Toplam KarmaÅŸÄ±klÄ±k Matrisi:")
        plot_confusion_matrix(cm, model_name, "All Datasets")

if __name__ == "__main__":
    main()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Dosya yollarÄ±
lda_file = '/content/drive/MyDrive/hibrit_lda_kfold_results.csv'
no_method_file = '/content/drive/MyDrive/hibrit_no_kfold_results.csv'
pca_file = '/content/drive/MyDrive/hibrit_pca_kfold_results.csv'

# DosyalarÄ± yÃ¼kle
lda_results = pd.read_csv(lda_file)
no_method_results = pd.read_csv(no_method_file)
pca_results = pd.read_csv(pca_file)

# Verilerin sÃ¼tun isimlerini kontrol et
print("LDA Results Columns: ", lda_results.columns)
print("PCA Results Columns: ", pca_results.columns)
print("No Method Results Columns: ", no_method_results.columns)

# Performans metrikleri isimlerini belirliyoruz (gerekirse doÄŸru isimlerle gÃ¼ncellenmiÅŸ)
lda_metrics = ['Accuracy', 'F1 Score', 'Sensitivity', 'Specificity', 'AUC ROC']
pca_metrics = ['Accuracy', 'F1 Score', 'Sensitivity', 'Specificity', 'AUC ROC']
no_method_metrics = ['Accuracy', 'F1 Score', 'Sensitivity', 'Specificity', 'AUC ROC']

# LDA ve PCA sonuÃ§larÄ±nÄ± uygun ÅŸekilde alÄ±yoruz (sÃ¼tun isimleri doÄŸruysa)
lda_means = lda_results[lda_metrics].mean().values if all(metric in lda_results.columns for metric in lda_metrics) else np.nan
pca_means = pca_results[pca_metrics].mean().values if all(metric in pca_results.columns for metric in pca_metrics) else np.nan

# No Method verisini 'Metric' ve 'Score' kolonlarÄ±na gÃ¶re dÃ¼zenliyoruz
# 'Metric' ve 'Score' sÃ¼tunlarÄ± yerine doÄŸrudan metriklerin adlarÄ±nÄ± kullanÄ±yoruz
no_method_means = []
for metric in lda_metrics:  # 'AUC ROC' hariÃ§
    if metric in no_method_results.columns:
        no_method_means.append(no_method_results[metric].mean())
    else:
        no_method_means.append(np.nan)

# Bar grafiÄŸi iÃ§in dÃ¼zenleme
x = np.arange(len(lda_metrics))  # Metriklerin konumu
width = 0.25  # BarlarÄ±n geniÅŸliÄŸi

# Grafik oluÅŸturma
fig, ax = plt.subplots(figsize=(10, 6))

# Barlar
ax.bar(x - width, pca_means, width, label='PCA')
ax.bar(x, lda_means, width, label='LDA')
ax.bar(x + width, no_method_means, width, label='No Method')

# GrafiÄŸi Ã¶zelleÅŸtirme
ax.set_xlabel('Perfrmans Metrikleri')
ax.set_ylabel('Skorlar')
ax.set_title('PCA, LDA ve PCA-LDA KullanÄ±lmadan PerformanslarÄ±n KarÅŸÄ±laÅŸÄ±rÄ±lmasÄ±')
ax.set_xticks(x)
ax.set_xticklabels(lda_metrics)
ax.legend()

# GrafiÄŸi gÃ¶sterme
plt.tight_layout()
plt.show()

# Ortalama deÄŸerleri hesapla
lda_avg = lda_results[lda_metrics].mean()
pca_avg = pca_results[pca_metrics].mean()
no_method_avg = no_method_results[no_method_metrics].mean()

# SonuÃ§larÄ± birleÅŸtir
results_df = pd.DataFrame({
    'Metric': lda_metrics,
    'PCA': pca_avg.values,
    'LDA': lda_avg.values,
    'No Method': no_method_avg.values
})

# SonuÃ§larÄ± yeni bir CSV dosyasÄ±na kaydet
results_df.to_csv('/content/drive/MyDrive/hibrit_combined_performance_results.csv', index=False)

# SonuÃ§larÄ± gÃ¶ster
print("Combined Results Saved:")
print(results_df)

"""# EÄÄ°TÄ°M VE TEST VERÄ°LERÄ°NÄ°N PERFORMANS DEÄERLENDÄ°RMESÄ°"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape

# Veri YÃ¼kleme
data_pca = pd.read_csv("/content/drive/MyDrive/data_with_pca_normalized.csv")
data_lda_train = pd.read_csv("/content/drive/MyDrive/data_with_lda_train.csv")
data_lda_test = pd.read_csv("/content/drive/MyDrive/data_with_lda_test.csv")
data_no_transform = pd.read_csv("/content/drive/MyDrive/5_data_balanced.csv")

# Veriyi 3D CNN iÃ§in Yeniden Åekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    print(f"Ã–zellik SayÄ±sÄ±: {num_features}, Time_steps: {time_steps}")

    # time_steps deÄŸerini, Ã¶zellik sayÄ±sÄ±na gÃ¶re ayarlama
    if num_features < time_steps:
        time_steps = num_features  # EÄŸer Ã¶zellik sayÄ±sÄ± time_steps'ten kÃ¼Ã§Ã¼kse, time_steps'i Ã¶zellik sayÄ±sÄ±na eÅŸitle
        print(f"Ã–zellik sayÄ±sÄ± zaman adÄ±mÄ± sayÄ±sÄ±ndan kÃ¼Ã§Ã¼k. time_steps deÄŸeri {time_steps} olarak ayarlandÄ±.")

    if num_features % time_steps != 0:
        new_features = (num_features // time_steps) * time_steps
        print(f"FazlalÄ±k Ã¶zellikler atÄ±lacak. Yeni Ã¶zellik sayÄ±sÄ±: {new_features}")
        X = X[:, :new_features]

    features_per_step = X.shape[1] // time_steps

    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model TanÄ±mlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same',
               input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(2, 2, 1), padding='same'),
        Flatten(),
        Reshape((time_steps, -1)),
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# K-Fold Ã‡apraz DoÄŸrulama
def cross_validation(X, y, time_steps, model_name):
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    all_histories = []
    accuracy_scores = []

    for train_idx, test_idx in kfold.split(X, y):
        # EÄŸitim ve Test Verilerini AyÄ±rma
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        # Veriyi Yeniden Åekillendirme
        X_train = reshape_for_3d_cnn(X_train, time_steps)
        X_test = reshape_for_3d_cnn(X_test, time_steps)

        # Modeli OluÅŸturma
        model = create_hybrid_model(time_steps, X_train.shape[2])

        # Modeli EÄŸitme
        history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=0)

        # GeÃ§miÅŸi Saklama
        all_histories.append(history.history)

        # Test Seti ile DoÄŸruluk Hesaplama
        y_pred = np.argmax(model.predict(X_test), axis=1)
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

    # Ortalama DoÄŸruluk
    avg_accuracy = np.mean(accuracy_scores)
    print(f'{model_name} Ortalama DoÄŸruluk: {avg_accuracy:.4f}')

    return avg_accuracy, all_histories

# DoÄŸruluklarÄ±n Tablosunu ve GrafiÄŸini Ã‡izme
def plot_accuracy_graph(title, accuracies, histories):
    # Her model iÃ§in doÄŸruluklarÄ± Ã§izme
    plt.figure(figsize=(10, 6))
    for model_name, (accuracy, history) in accuracies.items():
        # Epoch baÅŸÄ±na eÄŸitim ve test doÄŸruluÄŸu
        epochs = range(1, len(history[0]['accuracy']) + 1)
        avg_train_accuracy = np.mean([h['accuracy'] for h in history], axis=0)
        avg_val_accuracy = np.mean([h['val_accuracy'] for h in history], axis=0)

        # Grafik
        plt.plot(epochs, avg_train_accuracy, label=f'{model_name} EÄŸitim DoÄŸruluÄŸu')
        plt.plot(epochs, avg_val_accuracy, label=f'{model_name} Test DoÄŸruluÄŸu')

    plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel('DoÄŸruluk')
    plt.legend()
    plt.grid(True)
    plt.show()

# Veriyi YÃ¼kleme
X_pca = data_pca.iloc[:, :-1].values
y_pca = data_pca.iloc[:, -1].values
X_lda_train = data_lda_train.iloc[:, :-1].values
y_lda_train = data_lda_train.iloc[:, -1].values
X_lda_test = data_lda_test.iloc[:, :-1].values
y_lda_test = data_lda_test.iloc[:, -1].values
X_no_transform = data_no_transform.iloc[:, :-1].values
y_no_transform = data_no_transform.iloc[:, -1].values

# Zaman adÄ±mÄ±nÄ± Ã¶zellik sayÄ±sÄ±na gÃ¶re uyarlama
time_steps_pca = min(X_pca.shape[1], 10)
time_steps_lda = min(X_lda_train.shape[1], 10)
time_steps_no_transform = min(X_no_transform.shape[1], 10)

# PCA ile eÄŸitim ve doÄŸruluk
accuracy_pca, history_pca = cross_validation(X_pca, y_pca, time_steps_pca, "PCA")

# LDA ile eÄŸitim ve doÄŸruluk
accuracy_lda, history_lda = cross_validation(X_lda_train, y_lda_train, time_steps_lda, "LDA")

# LDA ve PCA uygulanmamÄ±ÅŸ verilerle eÄŸitim ve doÄŸruluk
accuracy_no_transform, history_no_transform = cross_validation(X_no_transform, y_no_transform, time_steps_no_transform, "No Transformation")

# SonuÃ§larÄ± Grafikle GÃ¶sterme
accuracies = {
    'PCA': (accuracy_pca, history_pca),
    'LDA': (accuracy_lda, history_lda),
    'No Transformation': (accuracy_no_transform, history_no_transform)
}

plot_accuracy_graph('EÄŸitim ve Test DoÄŸruluÄŸu KarÅŸÄ±laÅŸtÄ±rmasÄ±', accuracies, {
    'PCA': history_pca,
    'LDA': history_lda,
    'No Transformation': history_no_transform
})

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape
from sklearn.metrics import accuracy_score

# Veri YÃ¼kleme
data_pca = pd.read_csv("/content/drive/MyDrive/data_with_pca_normalized.csv")
data_lda_train = pd.read_csv("/content/drive/MyDrive/data_with_lda_train.csv")
data_lda_test = pd.read_csv("/content/drive/MyDrive/data_with_lda_test.csv")
data_no_transform = pd.read_csv("/content/drive/MyDrive/5_data_balanced.csv")

# Veriyi 3D CNN iÃ§in Yeniden Åekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    print(f"Ã–zellik SayÄ±sÄ±: {num_features}, Time_steps: {time_steps}")

    # time_steps deÄŸerini, Ã¶zellik sayÄ±sÄ±na gÃ¶re ayarlama
    if num_features < time_steps:
        time_steps = num_features  # EÄŸer Ã¶zellik sayÄ±sÄ± time_steps'ten kÃ¼Ã§Ã¼kse, time_steps'i Ã¶zellik sayÄ±sÄ±na eÅŸitle
        print(f"Ã–zellik sayÄ±sÄ± zaman adÄ±mÄ± sayÄ±sÄ±ndan kÃ¼Ã§Ã¼k. time_steps deÄŸeri {time_steps} olarak ayarlandÄ±.")

    if num_features % time_steps != 0:
        new_features = (num_features // time_steps) * time_steps
        print(f"FazlalÄ±k Ã¶zellikler atÄ±lacak. Yeni Ã¶zellik sayÄ±sÄ±: {new_features}")
        X = X[:, :new_features]

    features_per_step = X.shape[1] // time_steps

    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model TanÄ±mlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same',
               input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(2, 2, 1), padding='same'),  # Padding'i 'same' olarak ayarladÄ±k
        Flatten(),
        Reshape((time_steps, -1)),
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# EÄŸitim ve Test DoÄŸruluklarÄ±nÄ±n ToplanmasÄ±
def plot_combined_history(all_histories, title):
    epochs = range(1, len(all_histories[0]['accuracy']) + 1)
    avg_train_accuracy = np.mean([history['accuracy'] for history in all_histories], axis=0)
    avg_val_accuracy = np.mean([history['val_accuracy'] for history in all_histories], axis=0)
    avg_train_loss = np.mean([history['loss'] for history in all_histories], axis=0)
    avg_val_loss = np.mean([history['val_loss'] for history in all_histories], axis=0)

    # DoÄŸruluk GrafiÄŸi
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, avg_train_accuracy, label='Ortalama EÄŸitim DoÄŸruluÄŸu', color='blue')
    plt.plot(epochs, avg_val_accuracy, label='Ortalama Test DoÄŸruluÄŸu', color='orange')
    plt.title(f'{title} - EÄŸitim ve Test DoÄŸruluklarÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rmasÄ±')
    plt.xlabel('Epochs')
    plt.ylabel('DoÄŸruluk')
    plt.legend()
    plt.grid()
    plt.show()

    # KayÄ±p GrafiÄŸi
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, avg_train_loss, label='Ortalama EÄŸitim KaybÄ±', color='blue')
    plt.plot(epochs, avg_val_loss, label='Ortalama Test KaybÄ±', color='orange')
    plt.title(f'{title} - EÄŸitim ve Test KayÄ±plarÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rmasÄ±')
    plt.xlabel('Epochs')
    plt.ylabel('KayÄ±p')
    plt.legend()
    plt.grid()
    plt.show()

# K-Fold Ã‡apraz DoÄŸrulama
def cross_validation(X, y, time_steps, model_name):
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    all_histories = []
    fold = 1
    accuracy_scores = []
    train_accuracies = []
    val_accuracies = []

    for train_idx, test_idx in kfold.split(X, y):
        print(f"Fold {fold} BaÅŸlÄ±yor...")

        # EÄŸitim ve Test Verilerini AyÄ±rma
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        # Veriyi Yeniden Åekillendirme
        X_train = reshape_for_3d_cnn(X_train, time_steps)
        X_test = reshape_for_3d_cnn(X_test, time_steps)

        # Modeli OluÅŸturma
        model = create_hybrid_model(time_steps, X_train.shape[2])

        # Modeli EÄŸitme
        history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

        # GeÃ§miÅŸi Saklama
        all_histories.append(history.history)

        # EÄŸitim ve Test doÄŸruluklarÄ±nÄ± kaydetme
        train_accuracies.append(np.mean(history.history['accuracy']))
        val_accuracies.append(np.mean(history.history['val_accuracy']))

        # Test Seti ile DoÄŸruluk Hesaplama
        y_pred = np.argmax(model.predict(X_test), axis=1)
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

        fold += 1

    # EÄŸitim ve Test DoÄŸruluklarÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±
    plot_combined_history(all_histories, model_name)

    # Ortalama DoÄŸruluk
    print(f'{model_name} Ortalama DoÄŸruluk: {np.mean(accuracy_scores):.4f}')
    return accuracy_scores, train_accuracies, val_accuracies

# Veriyi YÃ¼kleme
X_pca = data_pca.iloc[:, :-1].values
y_pca = data_pca.iloc[:, -1].values
X_lda_train = data_lda_train.iloc[:, :-1].values
y_lda_train = data_lda_train.iloc[:, -1].values
X_lda_test = data_lda_test.iloc[:, :-1].values
y_lda_test = data_lda_test.iloc[:, -1].values
X_no_transform = data_no_transform.iloc[:, :-1].values
y_no_transform = data_no_transform.iloc[:, -1].values

# Zaman adÄ±mÄ±nÄ± Ã¶zellik sayÄ±sÄ±na gÃ¶re uyarlama (Ã¶zellik sayÄ±sÄ±na bakarak en uygun time_steps deÄŸerini ayarlayacaÄŸÄ±z)
time_steps_pca = min(X_pca.shape[1], 10)
time_steps_lda = min(X_lda_train.shape[1], 10)
time_steps_no_transform = min(X_no_transform.shape[1], 10)

# PCA ile eÄŸitim ve doÄŸruluk
accuracy_pca, train_acc_pca, val_acc_pca = cross_validation(X_pca, y_pca, time_steps_pca, "PCA")

# LDA ile eÄŸitim ve doÄŸruluk
accuracy_lda, train_acc_lda, val_acc_lda = cross_validation(X_lda_train, y_lda_train, time_steps_lda, "LDA")

# LDA ve PCA uygulanmamÄ±ÅŸ verilerle eÄŸitim ve doÄŸruluk
accuracy_no_transform, train_acc_no_transform, val_acc_no_transform = cross_validation(X_no_transform, y_no_transform, time_steps_no_transform, "No Transformation")

# DoÄŸruluklarÄ±n Tablosu
results_table = pd.DataFrame({
    'PCA Accuracy': accuracy_pca,
    'LDA Accuracy': accuracy_lda,
    'No Transformation Accuracy': accuracy_no_transform,
    'PCA Train Accuracy': train_acc_pca,
    'PCA Val Accuracy': val_acc_pca,
    'LDA Train Accuracy': train_acc_lda,
    'LDA Val Accuracy': val_acc_lda,
    'No Transform Train Accuracy': train_acc_no_transform,
    'No Transform Val Accuracy': val_acc_no_transform
})

print("DoÄŸruluklar Tablosu:")
print(results_table)

# Bar Grafik - DoÄŸruluklarÄ± GÃ¶rselleÅŸtirme
model_names = ['PCA', 'LDA', 'No Transformation']
mean_accuracy = [np.mean(accuracy_pca), np.mean(accuracy_lda), np.mean(accuracy_no_transform)]

# Ã‡izgi grafiÄŸi ve bar grafiÄŸi aynÄ± anda oluÅŸturma
plt.figure(figsize=(12, 6))

# Bar grafiÄŸi
plt.subplot(1, 2, 1)  # (satÄ±r, sÃ¼tun, aktif grafik)
plt.bar(model_names, mean_accuracy, color=['blue', 'orange', 'green'])
plt.title('Ortalama DoÄŸruluklar (Bar GrafiÄŸi)')
plt.xlabel('Model')
plt.ylabel('Ortalama DoÄŸruluk')
plt.ylim(0, 1)
plt.grid(True)

# Ã‡izgi grafiÄŸi
plt.subplot(1, 2, 2)
epochs = range(1, len(accuracy_pca) + 1)
plt.plot(epochs, accuracy_pca, label='PCA', color='blue')
plt.plot(epochs, accuracy_lda, label='LDA', color='orange')
plt.plot(epochs, accuracy_no_transform, label='No Transformation', color='green')
plt.title('DoÄŸruluklarÄ±n Epoch BazÄ±nda KarÅŸÄ±laÅŸtÄ±rmasÄ±')
plt.xlabel('Epochs')
plt.ylabel('DoÄŸruluk')
plt.legend()
plt.grid(True)

# Grafikleri gÃ¶sterme
plt.tight_layout()
plt.show()

"""# ZAMAN VE KAYNAK KULLANIMI DEÄERLENDÄ°RMESÄ°"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
import time

# Veri dosyalarÄ±
pca_path = "/content/drive/MyDrive/data_with_pca_normalized.csv"
lda_train_path = "/content/drive/MyDrive/data_with_lda_train.csv"
lda_test_path = "/content/drive/MyDrive/data_with_lda_test.csv"
ham_data_path = "/content/drive/MyDrive/5_data_balanced.csv"

# 1. PCA uygulanmÄ±ÅŸ veriyi yÃ¼kleme
data_pca = pd.read_csv(pca_path)
X_pca = data_pca.drop(columns=['label'])
y_pca = data_pca['label'].astype(int)

# 2. LDA uygulanmÄ±ÅŸ veriyi yÃ¼kleme
lda_train = pd.read_csv(lda_train_path)
lda_test = pd.read_csv(lda_test_path)
X_lda = pd.concat([lda_train.drop(columns=['label']), lda_test.drop(columns=['label'])])
y_lda = pd.concat([lda_train['label'], lda_test['label']]).astype(int)

# 3. PCA ve LDA uygulanmamÄ±ÅŸ veriyi yÃ¼kleme
ham_data = pd.read_csv(ham_data_path)
X_ham = ham_data.drop(columns=['label'])
y_ham = ham_data['label'].astype(int)

# Modeller
models = {
    "SVM": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "GBM": GradientBoostingClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Performans metrikleri hesaplama fonksiyonu (EÄŸitim ve test sÃ¼relerini Ã¶lÃ§me)
def evaluate_model(X, y, model_name):
    results = []
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    for name, model in models.items():
        fold_train_times = []
        fold_test_times = []

        for train_index, val_index in cv.split(X, y):
            X_train, X_val = X.iloc[train_index], X.iloc[val_index]
            y_train, y_val = y.iloc[train_index], y.iloc[val_index]

            # Ã–zellik standardizasyonu
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_val = scaler.transform(X_val)

            # EÄŸitim sÃ¼resi
            start_train_time = time.time()
            model.fit(X_train, y_train)
            end_train_time = time.time()

            # Test sÃ¼resi
            start_test_time = time.time()
            model.predict(X_val)
            end_test_time = time.time()

            # EÄŸitim ve test sÃ¼relerini kaydetme
            fold_train_times.append(end_train_time - start_train_time)
            fold_test_times.append(end_test_time - start_test_time)

        # Ortalama eÄŸitim ve test sÃ¼relerini hesapla
        results.append({
            'Model': name,
            'Average Train Time (s)': np.mean(fold_train_times),
            'Average Test Time (s)': np.mean(fold_test_times),
            'Dataset': model_name
        })

    return results

# TÃ¼m veri setleri iÃ§in eÄŸitim ve test sÃ¼relerini alma
train_test_times_pca = evaluate_model(X_pca, y_pca, "PCA")
train_test_times_lda = evaluate_model(X_lda, y_lda, "LDA")
train_test_times_ham = evaluate_model(X_ham, y_ham, "Ham")

# SonuÃ§larÄ± birleÅŸtirme
all_train_test_times = pd.DataFrame(train_test_times_pca + train_test_times_lda + train_test_times_ham)

# SonuÃ§larÄ± CSV olarak kaydetme
all_train_test_times.to_csv('/content/drive/MyDrive/ml_train_test_times.csv', index=False)

# EÄŸitim ve test sÃ¼relerini yazdÄ±rma
print(all_train_test_times)

import time
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Dropout, Flatten
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli
def create_dbn_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Autoencoder Modeli
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli EÄŸitme ve Test Etme
def train_and_predict_kfold(features, labels, model_name, input_dim, input_shape=None, n_splits=5):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    train_times = []
    test_times = []

    for train_index, test_index in skf.split(features, labels):
        X_train, X_test = features[train_index], features[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        y_train = to_categorical(y_train)
        y_test = to_categorical(y_test)

        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        # EÄŸitim SÃ¼resi
        start_train = time.time()
        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)
        end_train = time.time()
        train_times.append(end_train - start_train)

        # Test SÃ¼resi
        start_test = time.time()
        model.evaluate(X_test, y_test, verbose=0)
        end_test = time.time()
        test_times.append(end_test - start_test)

    # Ortalama EÄŸitim ve Test SÃ¼releri
    avg_train_time = np.mean(train_times)
    avg_test_time = np.mean(test_times)

    return avg_train_time, avg_test_time

def main():
    # Veri yollarÄ± ve Ã¶znitelik Ã§Ä±karma yÃ¶ntemleri
    paths = [
        ("/content/drive/MyDrive/data_with_lda_train.csv", "LDA"),
        ("/content/drive/MyDrive/data_with_pca_normalized.csv", "PCA"),
        ("/content/drive/MyDrive/5_data_balanced.csv", "Balanced"),
        ("/content/drive/MyDrive/data_with_lda_test.csv", "LDA Test")
    ]

    # Her model iÃ§in sÃ¼relerin toplanacaÄŸÄ± sÃ¶zlÃ¼k
    train_times_dict = {model: [] for model in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]}
    test_times_dict = {model: [] for model in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]}

    # Her veri seti ile iÅŸlem yapÄ±lacak
    for path, method in paths:
        data = pd.read_csv(path)

        # Etiketler ve Ã–zellikler
        labels = data.iloc[:, -1].values
        features = data.iloc[:, :-1].values

        print(f"Veri seti: {path} - Ã–znitelik YÃ¶ntemi: {method}")
        print(f"Etiketlerin ilk 5 deÄŸeri: {labels[:5]}")
        print(f"Ã–zelliklerin boyutu: {features.shape}")

        # Her model iÃ§in eÄŸitim ve test sÃ¼relerini toplama
        for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
            avg_train_time, avg_test_time = train_and_predict_kfold(features, labels, model_name, features.shape[1])
            train_times_dict[model_name].append(avg_train_time)
            test_times_dict[model_name].append(avg_test_time)

    # EÄŸitim ve Test SÃ¼relerinin YazdÄ±rÄ±lmasÄ±
    for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
        print(f"\n{model_name} Modeli Ä°Ã§in Ortalama EÄŸitim ve Test SÃ¼releri:")
        for idx, (method, _) in enumerate(paths):
            print(f"  {method} - EÄŸitim SÃ¼resi: {train_times_dict[model_name][idx]:.4f} saniye")
            print(f"  {method} - Test SÃ¼resi: {test_times_dict[model_name][idx]:.4f} saniye")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Flatten, Conv3D
from keras.utils import to_categorical
import time

# 3D CNN+LSTM Modeli
def create_3d_cnn_lstm_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))
    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same'))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli EÄŸitme ve Test Etme
def train_and_predict(features, labels, model_name):
    # EÄŸitim ve test verilerini ayÄ±rma
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

    # Veriyi StandartlaÅŸtÄ±rma
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Kategorik hale getirme
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    # Modelin giriÅŸ ÅŸekli iÃ§in reshape iÅŸlemi
    time_steps = 1
    n_features = X_train.shape[1]
    X_train = X_train.reshape(X_train.shape[0], time_steps, n_features // time_steps, 1, 1)
    X_test = X_test.reshape(X_test.shape[0], time_steps, n_features // time_steps, 1, 1)

    model = create_3d_cnn_lstm_model((time_steps, X_train.shape[2], 1, 1), y_train.shape[1])

    # EÄŸitim SÃ¼resi
    start_time = time.time()
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)
    training_time = time.time() - start_time

    # Test SÃ¼resi
    start_time = time.time()
    y_pred = model.predict(X_test)
    prediction_time = time.time() - start_time

    # SonuÃ§larÄ± dÃ¶ndÃ¼rme
    return training_time, prediction_time

# EÄŸitim ve Tahmin SÃ¼relerini Toplama ve GÃ¶rselleÅŸtirme
def summarize_training_times(paths):
    results = []

    for path in paths:
        data = pd.read_csv(path)

        labels = data.iloc[:, -1].values
        features = data.iloc[:, :-1].values

        # Dosya adÄ± Ã¼zerinden PCA veya LDA etiketini belirleme
        if 'lda' in path.lower():
            dataset_label = 'LDA'
        elif 'pca' in path.lower():
            dataset_label = 'PCA'
        else:
            dataset_label = 'Original'  # Orijinal veri

        # Modeli orijinal veri ile Ã§alÄ±ÅŸtÄ±r
        train_time, pred_time = train_and_predict(features, labels, "3D_CNN_LSTM")

        # SonuÃ§larÄ± toplama
        results.append({
            "Model": "3D_CNN_LSTM",
            "Dataset": f"{dataset_label} - {path.split('/')[-1]}",
            "Training Time (s)": train_time,
            "Prediction Time (s)": pred_time
        })

    # SonuÃ§larÄ± DataFrame olarak dÃ¶ndÃ¼rme
    results_df = pd.DataFrame(results)
    return results_df

def main():
    # Veri yollarÄ±
    paths = [
        "/content/drive/MyDrive/data_with_lda_train.csv",
        "/content/drive/MyDrive/data_with_lda_test.csv",
        "/content/drive/MyDrive/5_data_balanced.csv",
        "/content/drive/MyDrive/data_with_pca_normalized.csv"
    ]

    # EÄŸitim ve tahmin sÃ¼relerini hesapla
    results_df = summarize_training_times(paths)

    # SonuÃ§larÄ± yazdÄ±r
    print(results_df)

    # SonuÃ§larÄ± CSV olarak kaydetme
    results_df.to_csv('/content/drive/MyDrive/model_training_results.csv', index=False)

if __name__ == "__main__":
    main()

"""# MODEL GENELLEMESÄ° VE K-FOLD GEÃ‡ERLEME SONUÃ‡LARI
Hibrit model iÃ§in PCA-LDA-NO_PCA_LDA dosyalarÄ±na uygulanacak her bir fold iÃ§in deÄŸerelndirme sonuÃ§larÄ±
> HÄ°BRÄ°T MODEL EÄÄ°TÄ°M VE PERFORMANSI
bÃ¶lÃ¼mÃ¼nde Ã¶nceden yapÄ±lmÄ±ÅŸtÄ±r.

# DOÄRULAMA VERÄ°SÄ° HAZIRLIÄI
"""

import cv2
import mediapipe as mp
import os
import pandas as pd

# MediaPipe Pose setup
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False)

# GÃ¶rsel dizinleri (Ana dizin)
image_paths = [
    "/content/drive/MyDrive/validation_veri_seti"  # GÃ¶rsellerin bulunduÄŸu ana dizin
]

# Ã‡Ä±ktÄ± dosyasÄ±
output_csv_path = "/content/drive/MyDrive/validation_dataset_skeleton_keypoints.csv"

# Ã‡Ä±ktÄ±lar iÃ§in bir liste
all_keypoints = []

# Her gÃ¶rsel dizini iÃ§in iÅŸlemler
for path in image_paths:
    for root, dirs, files in os.walk(path):  # Ana dizin ve alt dizinleri tarar
        for file in files:
            if file.endswith((".jpg", ".jpeg", ".png")):  # GÃ¶rsel dosyalarÄ±nÄ± iÅŸler
                image_path = os.path.join(root, file)  # GÃ¶rselin tam yolu
                print(f"Processing image: {image_path}")

                # GÃ¶rseli oku
                frame = cv2.imread(image_path)

                # BGR'den RGB'ye dÃ¶nÃ¼ÅŸtÃ¼r
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # Pose algÄ±lama
                results = pose.process(rgb_frame)

                # EÄŸer poz tespit edilmiÅŸse
                if results.pose_landmarks:
                    keypoints = []
                    for lm in results.pose_landmarks.landmark:
                        keypoints.append((lm.x, lm.y, lm.z))
                    all_keypoints.append({
                        "image_path": image_path,
                        "keypoints": keypoints
                    })

# TÃ¼m keypoint verilerini DataFrame'e Ã§evir ve kaydet
df = pd.DataFrame(all_keypoints)
df.to_csv(output_csv_path, index=False)
print(f"Keypoints saved to: {output_csv_path}")

import pandas as pd
import numpy as np
import ast

# CSV dosyasÄ±nÄ± yÃ¼kleyelim
file_path = "/content/drive/MyDrive/validation_dataset_skeleton_keypoints.csv"
data = pd.read_csv(file_path)

# 1. 'keypoints' sÃ¼tunundaki verilerin iÅŸlenmesi
def expand_keypoints(row):
    try:
        keypoints = ast.literal_eval(row)  # Keypoints sÃ¼tununu bir listeye dÃ¶nÃ¼ÅŸtÃ¼r
        flat_keypoints = [coord for point in keypoints for coord in point]  # x, y, z'yi birleÅŸtir
        return flat_keypoints
    except Exception as e:
        print(f"Hata oluÅŸtu: {e} -> {row}")  # Hata mesajÄ±nÄ± daha ayrÄ±ntÄ±lÄ± ÅŸekilde yazdÄ±ralÄ±m
        return [np.nan] * (num_keypoints * 3)  # Hata durumunda NaN deÄŸerleri dÃ¶ndÃ¼r

# keypoints sÃ¼tununun Ã¶rnek uzunluÄŸunu (keypoint sayÄ±sÄ±nÄ±) bulalÄ±m
num_keypoints = len(ast.literal_eval(str(data['keypoints'].iloc[0])))  # ilk satÄ±rdan Ã¶rnek al

# Anahtar noktalarÄ± sÃ¼tunlara ayÄ±rmak iÃ§in yeni sÃ¼tun isimleri oluÅŸturalÄ±m
keypoint_columns = [f"kp{i}_{axis}" for i in range(num_keypoints) for axis in ["x", "y", "z"]]

# 'keypoints' sÃ¼tununu dÃ¶nÃ¼ÅŸtÃ¼rÃ¼p yeni sÃ¼tunlar ekleyelim
expanded_keypoints = data['keypoints'].apply(expand_keypoints)

# Yeni sÃ¼tunlarÄ± oluÅŸturup, 'keypoints' sÃ¼tununu kaldÄ±rarak veriyi geniÅŸletelim
keypoints_df = pd.DataFrame(expanded_keypoints.tolist(), columns=keypoint_columns)
data_expanded = pd.concat([data.drop(columns=['keypoints']), keypoints_df], axis=1)

# Verinin tamamÄ±nÄ± kaydedelim
output_full_path = "/content/drive/MyDrive/validation_dataset_skeleton_keypoints_data_expanded_full.csv"
data_expanded.to_csv(output_full_path, index=False)

print(f"Veri baÅŸarÄ±yla kaydedildi: {output_full_path}")

import os
import json
import pandas as pd

# Function to parse COCO JSON files and extract relevant data
def parse_coco_json(json_file, folder_path, valid_category_ids):
    data = []
    with open(json_file, 'r') as file:
        annotations = json.load(file)

        # Extract image information
        images = {img['id']: img['file_name'] for img in annotations.get('images', [])}

        # Process annotations
        for annotation in annotations.get('annotations', []):
            image_id = annotation.get('image_id')
            category_id = annotation.get('category_id')
            bbox = annotation.get('bbox')

            # Validate category ID and bbox
            if category_id in valid_category_ids and bbox is not None:
                image_name = images.get(image_id)
                if image_name:
                    image_path = os.path.join(folder_path, image_name)

                    # Check if the image file exists
                    if os.path.exists(image_path):
                        data.append({
                            "image_path": image_path,
                            "category_id": category_id,
                            "bbox": bbox
                        })
                    else:
                        print(f"Warning: Image file {image_path} not found for JSON {json_file}")
    return data

# Define valid category IDs for backhand, forehand, and serve
valid_category_ids = [1, 2, 3]  # Replace with actual IDs

# Define paths to train, valid, and test folders
train_dir = "/content/drive/MyDrive/validation_veri_seti/test"
valid_dir = "/content/drive/MyDrive/validation_veri_seti/valid"
test_dir = "/content/drive/MyDrive/validation_veri_seti/test"

# Parse JSON files from each folder
train_json_file = os.path.join(train_dir, "_annotations.coco.json")
valid_json_file = os.path.join(valid_dir, "_annotations.coco.json")
test_json_file = os.path.join(test_dir, "_annotations.coco.json")

train_data = parse_coco_json(train_json_file, train_dir, valid_category_ids)
valid_data = parse_coco_json(valid_json_file, valid_dir, valid_category_ids)
test_data = parse_coco_json(test_json_file, test_dir, valid_category_ids)

# Convert parsed data to DataFrames
def create_dataframe(data, dataset_type):
    df = pd.DataFrame(data)
    df["dataset_type"] = dataset_type
    return df

train_df = create_dataframe(train_data, "train")
valid_df = create_dataframe(valid_data, "valid")
test_df = create_dataframe(test_data, "test")

# Combine all data into a single DataFrame
combined_df = pd.concat([train_df, valid_df, test_df], ignore_index=True)

# Save the combined dataset to a CSV file
output_csv_path = "/content/drive/MyDrive/test_validprepared_dataset.csv"
combined_df.to_csv(output_csv_path, index=False)

print(f"Dataset prepared and saved at {output_csv_path}")

import os
import json
import pandas as pd

# Function to parse COCO JSON files and extract relevant data
def parse_coco_json(json_file, folder_path, valid_category_ids):
    data = []
    with open(json_file, 'r') as file:
        annotations = json.load(file)

        # Extract image information
        images = {img['id']: img['file_name'] for img in annotations.get('images', [])}

        # Process annotations
        for annotation in annotations.get('annotations', []):
            image_id = annotation.get('image_id')
            category_id = annotation.get('category_id')
            bbox = annotation.get('bbox')

            # Update category ID mapping
            category_mapping = {1: 0, 2: 1, 3: 2}
            if category_id in category_mapping:
                category_id = category_mapping[category_id]

            # Validate category ID and bbox
            if category_id in valid_category_ids and bbox is not None:
                image_name = images.get(image_id)
                if image_name:
                    image_path = os.path.join(folder_path, image_name)

                    # Check if the image file exists
                    if os.path.exists(image_path):
                        data.append({
                            "image_path": image_path,
                            "category_id": category_id,
                            "bbox": bbox
                        })
                    else:
                        print(f"Warning: Image file {image_path} not found for JSON {json_file}")
    return data

# Define valid category IDs for backhand, forehand, and serve
valid_category_ids = [0, 1, 2]  # Updated to reflect new mapping

# Define paths to train, valid, and test folders
train_dir = "/content/drive/MyDrive/validation_veri_seti/test"
valid_dir = "/content/drive/MyDrive/validation_veri_seti/valid"
test_dir = "/content/drive/MyDrive/validation_veri_seti/test"


# Parse JSON files from each folder
train_json_file = os.path.join(train_dir, "_annotations.coco.json")
valid_json_file = os.path.join(valid_dir, "_annotations.coco.json")
test_json_file = os.path.join(test_dir, "_annotations.coco.json")

train_data = parse_coco_json(train_json_file, train_dir, valid_category_ids)
valid_data = parse_coco_json(valid_json_file, valid_dir, valid_category_ids)
test_data = parse_coco_json(test_json_file, test_dir, valid_category_ids)

# Convert parsed data to DataFrames
def create_dataframe(data, dataset_type):
    df = pd.DataFrame(data)
    df["dataset_type"] = dataset_type
    return df

train_df = create_dataframe(train_data, "train")
valid_df = create_dataframe(valid_data, "valid")
test_df = create_dataframe(test_data, "test")

# Combine all data into a single DataFrame
combined_df = pd.concat([train_df, valid_df, test_df], ignore_index=True)

# Save the combined dataset to a CSV file
output_csv_path = "/content/drive/MyDrive/test_validprepared_dataset_labeled.csv"
combined_df.to_csv(output_csv_path, index=False)

print(f"Dataset prepared and saved at {output_csv_path}")

import pandas as pd

# 1. DosyalarÄ± yÃ¼kleyelim
labeled_data_path = "/content/drive/MyDrive/test_validprepared_dataset_labeled.csv"
keypoints_data_path = "/content/drive/MyDrive/validation_dataset_skeleton_keypoints_data_expanded_full.csv"

labeled_data = pd.read_csv(labeled_data_path)
keypoints_data = pd.read_csv(keypoints_data_path)

# 2. image_path verilerini karÅŸÄ±laÅŸtÄ±ralÄ±m
labeled_image_paths = set(labeled_data['image_path'])
keypoints_image_paths = set(keypoints_data['image_path'])

# 3. EÅŸleÅŸen image_path'leri bulalÄ±m
matching_image_paths = labeled_image_paths.intersection(keypoints_image_paths)

# 4. EÅŸleÅŸen image_path'leri yazdÄ±ralÄ±m
print(f"EÅŸleÅŸen image_path sayÄ±sÄ±: {len(matching_image_paths)}")

# 5. EÅŸleÅŸen image_path'ler iÃ§in etiketleme iÅŸlemi yapalÄ±m
merged_data = pd.merge(keypoints_data, labeled_data[['image_path', 'category_id']], on='image_path', how='left')

# 6. Etiketleme iÅŸlemi sonrasÄ± veriyi kaydedelim
output_file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged.csv"
merged_data.to_csv(output_file_path, index=False)

# 7. Kaydedilen dosyanÄ±n iÃ§eriÄŸindeki veri sayÄ±sÄ±nÄ± yazdÄ±ralÄ±m
saved_data = pd.read_csv(output_file_path)
print(f"Kaydedilen dosyada toplam {saved_data.shape[0]} veri bulunuyor.")

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# 1. DosyayÄ± yÃ¼kleyelim
file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged.csv"
data = pd.read_csv(file_path)

# 2. SayÄ±sal Veriler Ãœzerinde BoÅŸ Verileri Doldurma
# SayÄ±sal sÃ¼tunlarda boÅŸ verileri medyan ile dolduralÄ±m
# 'category_id' ve 'image_path' sÃ¼tunlarÄ±nÄ± dÄ±ÅŸarÄ±da bÄ±rakÄ±yoruz
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
numerical_columns = [col for col in numerical_columns if col not in ['category_id', 'image_path']]
data[numerical_columns] = data[numerical_columns].fillna(data[numerical_columns].median())

# 3. Veriyi Ã–lÃ§eklendirme
# SayÄ±sal sÃ¼tunlarÄ± belirleyip, Min-Max Ã¶lÃ§eklendirmesi uygulayalÄ±m
scaler = MinMaxScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# 4. Tekrarlanan Ã–ÄŸeleri Silme
# AynÄ± satÄ±rlarÄ±n tekrarlayanlarÄ±nÄ± silelim
data.drop_duplicates(inplace=True)

# 5. category_id ve image_path sÃ¼tununu dÄ±ÅŸarÄ±da bÄ±rakÄ±p, geri kalan sÃ¼tunlarla iÅŸlemi sÃ¼rdÃ¼rdÃ¼k
output_file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_cleaned_no_balance.csv"
data.to_csv(output_file_path, index=False)

# 6. Kaydedilen dosyanÄ±n iÃ§eriÄŸindeki veri sayÄ±sÄ±nÄ± yazdÄ±ralÄ±m
saved_data = pd.read_csv(output_file_path)
print(f"TemizlenmiÅŸ dosyada toplam {saved_data.shape[0]} veri bulunuyor.")

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# 1. DosyayÄ± yÃ¼kleyelim
file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_cleaned_no_balance.csv"
data = pd.read_csv(file_path)

# 2. EtiketlenmiÅŸ ve etiketsiz verileri ayÄ±ralÄ±m
train_data = data.dropna(subset=['category_id'])  # EtiketlenmiÅŸ veriler
test_data = data[data['category_id'].isna()]  # Etiketsiz veriler

# Etiketsiz veri olup olmadÄ±ÄŸÄ±nÄ± kontrol edelim
if test_data.empty:
    print("Etiketsiz veri bulunmamaktadÄ±r.")
else:
    # Ã–zellikler (features) ve etiketleri ayÄ±ralÄ±m
    X_train = train_data.drop(columns=['category_id', 'image_path'])  # image_path dÄ±ÅŸarÄ±da bÄ±rakÄ±ldÄ±
    y_train = train_data['category_id'].astype(int)  # Kategorik hale getirmek iÃ§in int tÃ¼rÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼rme

    X_test = test_data.drop(columns=['category_id', 'image_path'])  # image_path dÄ±ÅŸarÄ±da bÄ±rakÄ±ldÄ±

    # 3. Modeli baÅŸlatalÄ±m ve eÄŸitelim
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    # 4. BoÅŸ etiketli veriler iÃ§in tahmin yapalÄ±m
    predicted_labels = model.predict(X_test)

    # 5. Test verisindeki boÅŸ etiketleri tahminlerle dolduralÄ±m
    test_data['category_id'] = predicted_labels

    # 6. SonuÃ§larÄ± birleÅŸtirip kaydedelim
    data.loc[data['category_id'].isna(), 'category_id'] = predicted_labels
    output_file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_filled.csv"
    data.to_csv(output_file_path, index=False)

    print(f"BoÅŸ etiketler baÅŸarÄ±yla tahmin edilip kaydedildi: {output_file_path}")

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import RandomOverSampler

# 1. DosyayÄ± yÃ¼kleyelim
file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_filled.csv"
data = pd.read_csv(file_path)

# 2. SayÄ±sal Veriler Ãœzerinde BoÅŸ Verileri Doldurma
# 'category_id' ve 'image_path' sÃ¼tunlarÄ±nÄ± dÄ±ÅŸarÄ±da bÄ±rakÄ±yoruz
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
numerical_columns = [col for col in numerical_columns if col not in ['category_id', 'image_path']]
data[numerical_columns] = data[numerical_columns].fillna(data[numerical_columns].median())

# 3. Veriyi Ã–lÃ§eklendirme
# SayÄ±sal sÃ¼tunlarÄ± belirleyip, Min-Max Ã¶lÃ§eklendirmesi uygulayalÄ±m
scaler = MinMaxScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# 4. Tekrarlanan Ã–ÄŸeleri Silme
# AynÄ± satÄ±rlarÄ±n tekrarlayanlarÄ±nÄ± silelim
data.drop_duplicates(inplace=True)

# 5. Veriyi Dengeleme (Over-sampling)
# 'category_id' hedef sÃ¼tunu (label) olarak alÄ±yoruz
X = data.drop(columns=['category_id', 'image_path'])
y = data['category_id']

# RandomOverSampler ile dengeleme iÅŸlemi yapÄ±yoruz
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

# Dengeleme sonrasÄ± yeni veriyi tekrar birleÅŸtiriyoruz
data_resampled = pd.DataFrame(X_resampled, columns=X.columns)
data_resampled['category_id'] = y_resampled
data_resampled['image_path'] = data['image_path'].iloc[:len(data_resampled)].reset_index(drop=True)

# 6. TemizlenmiÅŸ ve dengelenmiÅŸ veriyi kaydedelim
output_file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_cleaned_balanced.csv"
data_resampled.to_csv(output_file_path, index=False)

# 7. Kaydedilen dosyanÄ±n iÃ§eriÄŸindeki veri sayÄ±sÄ±nÄ± yazdÄ±ralÄ±m
saved_data = pd.read_csv(output_file_path)
print(f"DengelenmiÅŸ dosyada toplam {saved_data.shape[0]} veri bulunuyor.")

"""# MODEL GENELLEME YETENEÄÄ°"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# DosyayÄ± yÃ¼kle
file_path = '/content/drive/MyDrive/results_optimation.csv'
data = pd.read_csv(file_path)

# Ä°lk birkaÃ§ satÄ±ra gÃ¶z atalÄ±m
data.head(), data.info()

# Grafik boyutlarÄ±nÄ± ve genel stil ayarlarÄ±nÄ± belirleyin
import matplotlib.pyplot as plt
import seaborn as sns

# Grafik boyutlarÄ±nÄ± ve genel stil ayarlarÄ±nÄ± belirleyin
sns.set_theme(style="whitegrid")
plt.figure(figsize=(16, 10))

# Subplot 1: Epochs'e gÃ¶re Accuracy ve F1 Score
plt.subplot(2, 2, 1)
sns.lineplot(data=data, x="Epochs", y="Accuracy", label="Accuracy", marker="o", color="blue")
sns.lineplot(data=data, x="Epochs", y="F1 Score", label="F1 Score", marker="o", color="green")
plt.title("Accuracy & F1 Score vs Epochs", fontsize=14)
plt.xlabel("Epochs", fontsize=12)
plt.ylabel("Score", fontsize=12)
plt.legend()
plt.grid(True)

# Subplot 2: Learning Rate'e gÃ¶re AUC ROC (colorbar dÃ¼zeltmesiyle)
plt.subplot(2, 2, 2)
scatter = plt.scatter(
    data["Learning Rate"],
    data["AUC ROC"],
    c=data["Batch Size"],
    s=data["Epochs"] * 10,  # Boyutu Epochs ile iliÅŸkilendir
    cmap="viridis",
    edgecolor="k"
)
plt.title("AUC ROC vs Learning Rate", fontsize=14)
plt.xlabel("Learning Rate", fontsize=12)
plt.ylabel("AUC ROC", fontsize=12)
cbar = plt.colorbar(scatter)
cbar.set_label("Batch Size")
plt.grid(True)

# Subplot 3: Sensitivity ve Precision karÅŸÄ±laÅŸtÄ±rmasÄ±
plt.subplot(2, 2, 3)
sns.barplot(data=data, x="Batch Size", y="Sensitivity", hue="Epochs", palette="pastel", edgecolor="k")
plt.title("Sensitivity vs Batch Size", fontsize=14)
plt.xlabel("Batch Size", fontsize=12)
plt.ylabel("Sensitivity", fontsize=12)
plt.legend(title="Epochs")
plt.grid(True)

# Subplot 4: Precision ve Accuracy arasÄ±ndaki iliÅŸki
plt.subplot(2, 2, 4)
sns.regplot(data=data, x="Precision", y="Accuracy", scatter_kws={'color': 'purple'}, line_kws={'color': 'red'})
plt.title("Precision vs Accuracy", fontsize=14)
plt.xlabel("Precision", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)
plt.grid(True)

# Genel baÅŸlÄ±k ve gÃ¶sterim
plt.suptitle("Results Optimization Visualization", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# DosyayÄ± yÃ¼kle
file_path = '/content/drive/MyDrive/results_optimation.csv'
data = pd.read_csv(file_path)

# Genel stil ayarlarÄ±
sns.set_theme(style="whitegrid")

# 1. Grafik: Epochs'e gÃ¶re Accuracy ve F1 Score
plt.figure(figsize=(6, 4))
sns.lineplot(data=data, x="Epochs", y="Accuracy", label="Accuracy", marker="o", color="blue")
sns.lineplot(data=data, x="Epochs", y="F1 Score", label="F1 Score", marker="o", color="green")
plt.title("Accuracy & F1 Score vs Epochs")
plt.xlabel("Epochs")
plt.ylabel("Score")
plt.legend()
plt.grid(True)
plt.savefig("accuracy_f1_vs_epochs.png")  # Kaydet
plt.show()

# 2. Grafik: Learning Rate'e gÃ¶re AUC ROC
plt.figure(figsize=(6, 4))
scatter = plt.scatter(
    data["Learning Rate"],
    data["AUC ROC"],
    c=data["Batch Size"],
    s=data["Epochs"] * 10,
    cmap="viridis",
    edgecolor="k"
)
plt.title("AUC ROC vs Learning Rate")
plt.xlabel("Learning Rate")
plt.ylabel("AUC ROC")
cbar = plt.colorbar(scatter)
cbar.set_label("Batch Size")
plt.grid(True)
plt.savefig("auc_vs_learning_rate.png")  # Kaydet
plt.show()

# 3. Grafik: Sensitivity vs Batch Size
plt.figure(figsize=(6, 4))
sns.barplot(data=data, x="Batch Size", y="Sensitivity", hue="Epochs", palette="pastel", edgecolor="k")
plt.title("Sensitivity vs Batch Size")
plt.xlabel("Batch Size")
plt.ylabel("Sensitivity")
plt.legend(title="Epochs")
plt.grid(True)
plt.savefig("sensitivity_vs_batch.png")  # Kaydet
plt.show()

# 4. Grafik: Precision vs Accuracy
plt.figure(figsize=(6, 4))
sns.regplot(data=data, x="Precision", y="Accuracy", scatter_kws={'color': 'purple'}, line_kws={'color': 'red'})
plt.title("Precision vs Accuracy")
plt.xlabel("Precision")
plt.ylabel("Accuracy")
plt.grid(True)
plt.savefig("precision_vs_accuracy.png")  # Kaydet
plt.show()