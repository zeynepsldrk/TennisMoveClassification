# -*- coding: utf-8 -*-
"""Makale.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K68scpKmypTGZlJfNA-cYbIhp5Bkd3j7

# EĞİTİMLERE BAŞLAMADAN ÖNCE GEREKLİ VERİ HAZIRLIKLARI
"""

!pip install mediapipe

!pip install opencv-python mediapipe pandas

import cv2
import mediapipe as mp
import os
import pandas as pd
import numpy as np

# MediaPipe Pose setup
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False)

# Video dizinleri
video_paths = [
    "/content/drive/MyDrive/makale tüm dosyalar/makale pdfleri/dataset-main/VIDEO_RGB"
]

# Çıktı dosyası
output_csv_path = "/content/drive/MyDrive/Sskeleton_keypoints.csv"
output_npy_path = "/content/drive/MyDrive/Sskeleton_5d_keypoints.npy"  # 5 boyutlu veriyi kaydedeceğimiz dosya

# Çıktılar için bir liste
all_keypoints = []

# Her video dizini için işlemler
for path in video_paths:
    for root, dirs, files in os.walk(path):
        for file in files:
            if file.endswith(".avi"):  # Sadece video dosyalarını işler
                video_path = os.path.join(root, file)
                print(f"Processing video: {video_path}")

                cap = cv2.VideoCapture(video_path)

                frame_index = 0
                video_keypoints = []

                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break

                    # Yalnızca her 10. kareyi işle
                    if frame_index % 10 == 0:
                        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        results = pose.process(rgb_frame)

                        if results.pose_landmarks:
                            keypoints = []
                            for lm in results.pose_landmarks.landmark:
                                keypoints.append((lm.x, lm.y, lm.z))
                            video_keypoints.append({
                                "video_path": video_path,
                                "frame": frame_index,
                                "keypoints": keypoints
                            })

                    frame_index += 1

                cap.release()
                all_keypoints.extend(video_keypoints)

# Veriyi DataFrame'e çevir ve kaydet
df = pd.DataFrame(all_keypoints)
df.to_csv(output_csv_path, index=False)
print(f"Keypoints saved to: {output_csv_path}")

# 5. Adım: 5 Boyutlu Veriyi Hazırlama

# Veriyi NumPy array'e dönüştürme
num_keypoints = 33  # MediaPipe'den gelen 33 anahtar nokta
num_coordinates = 3  # x, y, z koordinatları
time_steps = 10  # Her 10. kareyi birleştiriyoruz

# Tüm keypoint'leri numpy array'e dönüştürme
X = np.array([video['keypoints'] for video in all_keypoints])  # all_keypoints'ten keypoint verisini al

# X array'inin boyutunu kontrol edelim
print("Original X shape:", X.shape)

# X array'inin boyutunu kontrol ettikten sonra zaman adımlarını uyarlayalım
num_frames = X.shape[0]  # Toplam kare sayısı

# Zaman adımlarını ve kareleri kontrol et
time_steps = 10  # İstediğiniz zaman adımı sayısını buradan değiştirebilirsiniz
frames_per_time_step = num_frames // time_steps

# Eğer zaman adımlarına göre tam bir bölünme yoksa, eksik olan kısmı keselim
X = X[:frames_per_time_step * time_steps]  # Zaman adımlarını tam hale getirmek için kesiyoruz

# X'i yeniden şekillendirelim
X_reshaped = X.reshape(-1, time_steps, num_keypoints, num_coordinates)

# 5 boyutlu veriyi kaydediyoruz
np.save(output_npy_path, X_reshaped)
print(f"5D Keypoints saved to: {output_npy_path}")


# 4. Adım: 5 Boyutlu Veriyi İşlemek ve Etiketlemek

# CSV dosyasını yükleyelim
data = pd.read_csv(output_csv_path)

# 1. 'keypoints' sütunundaki verilerin işlenmesi
def expand_keypoints(row):
    try:
        keypoints = ast.literal_eval(row)  # Keypoints sütununu bir listeye dönüştür
        flat_keypoints = [coord for point in keypoints for coord in point]  # x, y, z'yi birleştir
        return flat_keypoints
    except Exception as e:
        print(f"Hata oluştu: {e} -> {row}")  # Hata mesajını daha ayrıntılı şekilde yazdıralım
        return [np.nan] * (num_keypoints * 3)  # Hata durumunda NaN değerleri döndür

# keypoints sütununun örnek uzunluğunu (keypoint sayısını) bulalım
num_keypoints = len(ast.literal_eval(str(data['keypoints'].iloc[0])))  # ilk satırdan örnek al

# Anahtar noktaları sütunlara ayırmak için yeni sütun isimleri oluşturalım
keypoint_columns = [f"kp{i}_{axis}" for i in range(num_keypoints) for axis in ["x", "y", "z"]]

# 'keypoints' sütununu dönüştürüp yeni sütunlar ekleyelim
expanded_keypoints = data['keypoints'].apply(expand_keypoints)

# Yeni sütunları oluşturup, 'keypoints' sütununu kaldırarak veriyi genişletelim
keypoints_df = pd.DataFrame(expanded_keypoints.tolist(), columns=keypoint_columns)
data_expanded = pd.concat([data.drop(columns=['keypoints']), keypoints_df], axis=1)

# Verinin tamamını kaydedelim
output_full_path = "/content/drive/MyDrive/Sskeleton_data_expanded_full.csv"
data_expanded.to_csv(output_full_path, index=False)

print(f"Veri başarıyla kaydedildi: {output_full_path}")

# 5. Adım: Etiketleme

def label_data(file_path):
    # CSV dosyasını oku
    df = pd.read_csv(file_path)

    # 'video_path' sütununda 'backhand2h', 'serflat' ve 'foreflat' anahtar kelimelerine göre etiketleme yap
    df['label'] = df['video_path'].apply(lambda x: 0 if 'backhand2h' in x else (1 if 'serflat' in x else (2 if 'foreflat' in x else None)))

    # 'video_path' ve 'frame' sütunlarını kaldır
    df.drop(columns=['video_path', 'frame'], inplace=True)

    # Yeni dosyayı kaydet
    df.to_csv('/content/drive/MyDrive/Sskeleton_last_labeled_data.csv', index=False)
    print("Yeni etiketli dosya 'labeled_data.csv' olarak kaydedildi.")

# Dosya yolunu belirterek fonksiyonu çağır
label_data('/content/drive/MyDrive/Sskeleton_data_expanded_full.csv')

"""# Veri Arttırma"""

import pandas as pd

file_path = "/content/drive/MyDrive/Sskeleton_last_labeled_data.csv"
df = pd.read_csv(file_path)

print(df.columns)

df.columns = df.columns.str.strip()  # Sütun isimlerindeki boşlukları temizler
print(df.columns)

matching_columns = [col for col in df.columns if 'key' in col.lower()]
print(matching_columns)

import pandas as pd
import numpy as np

# Dosyanın yüklenmesi
file_path = "/content/drive/MyDrive/Sskeleton_last_labeled_data.csv"
df = pd.read_csv(file_path)

# Veri artırımı için fonksiyonlar
def add_noise(data, noise_level=0.02):
    """Veriye rastgele gürültü ekler."""
    noise = np.random.normal(0, noise_level, data.shape)
    return data + noise

def translate(data, shift=5):
    """Veriyi belirli bir miktarda öteleyerek artırır."""
    return data + shift

def flip_horizontal(data):
    """X koordinatlarını ters çevirerek yatay eksende aynalama yapar."""
    flipped_data = data.copy()
    flipped_data.iloc[:, ::2] = -flipped_data.iloc[:, ::2]  # X koordinatlarını ters çevir
    return flipped_data

# Sayısal veri sütunlarını al (etiket sütunu hariç)
data_columns = df.columns[:-1]
label_column = df.columns[-1]

# Orijinal veriyi kopyala
augmented_data = df.copy()

# Artırma sayısını 10 kat yapacak şekilde ayarla
augmentation_count = 9  # İlk veri orijinal olduğu için 9 kez daha artıracağız

# Veri artırımı işlemini bir döngüde yap
for _ in range(augmentation_count):
    # Gürültü ekleme
    noisy_data = df[data_columns].apply(add_noise)
    noisy_data[label_column] = df[label_column]  # Etiketleri koru

    # Öteleme
    translated_data = df[data_columns].apply(translate)
    translated_data[label_column] = df[label_column]

    # Aynalama
    flipped_data = flip_horizontal(df[data_columns])
    flipped_data[label_column] = df[label_column]

    # Yeni verileri birleştir
    augmented_data = pd.concat([augmented_data, noisy_data, translated_data, flipped_data], axis=0).reset_index(drop=True)

# Yeni veri setini kaydet
augmented_file_path = "/content/drive/MyDrive/Sskeleton_augmented_data.csv"
augmented_data.to_csv(augmented_file_path, index=False)

# Veri setinin yeni boyutunu yazdır
print(f"Veri artırımı tamamlandı. Yeni veri setinin satır sayısı: {augmented_data.shape[0]}")
print(f"Yeni dosya kaydedildi: {augmented_file_path}")

import pandas as pd

# Orijinal ve artırılmış veri setlerinin yolları
original_file_path = "/content/drive/MyDrive/Sskeleton_last_labeled_data.csv"
augmented_file_path = "/content/drive/MyDrive/Sskeleton_augmented_data.csv"

# Veri setlerini yükleyelim
original_df = pd.read_csv(original_file_path)
augmented_df = pd.read_csv(augmented_file_path)

# Etiket sütununun adını belirleyelim
label_column = original_df.columns[-1]  # Son sütunun etiket olduğunu varsayıyoruz

# Orijinal veri setindeki etiket dağılımı
original_label_counts = original_df[label_column].value_counts().sort_index()

# Artırılmış veri setindeki etiket dağılımı
augmented_label_counts = augmented_df[label_column].value_counts().sort_index()

# Etiketlerin oranlarını hesapla
original_ratios = original_label_counts / len(original_df)
augmented_ratios = augmented_label_counts / len(augmented_df)

# Sonuçları yazdır
print("Orijinal veri setindeki etiket dağılımı:")
print(original_label_counts)
print("\nArtırılmış veri setindeki etiket dağılımı:")
print(augmented_label_counts)

print("\nOrijinal veri setindeki etiket oranları:")
print(original_ratios)
print("\nArtırılmış veri setindeki etiket oranları:")
print(augmented_ratios)

# Farkları analiz et
difference = augmented_ratios - original_ratios
print("\nEtiket dağılım farkları:")
print(difference)

import pandas as pd


# CSV dosyasını yükleme
file_path = "/content/drive/MyDrive/Sskeleton_balanced_scaled_data.csv"
df = pd.read_csv(file_path)

#  Veri tiplerini yazdır
print("🔹 Veri Tipleri:")
print(df.dtypes)

#  Eksik veri sayısı
print("\n🔹 Eksik Veri Sayısı:")
print(df.isnull().sum())

#  Sınıf dağılımını yazdır (Son sütunun 'label' olduğunu varsayarak)
if 'label' in df.columns:
    print("\n🔹 Sınıf Dağılımı:")
    print(df['label'].value_counts())
else:
    print("\n 'label' sütunu bulunamadı!")

import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import MinMaxScaler

#  1. Dosyanın Yüklenmesi
file_path = "/content/drive/MyDrive/Sskeleton_augmented_data.csv"
df = pd.read_csv(file_path)

#  2. Özellik ve Etiketleri Ayırma
X = df.iloc[:, :-1].values  # Son sütun hariç tüm özellikler
y = df.iloc[:, -1].values   # Son sütun (etiket)

#  3. SMOTE ile Veri Dengeleme
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

#  4. Veriyi Ölçeklendirme (Etiketleri Hariç Tut)
scaler = MinMaxScaler()
X_resampled_scaled = scaler.fit_transform(X_resampled)

#  5. Dengelenmiş ve Ölçeklendirilmiş Veriyi DataFrame'e Çevirme
df_balanced = pd.DataFrame(X_resampled_scaled, columns=df.columns[:-1])
df_balanced["label"] = y_resampled  # Etiket sütununu ekle

# 6. Dengelenmiş Veriyi Kaydetme
balanced_file_path = "/content/drive/MyDrive/Sskeleton_balanced_data.csv"
df_balanced.to_csv(balanced_file_path, index=False)

print(f" SMOTE ile dengelenmiş ve ölçeklendirilmiş veri başarıyla kaydedildi: {balanced_file_path}")

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

#  1. Veriyi Yükleme
file_path = "/content/drive/MyDrive/Sskeleton_balanced_data.csv"
df = pd.read_csv(file_path)

#  2. Özellik ve Etiketleri Ayırma
X = df.iloc[:, :-1].values  # Son sütun hariç tüm özellikler
y = df.iloc[:, -1].values   # Son sütun (etiket)

#  3. MinMaxScaler ile Ölçeklendirme
scaler = MinMaxScaler(feature_range=(0, 1))
X_scaled = scaler.fit_transform(X)

#  4. Ölçeklendirilmiş Veriyi DataFrame'e Çevirme
df_scaled = pd.DataFrame(X_scaled, columns=df.columns[:-1])
df_scaled["label"] = y  # Etiket sütununu ekle

#  5. Ölçeklendirilmiş Veriyi Kaydetme
scaled_file_path = "/content/drive/MyDrive/Sskeleton_balanced_scaled_data.csv"
df_scaled.to_csv(scaled_file_path, index=False)

print(f" Ölçeklendirilmiş veri başarıyla kaydedildi: {scaled_file_path}")

import numpy as np
import pandas as pd
from google.colab import drive


#  2. Veri Setinin Yolunu Tanımlama
file_path = "/content/drive/MyDrive/Sskeleton_balanced_scaled_data.csv"

#  3. CSV Dosyasını Okuma
df = pd.read_csv(file_path)

#  4. Pencere Boyutu ve Özellik Sayısı
window_size = 33
num_joints = 33  # 33 eklem noktası
num_features = num_joints * 3  # Her eklem noktası için (x, y, z)

#  5. Veriyi NumPy Dizisine Çevirme
data = df.iloc[:, :-1].values  # Son sütun label, onu çıkardık
labels = df["label"].values  # Hareket etiketleri

#  6. Pencereleme (33 karelik pencere)
X_sequences = []
y_sequences = []

for i in range(len(data) - window_size):
    X_sequences.append(data[i : i + window_size])  # 33 karelik pencere
    y_sequences.append(labels[i + window_size - 1])  # Son kare etiketi

#  7. NumPy Dizilerine Çevirme
X_sequences = np.array(X_sequences)  # (num_samples, 33, 99)
y_sequences = np.array(y_sequences)  # (num_samples,)

#  8. 3D CNN için Yeniden Şekillendirme (num_samples, 33, 33, 3)
X_3D_CNN = X_sequences.reshape(-1, window_size, num_joints, 3)

#  9. Veriyi `.csv` formatında kaydetme
df_reshaped = X_3D_CNN.reshape(X_3D_CNN.shape[0], -1)  # 2D hale getir
df_final = pd.DataFrame(df_reshaped)
df_final["label"] = y_sequences  # Etiketleri ekle

# Google Drive içine kaydetme
csv_output_path = "/content/drive/MyDrive/Sskeleton_balanced_scaled_data_33frames.csv"
df_final.to_csv(csv_output_path, index=False)
print(f" CSV dosyası kaydedildi: {csv_output_path}")

#  10. Veriyi `.npy` formatında kaydetme
npy_output_X = "/content/drive/MyDrive/X_3D_CNN.npy"
npy_output_y = "/content/drive/MyDrive/y_labels.npy"

np.save(npy_output_X, X_3D_CNN)
np.save(npy_output_y, y_sequences)

print(f" NPY dosyaları kaydedildi:\n{npy_output_X}\n{npy_output_y}")

#  11. İşlemin Doğru Yapıldığını Kontrol Etme

# 1. Pencereleme ve etiketlerin eşleşip eşleşmediğini kontrol etme
print(f"Pencere sayısı: {len(X_sequences)}")
print(f"Etiket sayısı: {len(y_sequences)}")
assert len(X_sequences) == len(y_sequences), "Pencere sayısı ile etiket sayısı eşleşmiyor!"

# 2. 3D CNN için verinin doğru şekilde şekillendirildiğini kontrol etme
print(f"X_3D_CNN şekli: {X_3D_CNN.shape}")
assert X_3D_CNN.shape[1] == window_size, f"Pencere boyutu yanlış, beklenen: {window_size}, mevcut: {X_3D_CNN.shape[1]}"
assert X_3D_CNN.shape[2] == num_joints, f"Eklem sayısı yanlış, beklenen: {num_joints}, mevcut: {X_3D_CNN.shape[2]}"
assert X_3D_CNN.shape[3] == 3, "Koordinat sayısı (x, y, z) yanlış, beklenen: 3, mevcut: {X_3D_CNN.shape[3]}"

# 3. Verinin belirli bir örneğini kontrol etme
print(f"Örnek bir pencere (ilk pencere):\n{X_3D_CNN[0]}")  # İlk pencereyi yazdır
print(f"İlk etiket: {y_sequences[0]}")

# 4. CSV dosyasının doğru şekilde kaydedildiğini kontrol etme
loaded_csv = pd.read_csv(csv_output_path)
print(f"CSV dosyasının ilk satırları:\n{loaded_csv.head()}")

# 5. NPY dosyasının yüklenip doğru kaydedildiğini kontrol etme
loaded_X = np.load(npy_output_X)
loaded_y = np.load(npy_output_y)
print(f"Yüklenen X_3D_CNN şekli: {loaded_X.shape}")
print(f"Yüklenen etiket sayısı: {loaded_y.shape[0]}")

import numpy as np
import pandas as pd
from collections import Counter

# Etiketleri yükle
npy_output_y = "/content/drive/MyDrive/y_labels.npy"
y_sequences = np.load(npy_output_y)

# Sınıf dağılımını yazdır
label_counts = Counter(y_sequences)
print(" Sınıf Dağılımı:", label_counts)

import numpy as np

# Veriyi yükleme
X = np.load("/content/drive/MyDrive/X_3D_CNN_reshaped.npy")
y = np.load("/content/drive/MyDrive/y_labels_reshaped.npy")

# Verinin şekline bakalım
print(f"X shape: {X.shape}")  # (num_samples, 33, 33, 3)
print(f"y shape: {y.shape}")  # (num_samples,)

import numpy as np

# Veriyi yükleme
X = np.load("/content/drive/MyDrive/X_3D_CNN.npy")

# Mevcut boyutu al
current_shape = X.shape
expected_dims = ["batch_size", "time_steps", "height", "width", "channels"]

# Beklenen 5 boyuttan hangisinin eksik olduğunu kontrol et
if len(current_shape) == 4:
    print(f"Mevcut veri boyutu: {current_shape} (4D)")
    print("Eksik boyut: 'time_steps' (Zaman adımı)")

elif len(current_shape) == 5:
    print(f"Mevcut veri boyutu: {current_shape} (5D)")
    print("Veri zaten 3D CNN için uygun!")

else:
    print(f"Beklenmeyen veri boyutu: {current_shape}!")

import numpy as np

#  Veriyi yükle
X = np.load("/content/drive/MyDrive/X_3D_CNN.npy")
y = np.load("/content/drive/MyDrive/y_labels.npy")

#  Time steps değeri
time_steps = 10  # Her 10 kare bir zaman dizisi olarak gruplandırılacak

#  NumPy dizisinin boyutunu kontrol et
num_samples = X.shape[0]  # 16935
height, width, channels = X.shape[1], X.shape[2], X.shape[3]  # (33, 33, 3)

#  Time steps ile tam bölünüp bölünmediğini kontrol et
if num_samples % time_steps != 0:
    new_num_samples = (num_samples // time_steps) * time_steps
    X = X[:new_num_samples]  # Fazla olan kareleri at
    y = y[:new_num_samples]  # Etiketleri de aynı şekilde düzenle

#  5D forma dönüştürme (batch_size, time_steps, height, width, channels)
X_reshaped = X.reshape(-1, time_steps, height, width, channels)
y_reshaped = y.reshape(-1, time_steps)  # Etiketleri de zaman serisi halinde düzenle

#  Düzenlenmiş verilerin boyutunu yazdır
print(f"Yeni X shape: {X_reshaped.shape}")  # (num_samples/time_steps, time_steps, 33, 33, 3)
print(f"Yeni y shape: {y_reshaped.shape}")  # (num_samples/time_steps, time_steps)

#  Verileri kaydet
np.save("/content/drive/MyDrive/X_3D_CNN_reshaped.npy", X_reshaped)
np.save("/content/drive/MyDrive/y_labels_reshaped.npy", y_reshaped)

print(" Düzenlenmiş veriler başarıyla kaydedildi!")

"""# Hibrit model eğitimi"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Input, Conv3D, MaxPooling3D, BatchNormalization,
                                     Flatten, LSTM, Dense, Dropout, Reshape)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import KFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, confusion_matrix)
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# =============================================================================
# 1. VERİLERİN YÜKLENMESİ VE HAZIRLANMASI
# =============================================================================

# X verisini yükleyin: Beklenen boyut: (num_groups, time_steps, height, width, channels)
X = np.load("/content/drive/MyDrive/X_3D_CNN_reshaped.npy")
# y verisini yükleyin: İlk hali (num_groups, time_steps)
y = np.load("/content/drive/MyDrive/y_labels_reshaped.npy")

# Orijinal veri boyutlarını yazdır
print("Orijinal X boyutları:", X.shape)  # Örnek çıktı: (1693, 10, 33, 33, 3)
print("Orijinal y boyutları:", y.shape)  # Örnek çıktı: (1693, 10)

# Eğer y verisi her sekans için 10 etiket içeriyorsa, her sekansın
# son etiketini alarak y verisini (num_groups,) boyutuna indiriyoruz.
if y.ndim == 2 and y.shape[1] > 1:
    y = y[:, -1]
print("Dönüştürülmüş y boyutları:", y.shape)  # Beklenen çıktı: (1693,)

# =============================================================================
# 2. 3D MODEL MİMARİSİNİN OLUŞTURULMASI
# =============================================================================

def create_3d_model(input_shape, num_classes):
    """
    Bu fonksiyon, verilen input_shape (time_steps, height, width, channels)
    ve num_classes (örneğin, 3) parametrelerine göre 3D bir model oluşturur.
    Model, her zaman adımındaki (frame) görüntüden 3D özellik çıkarıp,
    sonrasında LSTM ile zamansal ilişkileri modelleyerek her sekans için
    tek bir tahmin (output shape: (None, num_classes)) üretir.
    """
    model = Sequential()

    # İlk katman olarak Input eklenir
    model.add(Input(shape=input_shape))

    # 3D konvolüsyon işlemleri yapılır.
    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    # 5D çıktıyı 3D'ye dönüştürmek için Reshape ekleniyor.
    model.add(Reshape((-1, 128)))  # Bu katman, veriyi 3D hale getirir.

    # LSTM katmanı
    model.add(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))

    # Çıkış katmanı: softmax aktivasyon ile num_classes tahmini yapar.
    model.add(Dense(num_classes, activation='softmax'))

    # Tamsayı etiketler kullandığımız için sparse_categorical_crossentropy kullanılır.
    model.compile(optimizer=Adam(learning_rate=0.00001),  # Öğrenme oranını düşürdük
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# =============================================================================
# 3. K-FOLD CROSS VALIDATION VE MODEL EĞİTİMİ
# =============================================================================

kf = KFold(n_splits=10, shuffle=True, random_state=42)

metrics = {
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': [],
    'auc_roc': [],
}

y_true_all, y_pred_all = [], []

fold_no = 1
for train_index, val_index in kf.split(X):
    print(f'\nFold {fold_no} işleniyor...')

    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    class_weights = {0: 1.0, 1: 1.0, 2: 1.5}

    # Model her fold için yeniden oluşturulur.
    model = create_3d_model(input_shape=X_train.shape[1:], num_classes=3)

    # EarlyStopping ve ModelCheckpoint kullanımı
    #early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/fold_1_best_model.keras',
                                   monitor='val_loss',
                                   save_best_only=True,
                                   verbose=1)


    # Modeli eğitiyoruz.
    history = model.fit(
        X_train, y_train,
        epochs=15,
        batch_size=32,
        validation_data=(X_val, y_val),
        class_weight=class_weights,
        callbacks=[ model_checkpoint],
        verbose=1
    )

    # Doğrulama seti üzerinde tahmin yapıyoruz.
    y_pred = model.predict(X_val)
    y_pred_classes = np.argmax(y_pred, axis=1)

    auc = None
    try:
        y_val_onehot = np.eye(3)[y_val]
        if len(np.unique(y_val)) > 1:
            auc = roc_auc_score(y_val_onehot, y_pred, multi_class='ovr', average='macro')
        else:
            print("Uyarı: Yalnızca bir sınıf bulundu. AUC hesaplanamaz.")
    except ValueError as e:
        print(f"Uyarı: AUC hesaplanamadı: {e}")

    prec = precision_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    rec = recall_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)

    metrics['accuracy'].append(accuracy_score(y_val, y_pred_classes))
    metrics['precision'].append(prec)
    metrics['recall'].append(rec)
    metrics['f1'].append(f1)
    if auc is not None:
        metrics['auc_roc'].append(auc)

    y_true_all.extend(y_val)
    y_pred_all.extend(y_pred_classes)

    print(f"Fold {fold_no} - Accuracy: {metrics['accuracy'][-1]:.4f}, "
          f"Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}, "
          f"AUC-ROC: {auc if auc is not None else 'NaN'}")

    fold_no += 1

# =============================================================================
# 4. ORTALAMA METRİKLERİN HESAPLANMASI VE SONUÇLARIN KAYDEDİLMESİ
# =============================================================================

avg_metrics = {metric: np.mean(values) for metric, values in metrics.items()}
print("\nOrtalama Sonuçlar:")
for metric, value in avg_metrics.items():
    print(f"{metric}: {value:.4f}")

df_results = pd.DataFrame(metrics)
df_results.to_csv("/content/drive/MyDrive/NO_3D_CNN_LSTM_kfold_results.csv", index=False)

# =============================================================================
# 5. KARMAŞIKLIK MATRİSİNİN HESAPLANMASI VE GÖRSELLEŞTİRİLMESİ
# =============================================================================

conf_matrix = confusion_matrix(y_true_all, y_pred_all)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Backhand", "Forehand", "Serve"],
            yticklabels=["Backhand", "Forehand", "Serve"])
plt.title("Karmaşıklık Matrisi")
plt.xlabel("Tahmin Edilen Sınıf")
plt.ylabel("Gerçek Sınıf")
plt.show()

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Input, Conv3D, MaxPooling3D, BatchNormalization,
                                     Flatten, LSTM, Dense, Dropout, Reshape)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import KFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, confusion_matrix)
from sklearn.decomposition import PCA
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# =============================================================================
# 1. VERİLERİN YÜKLENMESİ VE HAZIRLANMASI
# =============================================================================

# X verisini yükleyin: Beklenen boyut: (num_groups, time_steps, height, width, channels)
X = np.load("/content/drive/MyDrive/X_3D_CNN_reshaped.npy")
# y verisini yükleyin: İlk hali (num_groups, time_steps)
y = np.load("/content/drive/MyDrive/y_labels_reshaped.npy")

# Orijinal veri boyutlarını yazdır
print("Orijinal X boyutları:", X.shape)  # Örnek çıktı: (1693, 10, 33, 33, 3)
print("Orijinal y boyutları:", y.shape)  # Örnek çıktı: (1693, 10)

# Eğer y verisi her sekans için 10 etiket içeriyorsa, her sekansın
# son etiketini alarak y verisini (num_groups,) boyutuna indiriyoruz.
if y.ndim == 2 and y.shape[1] > 1:
    y = y[:, -1]
print("Dönüştürülmüş y boyutları:", y.shape)  # Beklenen çıktı: (1693,)

# =============================================================================
# 2. MODEL MİMARİSİNİN OLUŞTURULMASI
# =============================================================================

def create_3d_model(input_shape, num_classes):
    """
    Bu fonksiyon, verilen input_shape (time_steps, height, width, channels)
    ve num_classes (örneğin, 3) parametrelerine göre 3D CNN - LSTM hibrit modeli oluşturur.
    """
    model = Sequential()
    model.add(Input(shape=input_shape))

    # 3D konvolüsyon işlemleri
    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(BatchNormalization())

    # 5D çıktıyı 3D'ye dönüştürmek için Reshape katmanı.
    model.add(Reshape((-1, 128)))

    # LSTM katmanı
    model.add(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))

    # Çıkış katmanı: softmax aktivasyon ile num_classes tahmini yapar.
    model.add(Dense(num_classes, activation='softmax'))

    # Tamsayı etiketler kullandığımız için sparse_categorical_crossentropy kullanılır.
    model.compile(optimizer=Adam(learning_rate=1e-5),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# =============================================================================
# 3. K-FOLD CROSS VALIDATION, PCA İLE ÖZ NİTELİK SEÇİMİ VE MODEL EĞİTİMİ
# =============================================================================

kf = KFold(n_splits=10, shuffle=True, random_state=42)

metrics = {
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': [],
    'auc_roc': [],
}

y_true_all, y_pred_all = [], []

# PCA parametreleri:
# Her frame orijinal boyutu: (33, 33, 3) => 33*33*3 = 3267
# PCA sonrasında n_components kadar özellik seçiyoruz, örneğin 64.
# Bu örnekte 64 bileşeni kare (8x8) formatında geri yerleştireceğiz.
n_components = 128
pca_target_shape = (8, 16, 1)  # 8 * 16 * 1 = 128


fold_no = 1
for train_index, val_index in kf.split(X):
    print(f'\nFold {fold_no} işleniyor...')

    # Bölünmüş veriler
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # --- PCA İle ÖZ NİTELİK SEÇİMİ ---
    # Her frame'i flatten edip PCA’ya sokacağız.
    num_train, time_steps, h, w, c = X_train.shape
    num_val = X_val.shape[0]

    # Eğitim verisini (num_train*time_steps, h*w*c) şeklinde yeniden şekillendir.
    X_train_reshaped = X_train.reshape(-1, h*w*c)
    # PCA’yı eğitim verisi üzerinde fit ediyoruz.
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train_reshaped)
    # PCA çıktısını (num_train, time_steps, pca_target_shape) şeklinde düzenliyoruz.
    X_train_pca = X_train_pca.reshape(num_train, time_steps, *pca_target_shape)

    # Doğrulama verisi için de aynı dönüşümü PCA parametreleri kullanarak uyguluyoruz.
    X_val_reshaped = X_val.reshape(-1, h*w*c)
    X_val_pca = pca.transform(X_val_reshaped)
    X_val_pca = X_val_pca.reshape(num_val, time_steps, *pca_target_shape)

    # Modelin giriş boyutu artık (time_steps, 8, 8, 1) şeklindedir.
    input_shape = X_train_pca.shape[1:]
    num_classes = 3  # Sınıf sayısı
    model = create_3d_model(input_shape=input_shape, num_classes=num_classes)

    # Ağırlıklandırma (örnekte sınıf 2 için hafif daha yüksek ağırlık)
    class_weights = {0: 1.0, 1: 1.0, 2: 1.5}

    # Erken durdurma ve model kaydetme callback'leri (ModelCheckpoint örneğinde fold numarasını kullanabilirsiniz)
    # early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    model_checkpoint = ModelCheckpoint(f'/content/drive/MyDrive/fold_{fold_no}_best_model.keras',
                                         monitor='val_loss',
                                         save_best_only=True,
                                         verbose=1)

    # Modeli eğitiyoruz.
    history = model.fit(
        X_train_pca, y_train,
        epochs=15,
        batch_size=32,
        validation_data=(X_val_pca, y_val),
        class_weight=class_weights,
        callbacks=[model_checkpoint],
        verbose=1
    )

    # Doğrulama seti üzerinde tahmin yapıyoruz.
    y_pred = model.predict(X_val_pca)
    y_pred_classes = np.argmax(y_pred, axis=1)

    # AUC-ROC hesaplama (sadece birden fazla sınıf varsa)
    auc = None
    try:
        y_val_onehot = np.eye(num_classes)[y_val]
        if len(np.unique(y_val)) > 1:
            auc = roc_auc_score(y_val_onehot, y_pred, multi_class='ovr', average='macro')
        else:
            print("Uyarı: Yalnızca bir sınıf bulundu. AUC hesaplanamaz.")
    except ValueError as e:
        print(f"Uyarı: AUC hesaplanamadı: {e}")

    prec = precision_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    rec = recall_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)

    metrics['accuracy'].append(accuracy_score(y_val, y_pred_classes))
    metrics['precision'].append(prec)
    metrics['recall'].append(rec)
    metrics['f1'].append(f1)
    if auc is not None:
        metrics['auc_roc'].append(auc)

    y_true_all.extend(y_val)
    y_pred_all.extend(y_pred_classes)

    print(f"Fold {fold_no} - Accuracy: {metrics['accuracy'][-1]:.4f}, "
          f"Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}, "
          f"AUC-ROC: {auc if auc is not None else 'NaN'}")

    fold_no += 1

# =============================================================================
# 4. ORTALAMA METRİKLERİN HESAPLANMASI VE SONUÇLARIN KAYDEDİLMESİ
# =============================================================================

avg_metrics = {metric: np.mean(values) for metric, values in metrics.items()}
print("\nOrtalama Sonuçlar:")
for metric, value in avg_metrics.items():
    print(f"{metric}: {value:.4f}")

df_results = pd.DataFrame(metrics)
df_results.to_csv("/content/drive/MyDrive/PCA_3D_CNN_LSTM_kfold_results.csv", index=False)

# =============================================================================
# 5. KARMAŞIKLIK MATRİSİNİN HESAPLANMASI VE GÖRSELLEŞTİRİLMESİ
# =============================================================================

conf_matrix = confusion_matrix(y_true_all, y_pred_all)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Backhand", "Forehand", "Serve"],
            yticklabels=["Backhand", "Forehand", "Serve"])
plt.title("Karmaşıklık Matrisi")
plt.xlabel("Tahmin Edilen Sınıf")
plt.ylabel("Gerçek Sınıf")
plt.show()

!pip uninstall -y pandas
!pip install --no-cache-dir --upgrade pandas numpy tensorflow

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import KFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, confusion_matrix)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# =============================================================================
# 1. VERİLERİN YÜKLENMESİ VE HAZIRLANMASI
# =============================================================================

# X verisini yükleyin: Beklenen boyut: (num_groups, time_steps, height, width, channels)
X = np.load("/content/drive/MyDrive/X_3D_CNN_reshaped.npy")
# y verisini yükleyin: İlk hali (num_groups, time_steps)
y = np.load("/content/drive/MyDrive/y_labels_reshaped.npy")

# Orijinal veri boyutlarını yazdır
print("Orijinal X boyutları:", X.shape)  # Örnek çıktı: (1693, 10, 33, 33, 3)
print("Orijinal y boyutları:", y.shape)  # Örnek çıktı: (1693, 10)

# Eğer y verisi her sekans için 10 etiket içeriyorsa, her sekansın son etiketini alıyoruz.
if y.ndim == 2 and y.shape[1] > 1:
    y = y[:, -1]
print("Dönüştürülmüş y boyutları:", y.shape)  # Beklenen çıktı: (1693,)

# (İsteğe bağlı) Gerekliyse verileri normalize edebilirsiniz.
# Örneğin, eğer görüntü değerleriniz 0-255 arasındaysa:
X = X.astype('float32') / 255.0

# =============================================================================
# 2. MODEL MİMARİSİNİN OLUŞTURULMASI (LSTM tabanlı model)
# =============================================================================
def create_lda_model(input_shape, num_classes):
    """
    Bu fonksiyon, LDA ile indirgenmiş özellikleri (time_steps, n_features) giriş alan,
    LSTM kullanarak sınıflandırma yapan basit bir modeli oluşturur.
    """
    model = Sequential()
    model.add(Input(shape=input_shape))

    # LSTM katmanı: time_steps boyunca ilişkileri öğrenir.
    model.add(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))

    # Çıkış katmanı: softmax ile sınıf tahmini.
    model.add(Dense(num_classes, activation='softmax'))

    # Derleme: Tamsayı etiketler için sparse_categorical_crossentropy kullanılıyor.
    model.compile(optimizer=Adam(learning_rate=1e-5),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# =============================================================================
# 3. K-FOLD CROSS VALIDATION, LDA İLE ÖZ NİTELİK SEÇİMİ VE MODEL EĞİTİMİ
# =============================================================================

kf = KFold(n_splits=10, shuffle=True, random_state=42)

metrics = {
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': [],
    'auc_roc': [],
}

y_true_all, y_pred_all = [], []

# LDA için ayarlar:
# Orijinal her frame boyutu: (33, 33, 3) => 33*33*3 = 3267 özellik
# LDA maksimum (n_classes - 1) bileşen üretebilir. 3 sınıf için n_components = 2.
n_components = 2

fold_no = 1
for train_index, val_index in kf.split(X):
    print(f'\nFold {fold_no} işleniyor...')

    # Bölünmüş veriler
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # --- LDA ile Öz Nitelik Seçimi ---
    # X_train: (num_train, time_steps, h, w, c)
    num_train, time_steps, h, w, c = X_train.shape
    num_val = X_val.shape[0]

    # Her frame'i flatten ediyoruz: (num_train*time_steps, h*w*c)
    X_train_reshaped = X_train.reshape(-1, h * w * c)
    # Her frame'in etiketi, video etiketi ile aynı kabul ediliyor:
    y_train_repeated = np.repeat(y_train, time_steps)

    # LDA'yı eğitim verisi üzerinde fit ediyoruz.
    lda = LDA(n_components=n_components)
    X_train_lda = lda.fit_transform(X_train_reshaped, y_train_repeated)

    # LDA çıktısı: (num_train*time_steps, n_components)
    # Her video için tekrar şekillendiriyoruz: (num_train, time_steps, n_components)
    X_train_lda = X_train_lda.reshape(num_train, time_steps, n_components)

    # Doğrulama verisi için aynı dönüşümü uyguluyoruz.
    X_val_reshaped = X_val.reshape(-1, h * w * c)
    X_val_lda = lda.transform(X_val_reshaped)
    X_val_lda = X_val_lda.reshape(num_val, time_steps, n_components)

    # Modelin giriş boyutu artık (time_steps, n_components) yani (10, 2) olacaktır.
    input_shape = X_train_lda.shape[1:]
    num_classes = 3  # Sınıf sayısı
    model = create_lda_model(input_shape=input_shape, num_classes=num_classes)

    # Sınıf ağırlıkları (örnekte, sınıf 2 için hafif daha yüksek ağırlık veriliyor)
    class_weights = {0: 1.0, 1: 1.0, 2: 1.5}

    # Callback: Erken durdurma veya model kaydetme isteğe bağlı eklenebilir.
    model_checkpoint = ModelCheckpoint(f'/content/drive/MyDrive/fold_{fold_no}_best_model.keras',
                                         monitor='val_loss',
                                         save_best_only=True,
                                         verbose=1)

    # Modeli eğitiyoruz.
    history = model.fit(
        X_train_lda, y_train,
        epochs=15,
        batch_size=32,
        validation_data=(X_val_lda, y_val),
        class_weight=class_weights,
        callbacks=[model_checkpoint],
        verbose=1
    )

    # Doğrulama seti üzerinde tahmin yapıyoruz.
    y_pred = model.predict(X_val_lda)
    y_pred_classes = np.argmax(y_pred, axis=1)

    # AUC-ROC hesaplama (birden fazla sınıf varsa)
    auc = None
    try:
        y_val_onehot = np.eye(num_classes)[y_val]
        if len(np.unique(y_val)) > 1:
            auc = roc_auc_score(y_val_onehot, y_pred, multi_class='ovr', average='macro')
        else:
            print("Uyarı: Yalnızca bir sınıf bulundu. AUC hesaplanamaz.")
    except ValueError as e:
        print(f"Uyarı: AUC hesaplanamadı: {e}")

    prec = precision_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    rec = recall_score(y_val, y_pred_classes, average='weighted', zero_division=0)
    f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)

    metrics['accuracy'].append(accuracy_score(y_val, y_pred_classes))
    metrics['precision'].append(prec)
    metrics['recall'].append(rec)
    metrics['f1'].append(f1)
    if auc is not None:
        metrics['auc_roc'].append(auc)

    y_true_all.extend(y_val)
    y_pred_all.extend(y_pred_classes)

    print(f"Fold {fold_no} - Accuracy: {metrics['accuracy'][-1]:.4f}, "
          f"Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}, "
          f"AUC-ROC: {auc if auc is not None else 'NaN'}")

    fold_no += 1

# =============================================================================
# 4. ORTALAMA METRİKLERİN HESAPLANMASI VE SONUÇLARIN KAYDEDİLMESİ
# =============================================================================

avg_metrics = {metric: np.mean(values) for metric, values in metrics.items()}
print("\nOrtalama Sonuçlar:")
for metric, value in avg_metrics.items():
    print(f"{metric}: {value:.4f}")

df_results = pd.DataFrame(metrics)
df_results.to_csv("/content/drive/MyDrive/LDA_3D_CNN_LSTM_kfold_results.csv", index=False)

# =============================================================================
# 5. KARMAŞIKLIK MATRİSİNİN HESAPLANMASI VE GÖRSELLEŞTİRİLMESİ
# =============================================================================

conf_matrix = confusion_matrix(y_true_all, y_pred_all)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Backhand", "Forehand", "Serve"],
            yticklabels=["Backhand", "Forehand", "Serve"])
plt.title("Karmaşıklık Matrisi")
plt.xlabel("Tahmin Edilen Sınıf")
plt.ylabel("Gerçek Sınıf")
plt.show()

"""# MAKİNE ÖĞRENİMİ

"""

import numpy as np
import pandas as pd

# Makine öğrenmesi modülleri
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

# Multioutput için
from sklearn.multioutput import MultiOutputClassifier

# Metrik modülleri
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score

# Model kopyalamak için
from sklearn.base import clone

# -----------------------------
# Veri Yükleme ve Hazırlama
# -----------------------------
x_path = "/content/drive/MyDrive/X_3D_CNN_reshaped.npy"
y_path = "/content/drive/MyDrive/y_labels_reshaped.npy"

# Verileri yükleyelim
X = np.load(x_path)
y = np.load(y_path)

# Eğer veriler 3 boyutlu ise ve modellerin kabul edeceği 2B formata getirmek gerekiyorsa:
X = X.reshape(X.shape[0], -1)

# -----------------------------
# Modellerin Tanımlanması
# -----------------------------
models = {
    "SVM": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "GBM": GradientBoostingClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Uygulanacak dönüşüm stratejileri:
# "None": hiçbir boyut indirgeme uygulanmaz,
# "PCA": PCA ile, verinin %95 varyansı korunarak boyut indirgeme,
# "LDA": LDA ile, etiket bilgisini kullanarak boyut indirgeme
transformations = {
    "None": None,
    "PCA": PCA(n_components=0.95, random_state=42),
    "LDA": LDA()  # LDA, sınıf sayısına bağlı olarak n_components belirler.
}

# -----------------------------
# Yardımcı Fonksiyonlar
# -----------------------------
def compute_confusion_metrics(y_true, y_pred):
    """
    İkili sınıflandırmada confusion matrix'den sensitivity (TPR) ve specificity (TNR) hesaplar.
    Varsayım: y_true ve y_pred binary değerler içeriyor.
    """
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    return sensitivity, specificity

def evaluate_metrics(y_true, y_pred, y_prob=None):
    """
    Eğer y_true tek boyutlu ise doğrudan metrikleri hesaplar.
    Çoklu çıktı (multioutput) durumunda, her kolon için metrik hesaplanıp ortalaması alınır.
    """
    # Eğer tek çıktıysa:
    if y_true.ndim == 1:
        acc = accuracy_score(y_true, y_pred)
        sensitivity, specificity = compute_confusion_metrics(y_true, y_pred)
        f_measure = f1_score(y_true, y_pred)
        auc = None
        if y_prob is not None:
            try:
                auc = roc_auc_score(y_true, y_prob)
            except Exception:
                auc = None
        return acc, sensitivity, specificity, f_measure, auc
    else:
        n_outputs = y_true.shape[1]
        acc_list, sens_list, spec_list, f1_list, auc_list = [], [], [], [], []
        for i in range(n_outputs):
            acc_list.append(accuracy_score(y_true[:, i], y_pred[:, i]))
            sens, spec = compute_confusion_metrics(y_true[:, i], y_pred[:, i])
            sens_list.append(sens)
            spec_list.append(spec)
            f1_list.append(f1_score(y_true[:, i], y_pred[:, i]))
            # AUC ROC: yalnızca y_prob sağlanırsa hesaplanır.
            if y_prob is not None:
                try:
                    auc_i = roc_auc_score(y_true[:, i], y_prob[i][:, 1])
                except Exception:
                    auc_i = None
                if auc_i is not None:
                    auc_list.append(auc_i)
        avg_auc = np.mean(auc_list) if auc_list else None
        return np.mean(acc_list), np.mean(sens_list), np.mean(spec_list), np.mean(f1_list), avg_auc

# -----------------------------
# Ana Eğitim & Değerlendirme Döngüsü
# -----------------------------
results = {}  # Sonuçları saklamak için

# Çoklu etiket (multioutput) problemleri için, stratifiye yapılamadığı için KFold kullanıyoruz.
cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Her dönüşüm stratejisi için:
for trans_name, transformer in transformations.items():
    results[trans_name] = {}

    # Her model için:
    for model_name, model in models.items():
        fold_accuracies = []
        fold_sensitivities = []
        fold_specificities = []
        fold_f_measures = []
        fold_auc_rocs = []

        # Her fold için:
        for train_index, test_index in cv.split(X):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # Ölçeklendirme (StandardScaler)
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)

            # Uygulanan dönüşüm varsa (PCA veya LDA):
            if transformer is not None:
                # Her fold için transformer'ı yeniden oluşturmak daha doğru.
                if trans_name == "PCA":
                    trans = PCA(n_components=0.95, random_state=42)
                    X_train = trans.fit_transform(X_train)
                    X_test = trans.transform(X_test)
                elif trans_name == "LDA":
                    trans = LDA()
                    # LDA, etiket bilgisine ihtiyaç duyar.
                    X_train = trans.fit_transform(X_train, y_train if y_train.ndim==1 else y_train[:,0])
                    X_test = trans.transform(X_test)

            # Eğer çoklu etiket (multioutput) varsa, bazı modellerin bu desteği yoktur.
            # Bu durumda, tüm modelleri MultiOutputClassifier ile sarmalıyoruz.
            if y.ndim > 1 and y.shape[1] > 1:
                clf = MultiOutputClassifier(clone(model))
            else:
                clf = clone(model)

            # Modeli eğitelim
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)

            # AUC ROC için tahmin olasılıklarını almaya çalışalım (varsa)
            y_prob = None
            try:
                # Eğer multioutput ise, MultiOutputClassifier.predict_proba dönen liste verir.
                if hasattr(clf, "predict_proba"):
                    if y.ndim > 1 and y.shape[1] > 1:
                        y_prob = clf.predict_proba(X_test)  # Bu, her çıktı için bir liste döner.
                    else:
                        y_prob = clf.predict_proba(X_test)[:, 1]
                elif hasattr(clf, "decision_function"):
                    y_prob = clf.decision_function(X_test)
            except Exception:
                y_prob = None

            # Eğer y_test çoklu etiketliyse, y_pred'nin de aynı forma sahip olduğundan emin olalım.
            # Bazı durumlarda predict metodu 1D dönebilir.
            if y.ndim > 1 and y_pred.ndim == 1:
                y_pred = y_pred.reshape(-1, 1)

            # Metrikleri hesaplayalım
            acc, sens, spec, f_measure, auc = evaluate_metrics(y_test, y_pred, y_prob)
            fold_accuracies.append(acc)
            fold_sensitivities.append(sens)
            fold_specificities.append(spec)
            fold_f_measures.append(f_measure)
            if auc is not None:
                fold_auc_rocs.append(auc)

        avg_auc = np.mean(fold_auc_rocs) if fold_auc_rocs else None
        results[trans_name][model_name] = {
            'Accuracy': np.mean(fold_accuracies),
            'Sensitivity': np.mean(fold_sensitivities),
            'Specificity': np.mean(fold_specificities),
            'F-Measure': np.mean(fold_f_measures),
            'AUC ROC': avg_auc
        }

# -----------------------------
# Sonuçların CSV'ye Kaydedilmesi
# -----------------------------
all_results = []
for trans_name, models_dict in results.items():
    for model_name, metrics in models_dict.items():
        row = {"Transformation": trans_name, "Model": model_name}
        row.update(metrics)
        all_results.append(row)

df_results = pd.DataFrame(all_results)
output_file = "/content/drive/MyDrive/model_evaluation_results.csv"
df_results.to_csv(output_file, index=False)

print(f"Sonuçlar '{output_file}' dosyasına kaydedildi.")

import numpy as np
import pandas as pd

# Makine öğrenmesi modülleri
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

# Metrik modülleri
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score

# Model kopyalamak için
from sklearn.base import clone

# Verilerin yolu
x_path = "/content/drive/MyDrive/X_3D_CNN_reshaped.npy"
y_path = "/content/drive/MyDrive/y_labels_reshaped.npy"

# Veriyi yükleyelim
X = np.load(x_path)
y = np.load(y_path)

# Eğer veriler 3 boyutlu ise (örneğin; görüntü, video vb.) ve modellerin kabul edeceği 2B formata dönüştürmek gerekiyorsa,
# aşağıdaki satır ile reshape edebilirsiniz:
# X = X.reshape(X.shape[0], -1)

# Modelleri tanımlıyoruz
models = {
    "SVM": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "GBM": GradientBoostingClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Uygulanacak ön işleme stratejileri
# 'None': Boyut indirgeme yapmadan,
# 'PCA': PCA ile boyut indirgeme (açıklanan varyansın %95’i korunur),
# 'LDA': LDA ile boyut indirgeme (etiket bilgisine bağlı)
transformations = {
    "None": None,
    "PCA": PCA(n_components=0.95, random_state=42),
    "LDA": LDA()  # LDA, sınıf sayısına bağlı olarak n_components ayarlar (ikili sınıflandırmada n_components=1 olur)
}

# Sonuçları saklamak için bir sözlük oluşturuyoruz
results = {}

# 5-fold stratifiye çapraz doğrulama
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Her bir dönüşüm stratejisi için:
for trans_name, transformer in transformations.items():
    results[trans_name] = {}  # Örneğin: results["PCA"] = { ... }

    # Her model için:
    for model_name, model in models.items():
        fold_accuracies = []
        fold_sensitivities = []
        fold_specificities = []
        fold_f_measures = []
        fold_auc_rocs = []

        # Çapraz doğrulama fold’ları
        for train_index, test_index in cv.split(X, y):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # Öncelikle StandardScaler ile ölçeklendirme (hem PCA hem de LDA daha iyi sonuç verir)
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)

            # Eğer bir dönüşüm (PCA veya LDA) uygulanacaksa:
            if transformer is not None:
                # Her fold’da transformer’ı yeniden tanımlamak en temiz yaklaşımdır.
                if trans_name == "PCA":
                    trans = PCA(n_components=0.95, random_state=42)
                    X_train = trans.fit_transform(X_train)
                    X_test = trans.transform(X_test)
                elif trans_name == "LDA":
                    trans = LDA()
                    # LDA fit ederken etiket bilgisine ihtiyaç duyar.
                    X_train = trans.fit_transform(X_train, y_train)
                    X_test = trans.transform(X_test)

            # Modeli fold için yeniden oluşturuyoruz (clone kullanarak global nesnenin etkilenmesini önleriz)
            clf = clone(model)
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)

            # Accuracy
            acc = accuracy_score(y_test, y_pred)
            fold_accuracies.append(acc)

            # Confusion Matrix'den sensitivity (TPR) ve specificity (TNR) hesaplama (ikili sınıflandırma varsayımıyla)
            # Confusion Matrix formatı: [[TN, FP], [FN, TP]]
            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
            fold_sensitivities.append(sensitivity)
            fold_specificities.append(specificity)

            # F-Measure (F1 Score)
            f1 = f1_score(y_test, y_pred)
            fold_f_measures.append(f1)

            # AUC ROC hesabı (modelin probability veya decision_function metoduna bağlı olarak)
            auc = None
            try:
                if hasattr(clf, "predict_proba"):
                    y_prob = clf.predict_proba(X_test)[:, 1]
                elif hasattr(clf, "decision_function"):
                    y_prob = clf.decision_function(X_test)
                auc = roc_auc_score(y_test, y_prob)
            except Exception as e:
                pass  # AUC hesaplanamıyorsa None kalır.
            if auc is not None:
                fold_auc_rocs.append(auc)

        # Fold’lardan elde edilen metriklerin ortalamasını hesaplıyoruz
        avg_auc = np.mean(fold_auc_rocs) if len(fold_auc_rocs) > 0 else None
        results[trans_name][model_name] = {
            'Accuracy': np.mean(fold_accuracies),
            'Sensitivity': np.mean(fold_sensitivities),
            'Specificity': np.mean(fold_specificities),
            'F-Measure': np.mean(fold_f_measures),
            'AUC ROC': avg_auc
        }

# Sonuçları bir DataFrame’e dönüştürüp CSV olarak kaydediyoruz.
all_results = []
for trans_name, models_dict in results.items():
    for model_name, metrics in models_dict.items():
        row = {"Transformation": trans_name, "Model": model_name}
        row.update(metrics)
        all_results.append(row)

df_results = pd.DataFrame(all_results)
output_file = "/content/drive/MyDrive/makine_model_evaluation_results.csv"
df_results.to_csv(output_file, index=False)

print(f"Sonuçlar '{output_file}' dosyasına kaydedildi.")

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# Veri dosyaları
pca_path = "/content/drive/MyDrive/data_with_pca_normalized.csv"
lda_train_path = "/content/drive/MyDrive/data_with_lda_train.csv"
lda_test_path = "/content/drive/MyDrive/data_with_lda_test.csv"
ham_data_path = "/content/drive/MyDrive/5_data_balanced.csv"

# 1. PCA uygulanmış veriyi yükleme
data_pca = pd.read_csv(pca_path)
X_pca = data_pca.drop(columns=['label'])
y_pca = data_pca['label'].astype(int)  # Sınıfları 0, 1, 2 olarak kullanma

# 2. LDA uygulanmış veriyi yükleme
lda_train = pd.read_csv(lda_train_path)
lda_test = pd.read_csv(lda_test_path)
X_lda = pd.concat([lda_train.drop(columns=['label']), lda_test.drop(columns=['label'])])
y_lda = pd.concat([lda_train['label'], lda_test['label']]).astype(int)  # Sınıfları 0, 1, 2 olarak kullanma

# 3. PCA ve LDA uygulanmamış veriyi yükleme
ham_data = pd.read_csv(ham_data_path)
X_ham = ham_data.drop(columns=['label'])
y_ham = ham_data['label'].astype(int)  # Sınıfları 0, 1, 2 olarak kullanma

# Modeller
models = {
    "SVM": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "GBM": GradientBoostingClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Performans metrikleri hesaplama fonksiyonu
def evaluate_model(X, y, model_name):
    results = []
    cm_dict = {name: [] for name in models.keys()}  # Her model için ayrı karmaşıklık matrisi saklama
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    for name, model in models.items():
        fold_accuracies = []
        fold_sensitivities = []
        fold_specificities = []
        fold_f_measures = []
        fold_auc_rocs = []

        for train_index, val_index in cv.split(X, y):
            X_train, X_val = X.iloc[train_index], X.iloc[val_index]
            y_train, y_val = y.iloc[train_index], y.iloc[val_index]

            # Özellik standardizasyonu
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_val = scaler.transform(X_val)

            # Modeli eğitme
            model.fit(X_train, y_train)

            # Tahmin yapma
            y_pred = model.predict(X_val)
            y_prob = model.predict_proba(X_val) if hasattr(model, 'predict_proba') else None

            # Performans metrikleri
            cm = confusion_matrix(y_val, y_pred, labels=[0, 1, 2])
            sensitivity = np.mean([cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0 for i in range(3)])
            specificity = np.mean([(cm.sum() - cm[:, i].sum() - cm[i].sum() + cm[i, i]) / (cm.sum() - cm[:, i].sum()) if cm.sum() - cm[:, i].sum() > 0 else 0 for i in range(3)])
            f_measure = f1_score(y_val, y_pred, average='weighted')
            auc_roc = roc_auc_score(y_val, y_prob, multi_class='ovr') if y_prob is not None else None

            fold_accuracies.append(accuracy_score(y_val, y_pred))
            fold_sensitivities.append(sensitivity)
            fold_specificities.append(specificity)
            fold_f_measures.append(f_measure)
            if auc_roc is not None:
                fold_auc_rocs.append(auc_roc)

            cm_dict[name].append(cm)

        # Ortalama sonuçlar
        results.append({
            'Model': name,
            'Accuracy': np.mean(fold_accuracies),
            'Sensitivity': np.mean(fold_sensitivities),
            'Specificity': np.mean(fold_specificities),
            'F-Measure': np.mean(fold_f_measures),
            'AUC ROC': np.mean(fold_auc_rocs) if fold_auc_rocs else None,
            'Dataset': model_name
        })

    return results, cm_dict

# Tüm veri setleri için çapraz doğrulama ve karmaşıklık matrislerini alma
results_pca, cm_pca_dict = evaluate_model(X_pca, y_pca, "PCA")
results_lda, cm_lda_dict = evaluate_model(X_lda, y_lda, "LDA")
results_ham, cm_ham_dict = evaluate_model(X_ham, y_ham, "Ham")

# Sonuçları birleştirme
all_results = pd.DataFrame(results_pca + results_lda + results_ham)

# Sonuçları CSV olarak kaydetme
all_results.to_csv('/content/drive/MyDrive/ml_All_model_results.csv', index=False)

# Her model için karmaşıklık matrislerini görselleştirme
for model_name, cm_list in {**cm_pca_dict, **cm_lda_dict, **cm_ham_dict}.items():
    total_cm = np.sum(cm_list, axis=0)
    plt.figure(figsize=(7, 7))
    sns.heatmap(total_cm, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=['Class 0', 'Class 1', 'Class 2'],
                yticklabels=['Class 0', 'Class 1', 'Class 2'])
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# 8. Sonuçları görselleştirme (üç ayrı grafik için düzenleme)

# Bar genişliği
bar_width = 0.15
x = np.arange(len(models))  # Modellerin sayısı kadar bar yerleştirilecek

# Metrikler
metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'F-Measure', 'AUC ROC']
metric_labels = ['Accuracy', 'Sensitivity', 'Specificity', 'F-Measure', 'AUC ROC']

# Renk paleti (her metrik için farklı renk)
colors = sns.color_palette("Set2", len(metrics))

# 1. PCA verisi için grafik
plt.figure(figsize=(14, 8))
for i, metric in enumerate(metrics):
    for j, model_name in enumerate(models.keys()):
        # PCA verisi için metrik değeri
        metric_data = all_results[(all_results['Model'] == model_name) & (all_results['Dataset'] == 'PCA')]
        metric_value = metric_data[metric].values[0]
        offset = bar_width * i  # Her metriği yatayda sıralamak için ofset
        plt.bar(x[j] + offset, metric_value, bar_width, color=colors[i], label=metric if j == 0 else "")

plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Performance Metrics for PCA')
plt.xticks(x + bar_width * 2, models.keys())
plt.legend(title='Metrics', loc='upper right')
plt.tight_layout()
plt.show()

# 2. LDA verisi için grafik
plt.figure(figsize=(14, 8))
for i, metric in enumerate(metrics):
    for j, model_name in enumerate(models.keys()):
        # LDA verisi için metrik değeri
        metric_data = all_results[(all_results['Model'] == model_name) & (all_results['Dataset'] == 'LDA')]
        metric_value = metric_data[metric].values[0]
        offset = bar_width * i  # Her metriği yatayda sıralamak için ofset
        plt.bar(x[j] + offset, metric_value, bar_width, color=colors[i], label=metric if j == 0 else "")

plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Performance Metrics for LDA')
plt.xticks(x + bar_width * 2, models.keys())
plt.legend(title='Metrics', loc='upper right')
plt.tight_layout()
plt.show()

# 3. Ham veri seti için grafik
plt.figure(figsize=(14, 8))
for i, metric in enumerate(metrics):
    for j, model_name in enumerate(models.keys()):
        # Ham veri seti için metrik değeri
        metric_data = all_results[(all_results['Model'] == model_name) & (all_results['Dataset'] == 'Ham')]
        metric_value = metric_data[metric].values[0]
        offset = bar_width * i  # Her metriği yatayda sıralamak için ofset
        plt.bar(x[j] + offset, metric_value, bar_width, color=colors[i], label=metric if j == 0 else "")

plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Performance Metrics for Raw (Ham) Data')
plt.xticks(x + bar_width * 2, models.keys())
plt.legend(title='Metrics', loc='upper right')
plt.tight_layout()
plt.show()

"""# DERİN ÖĞRENME

"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Dosya yolları
lda_file = '/content/drive/MyDrive/dl_lda_model_results_kfold.csv'
no_method_file = '/content/drive/MyDrive/dl_model_performance_results_kfold.csv'
pca_file = '/content/drive/MyDrive/dl_pca_model_performance_results_kfold.csv'

# Dosyaları yükle
lda_results = pd.read_csv(lda_file)
no_method_results = pd.read_csv(no_method_file)
pca_results = pd.read_csv(pca_file)

# Sütun adlarını kontrol et
print("LDA Columns:", lda_results.columns)
print("PCA Columns:", pca_results.columns)
print("No Method Columns:", no_method_results.columns)

# Performans metriklerini kontrol et
lda_metrics = ['accuracy', 'f1', 'sensitivity', 'specificity', 'auc_roc']
pca_metrics = ['accuracy', 'f1', 'sensitivity', 'specificity', 'auc_roc']
no_method_metrics = ['accuracy', 'f1', 'sensitivity', 'specificity', 'auc_roc']

# Verilerin eksik olup olmadığını kontrol et ve NaN yerine 0 ile doldur
lda_results = lda_results.fillna(0)
pca_results = pca_results.fillna(0)
no_method_results = no_method_results.fillna(0)

# Ortalamaları hesapla
lda_means = [lda_results[metric].mean() for metric in lda_metrics]
pca_means = [pca_results[metric].mean() for metric in pca_metrics]
no_method_means = [no_method_results[metric].mean() for metric in no_method_metrics]

# Metriklerin konumlarını belirle
x = np.arange(len(lda_metrics))  # Metriklerin konumu
width = 0.25  # Barların genişliği

# Grafik oluşturma
fig, ax = plt.subplots(figsize=(10, 6))

# Barlar
ax.bar(x - width, pca_means, width, label='PCA')
ax.bar(x, lda_means, width, label='LDA')
ax.bar(x + width, no_method_means, width, label='No Method')

# Grafiği özelleştirme
ax.set_xlabel('Performance Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of Performance Metrics for Deep Learning PCA, LDA, and No Method')
ax.set_xticks(x)
ax.set_xticklabels(lda_metrics)
ax.legend()

# Grafiği gösterme
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Dropout, Flatten
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli
def create_dbn_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Autoencoder Modeli
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli Eğitme ve Test Etme
def train_and_predict_kfold(features, labels, model_name, input_dim, input_shape=None, n_splits=5):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    total_cm = None

    for train_index, test_index in skf.split(features, labels):
        X_train, X_test = features[train_index], features[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        y_train = to_categorical(y_train)
        y_test = to_categorical(y_test)

        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

        y_pred = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_test_classes = np.argmax(y_test, axis=1)

        cm = confusion_matrix(y_test_classes, y_pred_classes)
        if total_cm is None:
            total_cm = cm
        else:
            total_cm += cm

    return total_cm

# Karmaşıklık Matrisini Görselleştirme
def plot_confusion_matrix(cm, model_name, dataset_name):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(cm.shape[0]), yticklabels=np.arange(cm.shape[0]))
    plt.title(f'Confusion Matrix - {model_name} on {dataset_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

def main():
    # Veri yolları
    paths = [
        "/content/drive/MyDrive/data_with_lda_train.csv",
        "/content/drive/MyDrive/data_with_lda_test.csv",
        "/content/drive/MyDrive/5_data_balanced.csv",
        "/content/drive/MyDrive/data_with_pca_normalized.csv"
    ]

    # Her model için karmaşıklık matrislerini toplamak
    confusion_matrices = {model: None for model in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]}

    # Her veri seti ile işlem yapılacak
    for path in paths:
        data = pd.read_csv(path)

        # Etiketler ve Özellikler
        labels = data.iloc[:, -1].values
        features = data.iloc[:, :-1].values

        print(f"Veri seti: {path}")
        print(f"Etiketlerin ilk 5 değeri: {labels[:5]}")
        print(f"Özelliklerin boyutu: {features.shape}")

        # Her model için eğitim ve karmaşıklık matrisi toplama
        for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
            cm = train_and_predict_kfold(features, labels, model_name, features.shape[1])
            if confusion_matrices[model_name] is None:
                confusion_matrices[model_name] = cm
            else:
                confusion_matrices[model_name] += cm

    # Karmaşıklık Matrislerinin Görselleştirilmesi
    for model_name, cm in confusion_matrices.items():
        print(f"{model_name} Modeli için Toplam Karmaşıklık Matrisi:")
        plot_confusion_matrix(cm, model_name, "All Datasets")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, roc_auc_score
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, LSTM, SimpleRNN, Dropout
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical
import seaborn as sns

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli (Deep Belief Network)
def create_dbn_model(input_dim, num_classes):
    model = create_mlp_model(input_dim, num_classes)
    return model

# Autoencoder Modeli (Sınıflandırma için)
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))  # Sınıflandırma amaçlı çıkış katmanı
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli Eğitme ve Test Etme
def train_and_predict(features, labels, model_name, input_dim, input_shape=None, kfold_splits=5):
    # K-Fold çapraz doğrulama
    kfold = KFold(n_splits=kfold_splits, shuffle=True, random_state=42)

    accuracies = []
    f1_scores = []
    sensitivities = []
    specificities = []
    auc_roc_scores = []
    cm_list = []

    for train_index, test_index in kfold.split(features):
        X_train, X_test = features[train_index], features[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        # Verilerin ölçeklendirilmesi
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Etiketlerin one-hot encoding formatına çevrilmesi
        y_train = to_categorical(y_train)
        y_test = to_categorical(y_test)

        # Modelin oluşturulması
        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # X_train için 3D şekil
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)  # X_test için 3D şekil
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        # Modeli eğitme
        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)

        # Tahmin yapma
        y_pred = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_test_classes = np.argmax(y_test, axis=1)

        # Başarıyı hesaplama
        accuracy = accuracy_score(y_test_classes, y_pred_classes)
        accuracies.append(accuracy)

        # Diğer metrikleri hesaplama
        f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
        f1_scores.append(f1)

        sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
        sensitivities.append(sensitivity)

        # Specificity hesaplama (True Negatives / (True Negatives + False Positives))
        cm = confusion_matrix(y_test_classes, y_pred_classes)
        specificity = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0
        specificities.append(specificity)

        # AUC ROC hesaplama
        auc_roc = roc_auc_score(y_test, y_pred, multi_class='ovr')
        auc_roc_scores.append(auc_roc)

        cm_list.append(cm)

    # Ortalama metrikleri hesaplama
    metrics = {
        'accuracy': np.mean(accuracies),
        'f1': np.mean(f1_scores),
        'sensitivity': np.mean(sensitivities),
        'specificity': np.mean(specificities),
        'auc_roc': np.mean(auc_roc_scores),
        'confusion_matrix': cm_list
    }

    return metrics

# Karmaşıklık Matrisini Görselleştirme
def plot_confusion_matrix(cm, model_name, save_path=None):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(cm.shape[0]), yticklabels=np.arange(cm.shape[0]))
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    # Karmaşıklık matrisini kaydetme
    if save_path:
        plt.savefig(save_path)
    else:
        plt.show()

# Model Performanslarını Görselleştirme
def plot_model_performance(metrics):
    model_names = list(metrics.keys())
    accuracy_values = [metrics[model]['accuracy'] for model in model_names]
    f1_values = [metrics[model]['f1'] for model in model_names]
    sensitivity_values = [metrics[model]['sensitivity'] for model in model_names]
    specificity_values = [metrics[model]['specificity'] for model in model_names]
    auc_roc_values = [metrics[model]['auc_roc'] for model in model_names]

    # Performans metriklerini bar grafiğiyle görselleştirme
    bar_width = 0.15
    x = np.arange(len(model_names))

    plt.figure(figsize=(10, 6))

    plt.bar(x - 2*bar_width, accuracy_values, bar_width, label='Accuracy', color='b')
    plt.bar(x - bar_width, f1_values, bar_width, label='F1 Score', color='g')
    plt.bar(x, sensitivity_values, bar_width, label='Sensitivity', color='r')
    plt.bar(x + bar_width, specificity_values, bar_width, label='Specificity', color='y')
    plt.bar(x + 2*bar_width, auc_roc_values, bar_width, label='AUC ROC', color='c')

    plt.xlabel('Model')
    plt.ylabel('Score')
    plt.title('Model Performance Comparison')
    plt.xticks(x, model_names)
    plt.legend()
    plt.show()

def main():
    # Veri setini yükleme
    file_path = "/content/drive/MyDrive/5_data_balanced.csv"
    data = pd.read_csv(file_path)

    # Etiketler ve özelliklerin ayrılması
    labels = data.iloc[:, -1].values
    features = data.iloc[:, :-1].values

    print(f"Etiketlerin ilk 5 değeri: {labels[:5]}")
    print(f"Özelliklerin boyutu: {features.shape}")

    # Model performanslarını saklayacak bir sözlük
    metrics = {}

    # Modelleri eğitme ve doğruluklarını saklama
    for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
        metrics[model_name] = train_and_predict(features, labels, model_name, features.shape[1])

        print(f"{model_name} model doğruluğu: {metrics[model_name]['accuracy']:.4f}")

        # Karmaşıklık matrisini görselleştirme ve kaydetme
        plot_confusion_matrix(metrics[model_name]['confusion_matrix'][0], model_name, save_path=f'{model_name}_confusion_matrix.png')

    # Modellerin performanslarını görselleştirme
    plot_model_performance(metrics)

    # Sonuçları bir CSV dosyasına kaydetme
    results_df = pd.DataFrame(metrics).T
    results_df.to_csv("/content/drive/MyDrive/dl_model_performance_results_kfold.csv")
    print("Sonuçlar 'dl_model_performance_results_kfold.csv' dosyasına kaydedildi.")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, roc_auc_score
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, LSTM, SimpleRNN, Dropout
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical
import seaborn as sns

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli (Deep Belief Network)
def create_dbn_model(input_dim, num_classes):
    model = create_mlp_model(input_dim, num_classes)
    return model

# Autoencoder Modeli (Sınıflandırma için)
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))  # Sınıflandırma amaçlı çıkış katmanı
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli Eğitme ve Test Etme
def train_and_predict(features, labels, model_name, input_dim, input_shape=None, kfold_splits=5):
    # K-Fold çapraz doğrulama
    kfold = KFold(n_splits=kfold_splits, shuffle=True, random_state=42)

    accuracies = []
    f1_scores = []
    sensitivities = []
    specificities = []
    auc_roc_scores = []
    cm_list = []

    for train_index, test_index in kfold.split(features):
        X_train, X_test = features[train_index], features[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        # Verilerin ölçeklendirilmesi
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Etiketlerin one-hot encoding formatına çevrilmesi
        y_train = to_categorical(y_train)
        y_test = to_categorical(y_test)

        # Modelin oluşturulması
        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # X_train için 3D şekil
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)  # X_test için 3D şekil
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        # Modeli eğitme
        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)

        # Tahmin yapma
        y_pred = model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_test_classes = np.argmax(y_test, axis=1)

        # Başarıyı hesaplama
        accuracy = accuracy_score(y_test_classes, y_pred_classes)
        accuracies.append(accuracy)

        # Diğer metrikleri hesaplama
        f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
        f1_scores.append(f1)

        sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
        sensitivities.append(sensitivity)

        # Specificity hesaplama (True Negatives / (True Negatives + False Positives))
        cm = confusion_matrix(y_test_classes, y_pred_classes)
        specificity = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0
        specificities.append(specificity)

        # AUC ROC hesaplama
        auc_roc = roc_auc_score(y_test, y_pred, multi_class='ovr')
        auc_roc_scores.append(auc_roc)

        cm_list.append(cm)

    # Ortalama metrikleri hesaplama
    metrics = {
        'accuracy': np.mean(accuracies),
        'f1': np.mean(f1_scores),
        'sensitivity': np.mean(sensitivities),
        'specificity': np.mean(specificities),
        'auc_roc': np.mean(auc_roc_scores),
        'confusion_matrix': cm_list
    }

    return metrics

# Karmaşıklık Matrisini Görselleştirme
def plot_confusion_matrix(cm, model_name, save_path=None):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(cm.shape[0]), yticklabels=np.arange(cm.shape[0]))
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    # Karmaşıklık matrisini kaydetme
    if save_path:
        plt.savefig(save_path)
    else:
        plt.show()

# Model Performanslarını Görselleştirme
def plot_model_performance(metrics):
    model_names = list(metrics.keys())
    accuracy_values = [metrics[model]['accuracy'] for model in model_names]
    f1_values = [metrics[model]['f1'] for model in model_names]
    sensitivity_values = [metrics[model]['sensitivity'] for model in model_names]
    specificity_values = [metrics[model]['specificity'] for model in model_names]
    auc_roc_values = [metrics[model]['auc_roc'] for model in model_names]

    # Performans metriklerini bar grafiğiyle görselleştirme
    bar_width = 0.15
    x = np.arange(len(model_names))

    plt.figure(figsize=(10, 6))

    plt.bar(x - 2*bar_width, accuracy_values, bar_width, label='Accuracy', color='b')
    plt.bar(x - bar_width, f1_values, bar_width, label='F1 Score', color='g')
    plt.bar(x, sensitivity_values, bar_width, label='Sensitivity', color='r')
    plt.bar(x + bar_width, specificity_values, bar_width, label='Specificity', color='y')
    plt.bar(x + 2*bar_width, auc_roc_values, bar_width, label='AUC ROC', color='c')

    plt.xlabel('Model')
    plt.ylabel('Score')
    plt.title('Model Performance Comparison')
    plt.xticks(x, model_names)
    plt.legend()
    plt.show()

def main():
    # Veri setini yükleme
    file_path = "/content/drive/MyDrive/data_with_pca_normalized.csv"
    data = pd.read_csv(file_path)

    # Etiketler ve özelliklerin ayrılması
    labels = data.iloc[:, -1].values
    features = data.iloc[:, :-1].values

    print(f"Etiketlerin ilk 5 değeri: {labels[:5]}")
    print(f"Özelliklerin boyutu: {features.shape}")

    # Model performanslarını saklayacak bir sözlük
    metrics = {}

    # Modelleri eğitme ve doğruluklarını saklama
    for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
        metrics[model_name] = train_and_predict(features, labels, model_name, features.shape[1])

        print(f"{model_name} model doğruluğu: {metrics[model_name]['accuracy']:.4f}")

        # Karmaşıklık matrisini görselleştirme ve kaydetme
        plot_confusion_matrix(metrics[model_name]['confusion_matrix'][0], model_name, save_path=f'{model_name}_confusion_matrix.png')

    # Modellerin performanslarını görselleştirme
    plot_model_performance(metrics)

    # Sonuçları bir CSV dosyasına kaydetme
    results_df = pd.DataFrame(metrics).T
    results_df.to_csv("/content/drive/MyDrive/dl_pca_model_performance_results_kfold.csv")
    print("Sonuçlar 'dl_pca_model_performance_results_kfold.csv' dosyasına kaydedildi.")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, roc_auc_score
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Dropout, Flatten
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical
import seaborn as sns
from keras.layers import GlobalAveragePooling1D
from sklearn.model_selection import KFold

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(GlobalAveragePooling1D())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli
def create_dbn_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Autoencoder Modeli
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def train_and_predict_with_kfold(features, labels, model_name, input_dim, input_shape=None, k=5):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    fold_metrics = []

    for train_index, val_index in kf.split(features):
        X_train, X_val = features[train_index], features[val_index]
        y_train, y_val = labels[train_index], labels[val_index]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_val = scaler.transform(X_val)

        y_train = to_categorical(y_train)
        y_val = to_categorical(y_val)

        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)

        y_pred = model.predict(X_val)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_val_classes = np.argmax(y_val, axis=1)

        accuracy = accuracy_score(y_val_classes, y_pred_classes)
        f1 = f1_score(y_val_classes, y_pred_classes, average='weighted')
        sensitivity = recall_score(y_val_classes, y_pred_classes, average='weighted')

        cm = confusion_matrix(y_val_classes, y_pred_classes)
        specificity = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0

        auc_roc = roc_auc_score(y_val, y_pred, multi_class='ovr')

        fold_metrics.append({
            'accuracy': accuracy,
            'f1': f1,
            'sensitivity': sensitivity,
            'specificity': specificity,
            'auc_roc': auc_roc,
            'confusion_matrix': cm
        })

    # Ortalama metrikleri hesapla
    avg_metrics = {
        'accuracy': np.mean([fold['accuracy'] for fold in fold_metrics]),
        'f1': np.mean([fold['f1'] for fold in fold_metrics]),
        'sensitivity': np.mean([fold['sensitivity'] for fold in fold_metrics]),
        'specificity': np.mean([fold['specificity'] for fold in fold_metrics]),
        'auc_roc': np.mean([fold['auc_roc'] for fold in fold_metrics])
    }

    return avg_metrics, fold_metrics

# Sonuçları Kaydetme ve Görselleştirme
def save_results_to_csv(metrics, file_name="/content/drive/MyDrive/dl_lda_model_results_kfold.csv"):
    results_df = pd.DataFrame.from_dict(metrics, orient='index')
    results_df.to_csv(file_name)
    print(f"Sonuçlar {file_name} dosyasına kaydedildi.")

def plot_model_performance(metrics):
    models = list(metrics.keys())
    avg_accuracy = [metrics[model]['accuracy'] for model in models]
    avg_f1 = [metrics[model]['f1'] for model in models]
    avg_sensitivity = [metrics[model]['sensitivity'] for model in models]
    avg_specificity = [metrics[model]['specificity'] for model in models]
    avg_auc_roc = [metrics[model]['auc_roc'] for model in models]

    plt.figure(figsize=(12, 8))

    x = np.arange(len(models))
    width = 0.15  # Bar width

    fig, ax = plt.subplots(figsize=(10, 6))

    ax.bar(x - 2*width, avg_accuracy, width, label='Accuracy')
    ax.bar(x - width, avg_f1, width, label='F1 Score')
    ax.bar(x, avg_sensitivity, width, label='Sensitivity')
    ax.bar(x + width, avg_specificity, width, label='Specificity')
    ax.bar(x + 2*width, avg_auc_roc, width, label='AUC ROC')

    ax.set_xlabel('Models')
    ax.set_ylabel('Metrics')
    ax.set_title('Model Performance Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(models)
    ax.legend()

    plt.tight_layout()
    plt.show()

def main():
    train_file_path = "/content/drive/MyDrive/data_with_lda_train.csv"
    test_file_path = "/content/drive/MyDrive/data_with_lda_test.csv"

    train_data = pd.read_csv(train_file_path)
    test_data = pd.read_csv(test_file_path)

    train_labels = train_data.iloc[:, -1].values
    train_features = train_data.iloc[:, :-1].values

    test_labels = test_data.iloc[:, -1].values
    test_features = test_data.iloc[:, :-1].values

    print(f"Etiketlerin ilk 5 değeri (Eğitim): {train_labels[:5]}")
    print(f"Özelliklerin boyutu (Eğitim): {train_features.shape}")
    print(f"Etiketlerin ilk 5 değeri (Test): {test_labels[:5]}")
    print(f"Özelliklerin boyutu (Test): {test_features.shape}")

    metrics = {}

    for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
        avg_metrics, fold_metrics = train_and_predict_with_kfold(train_features, train_labels, model_name, train_features.shape[1])

        metrics[model_name] = avg_metrics

        print(f"{model_name} modelinin K-Fold sonuçları:")
        print(f"Ortalama Doğruluk: {avg_metrics['accuracy']:.4f}")
        print(f"Ortalama F1 Skoru: {avg_metrics['f1']:.4f}")
        print(f"Ortalama Hassasiyet: {avg_metrics['sensitivity']:.4f}")
        print(f"Ortalama Spesifite: {avg_metrics['specificity']:.4f}")
        print(f"Ortalama AUC ROC: {avg_metrics['auc_roc']:.4f}")

    plot_model_performance(metrics)
    save_results_to_csv(metrics)

if __name__ == "__main__":
    main()

"""# HİBRİT MODEL EĞİTİM VE PERFORMANSI
 Hibrit model PCA ,LDA ve NO_PCA_LDA için her bir foldun performans metriklerinin hesaplanması ve kaydedilmesi ardından tüm sonuçları bar grafiğinde gösterilmek için birleştirilmesini, karmaşıklık matrisini içerir.
"""

pip install tensorflow

#HER BİR FOLD UN METİKLERİNİ AYRI AYARI HESAPLAYIP YAZDIRIYOR.
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape
import matplotlib.pyplot as plt
import seaborn as sns

# Veri Yükleme
balanced_data = pd.read_csv("/content/drive/MyDrive/5_data_balanced.csv")

# Veriyi 3D CNN için Yeniden Şekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    if num_features % time_steps != 0:
        # Fazlalık özellikleri atmak için boyutu ayarla
        X = X[:, :(num_features // time_steps) * time_steps]
    features_per_step = X.shape[1] // time_steps
    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model Tanımlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same',
               input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(2, 2, 1)),
        Flatten(),
        Reshape((time_steps, -1)),
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Değerlendirme Fonksiyonu
def evaluate_model(y_test_classes, y_pred_classes, y_test, y_pred_prob):
    accuracy = accuracy_score(y_test_classes, y_pred_classes)
    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
    sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
    cm = confusion_matrix(y_test_classes, y_pred_classes)
    specificity = cm[1, 1] / (cm[1, 1] + cm[0, 1]) if (cm[1, 1] + cm[0, 1]) > 0 else 0
    try:
        auc_roc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='macro')
    except ValueError:
        auc_roc = None
    try:
        precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=1)
    except ValueError:
        precision = None
    return accuracy, f1, sensitivity, specificity, precision, auc_roc, cm

# K-Fold Çapraz Doğrulama
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
X = balanced_data.iloc[:, :-1].values
y = balanced_data.iloc[:, -1].values
time_steps = 10

fold = 1
results = []
for train_idx, test_idx in kfold.split(X, y):
    print(f"Fold {fold} Başlıyor...")

    # Eğitim ve Test Verilerini Ayırma
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Veriyi Yeniden Şekillendirme
    X_train = reshape_for_3d_cnn(X_train, time_steps)
    X_test = reshape_for_3d_cnn(X_test, time_steps)

    # Modeli Oluşturma
    model = create_hybrid_model(time_steps, X_train.shape[2])

    # Modeli Eğitme
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    # Tahminler
    y_pred_classes = np.argmax(model.predict(X_test), axis=1)
    y_pred_prob = model.predict(X_test)

    # Değerlendirme
    accuracy, f1, sensitivity, specificity, precision, auc_roc, cm = evaluate_model(y_test, y_pred_classes, y_test, y_pred_prob)
    results.append({
        "Fold": fold,
        "Accuracy": accuracy,
        "F1 Score": f1,
        "Sensitivity": sensitivity,
        "Specificity": specificity,
        "Precision": precision,
        "AUC ROC": auc_roc
    })

    print(f"Fold {fold} Sonuçları:")
    print(f"Accuracy: {accuracy}")
    print(f"F1 Score: {f1}")
    print(f"Sensitivity: {sensitivity}")
    print(f"Specificity: {specificity}")
    print(f"Precision: {precision}")
    print(f"AUC ROC: {auc_roc}")
    print(f"Confusion Matrix: \n{cm}")
    fold += 1

# Tüm Fold Sonuçlarını Kaydetme
results_df = pd.DataFrame(results)
results_df.to_csv("/content/drive/MyDrive/hibrit_no_kfold_results.csv", index=False)

# Ortalama Sonuçları Yazdırma
print("\nK-Fold Ortalamaları:")
print(results_df.mean())

#HER BİR FOLD UN METRİKLERİNİ HESAPLAYIP YAZDIRIYOR
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape
import matplotlib.pyplot as plt
import seaborn as sns

# Veri Yükleme
balanced_data = pd.read_csv("/content/drive/MyDrive/data_with_pca_normalized.csv")

# Veriyi 3D CNN için Yeniden Şekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    if num_features % time_steps != 0:
        # Fazlalık özellikleri atmak için boyutu ayarla
        X = X[:, :(num_features // time_steps) * time_steps]
    features_per_step = X.shape[1] // time_steps
    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model Tanımlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same', input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(1, 1, 1)),  # Havuzlama boyutunu girişe uyarladık
        Flatten(),
        Reshape((time_steps, -1)),  # Veriyi zaman adımlarına göre yeniden şekillendir
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model


# Değerlendirme Fonksiyonu
def evaluate_model(y_test_classes, y_pred_classes, y_test, y_pred_prob):
    accuracy = accuracy_score(y_test_classes, y_pred_classes)
    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
    sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
    cm = confusion_matrix(y_test_classes, y_pred_classes)
    specificity = cm[1, 1] / (cm[1, 1] + cm[0, 1]) if (cm[1, 1] + cm[0, 1]) > 0 else 0
    try:
        auc_roc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='macro')
    except ValueError:
        auc_roc = None
    try:
        precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=1)
    except ValueError:
        precision = None
    return accuracy, f1, sensitivity, specificity, precision, auc_roc, cm

# K-Fold Çapraz Doğrulama
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
X = balanced_data.iloc[:, :-1].values
y = balanced_data.iloc[:, -1].values
time_steps = 10

fold = 1
results = []
for train_idx, test_idx in kfold.split(X, y):
    print(f"Fold {fold} Başlıyor...")

    # Eğitim ve Test Verilerini Ayırma
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Veriyi Yeniden Şekillendirme
    X_train = reshape_for_3d_cnn(X_train, time_steps)
    X_test = reshape_for_3d_cnn(X_test, time_steps)

    # Modeli Oluşturma
    model = create_hybrid_model(time_steps, X_train.shape[2])

    # Modeli Eğitme
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    # Tahminler
    y_pred_classes = np.argmax(model.predict(X_test), axis=1)
    y_pred_prob = model.predict(X_test)

    # Değerlendirme
    accuracy, f1, sensitivity, specificity, precision, auc_roc, cm = evaluate_model(y_test, y_pred_classes, y_test, y_pred_prob)
    results.append({
        "Fold": fold,
        "Accuracy": accuracy,
        "F1 Score": f1,
        "Sensitivity": sensitivity,
        "Specificity": specificity,
        "Precision": precision,
        "AUC ROC": auc_roc
    })

    print(f"Fold {fold} Sonuçları:")
    print(f"Accuracy: {accuracy}")
    print(f"F1 Score: {f1}")
    print(f"Sensitivity: {sensitivity}")
    print(f"Specificity: {specificity}")
    print(f"Precision: {precision}")
    print(f"AUC ROC: {auc_roc}")
    print(f"Confusion Matrix: \n{cm}")
    fold += 1

# Tüm Fold Sonuçlarını Kaydetme
results_df = pd.DataFrame(results)
results_df.to_csv("/content/drive/MyDrive/hibrit_pca_kfold_results.csv", index=False)

# Ortalama Sonuçları Yazdırma
print("\nK-Fold Ortalamaları:")
print(results_df.mean())

#HER BİR K FOLD İÇİN AYRI AYRI METRİKLERİ İÇERİR.
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape
import matplotlib.pyplot as plt
import seaborn as sns

# Veri Yükleme
train_data = pd.read_csv("/content/drive/MyDrive/data_with_lda_train.csv")
test_data = pd.read_csv("/content/drive/MyDrive/data_with_lda_test.csv")

# Veriyi 3D CNN için Yeniden Şekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    if num_features % time_steps != 0:
        # Fazlalık özellikleri atmak için boyutu ayarla
        X = X[:, :(num_features // time_steps) * time_steps]
    features_per_step = X.shape[1] // time_steps
    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model Tanımlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same', input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(1, 1, 1)),  # Havuzlama boyutunu girişe uyarladık
        Flatten(),
        Reshape((time_steps, -1)),  # Veriyi zaman adımlarına göre yeniden şekillendir
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Değerlendirme Fonksiyonu
def evaluate_model(y_test_classes, y_pred_classes, y_test, y_pred_prob):
    accuracy = accuracy_score(y_test_classes, y_pred_classes)
    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
    sensitivity = recall_score(y_test_classes, y_pred_classes, average='weighted')
    cm = confusion_matrix(y_test_classes, y_pred_classes)
    specificity = cm[1, 1] / (cm[1, 1] + cm[0, 1]) if (cm[1, 1] + cm[0, 1]) > 0 else 0
    try:
        auc_roc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='macro')
    except ValueError:
        auc_roc = None
    try:
        precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=1)
    except ValueError:
        precision = None
    return accuracy, f1, sensitivity, specificity, precision, auc_roc, cm

# K-Fold Çapraz Doğrulama
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
X = balanced_data.iloc[:, :-1].values
y = balanced_data.iloc[:, -1].values
time_steps = 10

fold = 1
results = []
for train_idx, test_idx in kfold.split(X, y):
    print(f"Fold {fold} Başlıyor...")

    # Eğitim ve Test Verilerini Ayırma
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Veriyi Yeniden Şekillendirme
    X_train = reshape_for_3d_cnn(X_train, time_steps)
    X_test = reshape_for_3d_cnn(X_test, time_steps)

    # Modeli Oluşturma
    model = create_hybrid_model(time_steps, X_train.shape[2])

    # Modeli Eğitme
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

    # Tahminler
    y_pred_classes = np.argmax(model.predict(X_test), axis=1)
    y_pred_prob = model.predict(X_test)

    # Değerlendirme
    accuracy, f1, sensitivity, specificity, precision, auc_roc, cm = evaluate_model(y_test, y_pred_classes, y_test, y_pred_prob)
    results.append({
        "Fold": fold,
        "Accuracy": accuracy,
        "F1 Score": f1,
        "Sensitivity": sensitivity,
        "Specificity": specificity,
        "Precision": precision,
        "AUC ROC": auc_roc
    })

    print(f"Fold {fold} Sonuçları:")
    print(f"Accuracy: {accuracy}")
    print(f"F1 Score: {f1}")
    print(f"Sensitivity: {sensitivity}")
    print(f"Specificity: {specificity}")
    print(f"Precision: {precision}")
    print(f"AUC ROC: {auc_roc}")
    print(f"Confusion Matrix: \n{cm}")
    fold += 1

# Tüm Fold Sonuçlarını Kaydetme
results_df = pd.DataFrame(results)
results_df.to_csv("/content/drive/MyDrive/hibrit_lda_kfold_results.csv", index=False)

# Ortalama Sonuçları Yazdırma
print("\nK-Fold Ortalamaları:")
print(results_df.mean())

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Flatten, Conv3D, MaxPooling3D
from keras.utils import to_categorical
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# 3D CNN+LSTM Modeli
def create_3d_cnn_lstm_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))
    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same'))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli Eğitme ve Test Etme
def train_and_predict(features, labels, model_name, input_shape=None):
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    if model_name == "3D_CNN_LSTM":
        time_steps = 1
        n_features = X_train.shape[1]
        X_train = X_train.reshape(X_train.shape[0], time_steps, n_features // time_steps, 1, 1)
        X_test = X_test.reshape(X_test.shape[0], time_steps, n_features // time_steps, 1, 1)

        model = create_3d_cnn_lstm_model((time_steps, X_train.shape[2], 1, 1), y_train.shape[1])

    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_test_classes = np.argmax(y_test, axis=1)

    cm = confusion_matrix(y_test_classes, y_pred_classes)

    return cm

# Karmaşıklık Matrisini Görselleştirme
def plot_confusion_matrix(cm, model_name, dataset_name):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(cm.shape[0]), yticklabels=np.arange(cm.shape[0]))
    plt.title(f'Confusion Matrix - {model_name} on {dataset_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

def main():
    # Veri yolları
    paths = [
        "/content/drive/MyDrive/data_with_lda_train.csv",
        "/content/drive/MyDrive/data_with_lda_test.csv",
        "/content/drive/MyDrive/5_data_balanced.csv",
        "/content/drive/MyDrive/data_with_pca_normalized.csv"
    ]

    # Her model için karmaşıklık matrislerini toplamak
    confusion_matrices = {model: None for model in ["3D_CNN_LSTM"]}

    # Her veri seti ile işlem yapılacak
    for path in paths:
        data = pd.read_csv(path)

        # Etiketler ve Özellikler
        labels = data.iloc[:, -1].values
        features = data.iloc[:, :-1].values

        print(f"Veri seti: {path}")
        print(f"Etiketlerin ilk 5 değeri: {labels[:5]}")
        print(f"Özelliklerin boyutu: {features.shape}")

        # K-Fold Çapraz Doğrulama için StratifiedKFold
        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        fold = 1
        for train_idx, test_idx in kfold.split(features, labels):
            print(f"Fold {fold} Başlıyor...")

            # Eğitim ve Test Verilerini Ayırma
            X_train, X_test = features[train_idx], features[test_idx]
            y_train, y_test = labels[train_idx], labels[test_idx]

            # Veriyi Standartlaştırma
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)

            y_train = to_categorical(y_train)
            y_test = to_categorical(y_test)

            # Zaman adımlarına göre şekillendirme
            time_steps = 1
            n_features = X_train.shape[1]
            X_train = X_train.reshape(X_train.shape[0], time_steps, n_features // time_steps, 1, 1)
            X_test = X_test.reshape(X_test.shape[0], time_steps, n_features // time_steps, 1, 1)

            # Modeli oluşturma
            model = create_3d_cnn_lstm_model((time_steps, X_train.shape[2], 1, 1), y_train.shape[1])

            # Modeli eğitme
            model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

            # Tahminler
            y_pred = model.predict(X_test)
            y_pred_classes = np.argmax(y_pred, axis=1)
            y_test_classes = np.argmax(y_test, axis=1)

            # Karmaşıklık Matrisi
            cm = confusion_matrix(y_test_classes, y_pred_classes)

            if confusion_matrices["3D_CNN_LSTM"] is None:
                confusion_matrices["3D_CNN_LSTM"] = cm
            else:
                confusion_matrices["3D_CNN_LSTM"] += cm

            fold += 1

    # Karmaşıklık Matrislerinin Görselleştirilmesi
    for model_name, cm in confusion_matrices.items():
        print(f"{model_name} Modeli için Toplam Karmaşıklık Matrisi:")
        plot_confusion_matrix(cm, model_name, "All Datasets")

if __name__ == "__main__":
    main()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Dosya yolları
lda_file = '/content/drive/MyDrive/hibrit_lda_kfold_results.csv'
no_method_file = '/content/drive/MyDrive/hibrit_no_kfold_results.csv'
pca_file = '/content/drive/MyDrive/hibrit_pca_kfold_results.csv'

# Dosyaları yükle
lda_results = pd.read_csv(lda_file)
no_method_results = pd.read_csv(no_method_file)
pca_results = pd.read_csv(pca_file)

# Verilerin sütun isimlerini kontrol et
print("LDA Results Columns: ", lda_results.columns)
print("PCA Results Columns: ", pca_results.columns)
print("No Method Results Columns: ", no_method_results.columns)

# Performans metrikleri isimlerini belirliyoruz (gerekirse doğru isimlerle güncellenmiş)
lda_metrics = ['Accuracy', 'F1 Score', 'Sensitivity', 'Specificity', 'AUC ROC']
pca_metrics = ['Accuracy', 'F1 Score', 'Sensitivity', 'Specificity', 'AUC ROC']
no_method_metrics = ['Accuracy', 'F1 Score', 'Sensitivity', 'Specificity', 'AUC ROC']

# LDA ve PCA sonuçlarını uygun şekilde alıyoruz (sütun isimleri doğruysa)
lda_means = lda_results[lda_metrics].mean().values if all(metric in lda_results.columns for metric in lda_metrics) else np.nan
pca_means = pca_results[pca_metrics].mean().values if all(metric in pca_results.columns for metric in pca_metrics) else np.nan

# No Method verisini 'Metric' ve 'Score' kolonlarına göre düzenliyoruz
# 'Metric' ve 'Score' sütunları yerine doğrudan metriklerin adlarını kullanıyoruz
no_method_means = []
for metric in lda_metrics:  # 'AUC ROC' hariç
    if metric in no_method_results.columns:
        no_method_means.append(no_method_results[metric].mean())
    else:
        no_method_means.append(np.nan)

# Bar grafiği için düzenleme
x = np.arange(len(lda_metrics))  # Metriklerin konumu
width = 0.25  # Barların genişliği

# Grafik oluşturma
fig, ax = plt.subplots(figsize=(10, 6))

# Barlar
ax.bar(x - width, pca_means, width, label='PCA')
ax.bar(x, lda_means, width, label='LDA')
ax.bar(x + width, no_method_means, width, label='No Method')

# Grafiği özelleştirme
ax.set_xlabel('Perfrmans Metrikleri')
ax.set_ylabel('Skorlar')
ax.set_title('PCA, LDA ve PCA-LDA Kullanılmadan Performansların Karşılaşırılması')
ax.set_xticks(x)
ax.set_xticklabels(lda_metrics)
ax.legend()

# Grafiği gösterme
plt.tight_layout()
plt.show()

# Ortalama değerleri hesapla
lda_avg = lda_results[lda_metrics].mean()
pca_avg = pca_results[pca_metrics].mean()
no_method_avg = no_method_results[no_method_metrics].mean()

# Sonuçları birleştir
results_df = pd.DataFrame({
    'Metric': lda_metrics,
    'PCA': pca_avg.values,
    'LDA': lda_avg.values,
    'No Method': no_method_avg.values
})

# Sonuçları yeni bir CSV dosyasına kaydet
results_df.to_csv('/content/drive/MyDrive/hibrit_combined_performance_results.csv', index=False)

# Sonuçları göster
print("Combined Results Saved:")
print(results_df)

"""# EĞİTİM VE TEST VERİLERİNİN PERFORMANS DEĞERLENDİRMESİ"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape

# Veri Yükleme
data_pca = pd.read_csv("/content/drive/MyDrive/data_with_pca_normalized.csv")
data_lda_train = pd.read_csv("/content/drive/MyDrive/data_with_lda_train.csv")
data_lda_test = pd.read_csv("/content/drive/MyDrive/data_with_lda_test.csv")
data_no_transform = pd.read_csv("/content/drive/MyDrive/5_data_balanced.csv")

# Veriyi 3D CNN için Yeniden Şekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    print(f"Özellik Sayısı: {num_features}, Time_steps: {time_steps}")

    # time_steps değerini, özellik sayısına göre ayarlama
    if num_features < time_steps:
        time_steps = num_features  # Eğer özellik sayısı time_steps'ten küçükse, time_steps'i özellik sayısına eşitle
        print(f"Özellik sayısı zaman adımı sayısından küçük. time_steps değeri {time_steps} olarak ayarlandı.")

    if num_features % time_steps != 0:
        new_features = (num_features // time_steps) * time_steps
        print(f"Fazlalık özellikler atılacak. Yeni özellik sayısı: {new_features}")
        X = X[:, :new_features]

    features_per_step = X.shape[1] // time_steps

    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model Tanımlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same',
               input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(2, 2, 1), padding='same'),
        Flatten(),
        Reshape((time_steps, -1)),
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# K-Fold Çapraz Doğrulama
def cross_validation(X, y, time_steps, model_name):
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    all_histories = []
    accuracy_scores = []

    for train_idx, test_idx in kfold.split(X, y):
        # Eğitim ve Test Verilerini Ayırma
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        # Veriyi Yeniden Şekillendirme
        X_train = reshape_for_3d_cnn(X_train, time_steps)
        X_test = reshape_for_3d_cnn(X_test, time_steps)

        # Modeli Oluşturma
        model = create_hybrid_model(time_steps, X_train.shape[2])

        # Modeli Eğitme
        history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=0)

        # Geçmişi Saklama
        all_histories.append(history.history)

        # Test Seti ile Doğruluk Hesaplama
        y_pred = np.argmax(model.predict(X_test), axis=1)
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

    # Ortalama Doğruluk
    avg_accuracy = np.mean(accuracy_scores)
    print(f'{model_name} Ortalama Doğruluk: {avg_accuracy:.4f}')

    return avg_accuracy, all_histories

# Doğrulukların Tablosunu ve Grafiğini Çizme
def plot_accuracy_graph(title, accuracies, histories):
    # Her model için doğrulukları çizme
    plt.figure(figsize=(10, 6))
    for model_name, (accuracy, history) in accuracies.items():
        # Epoch başına eğitim ve test doğruluğu
        epochs = range(1, len(history[0]['accuracy']) + 1)
        avg_train_accuracy = np.mean([h['accuracy'] for h in history], axis=0)
        avg_val_accuracy = np.mean([h['val_accuracy'] for h in history], axis=0)

        # Grafik
        plt.plot(epochs, avg_train_accuracy, label=f'{model_name} Eğitim Doğruluğu')
        plt.plot(epochs, avg_val_accuracy, label=f'{model_name} Test Doğruluğu')

    plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel('Doğruluk')
    plt.legend()
    plt.grid(True)
    plt.show()

# Veriyi Yükleme
X_pca = data_pca.iloc[:, :-1].values
y_pca = data_pca.iloc[:, -1].values
X_lda_train = data_lda_train.iloc[:, :-1].values
y_lda_train = data_lda_train.iloc[:, -1].values
X_lda_test = data_lda_test.iloc[:, :-1].values
y_lda_test = data_lda_test.iloc[:, -1].values
X_no_transform = data_no_transform.iloc[:, :-1].values
y_no_transform = data_no_transform.iloc[:, -1].values

# Zaman adımını özellik sayısına göre uyarlama
time_steps_pca = min(X_pca.shape[1], 10)
time_steps_lda = min(X_lda_train.shape[1], 10)
time_steps_no_transform = min(X_no_transform.shape[1], 10)

# PCA ile eğitim ve doğruluk
accuracy_pca, history_pca = cross_validation(X_pca, y_pca, time_steps_pca, "PCA")

# LDA ile eğitim ve doğruluk
accuracy_lda, history_lda = cross_validation(X_lda_train, y_lda_train, time_steps_lda, "LDA")

# LDA ve PCA uygulanmamış verilerle eğitim ve doğruluk
accuracy_no_transform, history_no_transform = cross_validation(X_no_transform, y_no_transform, time_steps_no_transform, "No Transformation")

# Sonuçları Grafikle Gösterme
accuracies = {
    'PCA': (accuracy_pca, history_pca),
    'LDA': (accuracy_lda, history_lda),
    'No Transformation': (accuracy_no_transform, history_no_transform)
}

plot_accuracy_graph('Eğitim ve Test Doğruluğu Karşılaştırması', accuracies, {
    'PCA': history_pca,
    'LDA': history_lda,
    'No Transformation': history_no_transform
})

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Dropout, TimeDistributed, Reshape
from sklearn.metrics import accuracy_score

# Veri Yükleme
data_pca = pd.read_csv("/content/drive/MyDrive/data_with_pca_normalized.csv")
data_lda_train = pd.read_csv("/content/drive/MyDrive/data_with_lda_train.csv")
data_lda_test = pd.read_csv("/content/drive/MyDrive/data_with_lda_test.csv")
data_no_transform = pd.read_csv("/content/drive/MyDrive/5_data_balanced.csv")

# Veriyi 3D CNN için Yeniden Şekillendirme
def reshape_for_3d_cnn(X, time_steps=10):
    num_features = X.shape[1]
    print(f"Özellik Sayısı: {num_features}, Time_steps: {time_steps}")

    # time_steps değerini, özellik sayısına göre ayarlama
    if num_features < time_steps:
        time_steps = num_features  # Eğer özellik sayısı time_steps'ten küçükse, time_steps'i özellik sayısına eşitle
        print(f"Özellik sayısı zaman adımı sayısından küçük. time_steps değeri {time_steps} olarak ayarlandı.")

    if num_features % time_steps != 0:
        new_features = (num_features // time_steps) * time_steps
        print(f"Fazlalık özellikler atılacak. Yeni özellik sayısı: {new_features}")
        X = X[:, :new_features]

    features_per_step = X.shape[1] // time_steps

    return X.reshape(-1, time_steps, features_per_step, 1, 1)

# Hibrit Model Tanımlama (3D CNN + LSTM)
def create_hybrid_model(time_steps, features_per_step):
    model = Sequential([
        Conv3D(32, kernel_size=(3, 3, 1), activation='relu', padding='same',
               input_shape=(time_steps, features_per_step, 1, 1)),
        MaxPooling3D(pool_size=(2, 2, 1), padding='same'),  # Padding'i 'same' olarak ayarladık
        Flatten(),
        Reshape((time_steps, -1)),
        TimeDistributed(Dense(64, activation='relu')),
        LSTM(64, return_sequences=False),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Eğitim ve Test Doğruluklarının Toplanması
def plot_combined_history(all_histories, title):
    epochs = range(1, len(all_histories[0]['accuracy']) + 1)
    avg_train_accuracy = np.mean([history['accuracy'] for history in all_histories], axis=0)
    avg_val_accuracy = np.mean([history['val_accuracy'] for history in all_histories], axis=0)
    avg_train_loss = np.mean([history['loss'] for history in all_histories], axis=0)
    avg_val_loss = np.mean([history['val_loss'] for history in all_histories], axis=0)

    # Doğruluk Grafiği
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, avg_train_accuracy, label='Ortalama Eğitim Doğruluğu', color='blue')
    plt.plot(epochs, avg_val_accuracy, label='Ortalama Test Doğruluğu', color='orange')
    plt.title(f'{title} - Eğitim ve Test Doğruluklarının Karşılaştırması')
    plt.xlabel('Epochs')
    plt.ylabel('Doğruluk')
    plt.legend()
    plt.grid()
    plt.show()

    # Kayıp Grafiği
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, avg_train_loss, label='Ortalama Eğitim Kaybı', color='blue')
    plt.plot(epochs, avg_val_loss, label='Ortalama Test Kaybı', color='orange')
    plt.title(f'{title} - Eğitim ve Test Kayıplarının Karşılaştırması')
    plt.xlabel('Epochs')
    plt.ylabel('Kayıp')
    plt.legend()
    plt.grid()
    plt.show()

# K-Fold Çapraz Doğrulama
def cross_validation(X, y, time_steps, model_name):
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    all_histories = []
    fold = 1
    accuracy_scores = []
    train_accuracies = []
    val_accuracies = []

    for train_idx, test_idx in kfold.split(X, y):
        print(f"Fold {fold} Başlıyor...")

        # Eğitim ve Test Verilerini Ayırma
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        # Veriyi Yeniden Şekillendirme
        X_train = reshape_for_3d_cnn(X_train, time_steps)
        X_test = reshape_for_3d_cnn(X_test, time_steps)

        # Modeli Oluşturma
        model = create_hybrid_model(time_steps, X_train.shape[2])

        # Modeli Eğitme
        history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)

        # Geçmişi Saklama
        all_histories.append(history.history)

        # Eğitim ve Test doğruluklarını kaydetme
        train_accuracies.append(np.mean(history.history['accuracy']))
        val_accuracies.append(np.mean(history.history['val_accuracy']))

        # Test Seti ile Doğruluk Hesaplama
        y_pred = np.argmax(model.predict(X_test), axis=1)
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

        fold += 1

    # Eğitim ve Test Doğruluklarının Karşılaştırılması
    plot_combined_history(all_histories, model_name)

    # Ortalama Doğruluk
    print(f'{model_name} Ortalama Doğruluk: {np.mean(accuracy_scores):.4f}')
    return accuracy_scores, train_accuracies, val_accuracies

# Veriyi Yükleme
X_pca = data_pca.iloc[:, :-1].values
y_pca = data_pca.iloc[:, -1].values
X_lda_train = data_lda_train.iloc[:, :-1].values
y_lda_train = data_lda_train.iloc[:, -1].values
X_lda_test = data_lda_test.iloc[:, :-1].values
y_lda_test = data_lda_test.iloc[:, -1].values
X_no_transform = data_no_transform.iloc[:, :-1].values
y_no_transform = data_no_transform.iloc[:, -1].values

# Zaman adımını özellik sayısına göre uyarlama (özellik sayısına bakarak en uygun time_steps değerini ayarlayacağız)
time_steps_pca = min(X_pca.shape[1], 10)
time_steps_lda = min(X_lda_train.shape[1], 10)
time_steps_no_transform = min(X_no_transform.shape[1], 10)

# PCA ile eğitim ve doğruluk
accuracy_pca, train_acc_pca, val_acc_pca = cross_validation(X_pca, y_pca, time_steps_pca, "PCA")

# LDA ile eğitim ve doğruluk
accuracy_lda, train_acc_lda, val_acc_lda = cross_validation(X_lda_train, y_lda_train, time_steps_lda, "LDA")

# LDA ve PCA uygulanmamış verilerle eğitim ve doğruluk
accuracy_no_transform, train_acc_no_transform, val_acc_no_transform = cross_validation(X_no_transform, y_no_transform, time_steps_no_transform, "No Transformation")

# Doğrulukların Tablosu
results_table = pd.DataFrame({
    'PCA Accuracy': accuracy_pca,
    'LDA Accuracy': accuracy_lda,
    'No Transformation Accuracy': accuracy_no_transform,
    'PCA Train Accuracy': train_acc_pca,
    'PCA Val Accuracy': val_acc_pca,
    'LDA Train Accuracy': train_acc_lda,
    'LDA Val Accuracy': val_acc_lda,
    'No Transform Train Accuracy': train_acc_no_transform,
    'No Transform Val Accuracy': val_acc_no_transform
})

print("Doğruluklar Tablosu:")
print(results_table)

# Bar Grafik - Doğrulukları Görselleştirme
model_names = ['PCA', 'LDA', 'No Transformation']
mean_accuracy = [np.mean(accuracy_pca), np.mean(accuracy_lda), np.mean(accuracy_no_transform)]

# Çizgi grafiği ve bar grafiği aynı anda oluşturma
plt.figure(figsize=(12, 6))

# Bar grafiği
plt.subplot(1, 2, 1)  # (satır, sütun, aktif grafik)
plt.bar(model_names, mean_accuracy, color=['blue', 'orange', 'green'])
plt.title('Ortalama Doğruluklar (Bar Grafiği)')
plt.xlabel('Model')
plt.ylabel('Ortalama Doğruluk')
plt.ylim(0, 1)
plt.grid(True)

# Çizgi grafiği
plt.subplot(1, 2, 2)
epochs = range(1, len(accuracy_pca) + 1)
plt.plot(epochs, accuracy_pca, label='PCA', color='blue')
plt.plot(epochs, accuracy_lda, label='LDA', color='orange')
plt.plot(epochs, accuracy_no_transform, label='No Transformation', color='green')
plt.title('Doğrulukların Epoch Bazında Karşılaştırması')
plt.xlabel('Epochs')
plt.ylabel('Doğruluk')
plt.legend()
plt.grid(True)

# Grafikleri gösterme
plt.tight_layout()
plt.show()

"""# ZAMAN VE KAYNAK KULLANIMI DEĞERLENDİRMESİ"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
import time

# Veri dosyaları
pca_path = "/content/drive/MyDrive/data_with_pca_normalized.csv"
lda_train_path = "/content/drive/MyDrive/data_with_lda_train.csv"
lda_test_path = "/content/drive/MyDrive/data_with_lda_test.csv"
ham_data_path = "/content/drive/MyDrive/5_data_balanced.csv"

# 1. PCA uygulanmış veriyi yükleme
data_pca = pd.read_csv(pca_path)
X_pca = data_pca.drop(columns=['label'])
y_pca = data_pca['label'].astype(int)

# 2. LDA uygulanmış veriyi yükleme
lda_train = pd.read_csv(lda_train_path)
lda_test = pd.read_csv(lda_test_path)
X_lda = pd.concat([lda_train.drop(columns=['label']), lda_test.drop(columns=['label'])])
y_lda = pd.concat([lda_train['label'], lda_test['label']]).astype(int)

# 3. PCA ve LDA uygulanmamış veriyi yükleme
ham_data = pd.read_csv(ham_data_path)
X_ham = ham_data.drop(columns=['label'])
y_ham = ham_data['label'].astype(int)

# Modeller
models = {
    "SVM": SVC(probability=True, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "GBM": GradientBoostingClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Performans metrikleri hesaplama fonksiyonu (Eğitim ve test sürelerini ölçme)
def evaluate_model(X, y, model_name):
    results = []
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    for name, model in models.items():
        fold_train_times = []
        fold_test_times = []

        for train_index, val_index in cv.split(X, y):
            X_train, X_val = X.iloc[train_index], X.iloc[val_index]
            y_train, y_val = y.iloc[train_index], y.iloc[val_index]

            # Özellik standardizasyonu
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_val = scaler.transform(X_val)

            # Eğitim süresi
            start_train_time = time.time()
            model.fit(X_train, y_train)
            end_train_time = time.time()

            # Test süresi
            start_test_time = time.time()
            model.predict(X_val)
            end_test_time = time.time()

            # Eğitim ve test sürelerini kaydetme
            fold_train_times.append(end_train_time - start_train_time)
            fold_test_times.append(end_test_time - start_test_time)

        # Ortalama eğitim ve test sürelerini hesapla
        results.append({
            'Model': name,
            'Average Train Time (s)': np.mean(fold_train_times),
            'Average Test Time (s)': np.mean(fold_test_times),
            'Dataset': model_name
        })

    return results

# Tüm veri setleri için eğitim ve test sürelerini alma
train_test_times_pca = evaluate_model(X_pca, y_pca, "PCA")
train_test_times_lda = evaluate_model(X_lda, y_lda, "LDA")
train_test_times_ham = evaluate_model(X_ham, y_ham, "Ham")

# Sonuçları birleştirme
all_train_test_times = pd.DataFrame(train_test_times_pca + train_test_times_lda + train_test_times_ham)

# Sonuçları CSV olarak kaydetme
all_train_test_times.to_csv('/content/drive/MyDrive/ml_train_test_times.csv', index=False)

# Eğitim ve test sürelerini yazdırma
print(all_train_test_times)

import time
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Dropout, Flatten
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import to_categorical

# MLP Modeli
def create_mlp_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# RNN Modeli
def create_rnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=input_shape, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# CNN Modeli
def create_cnn_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# DBN Modeli
def create_dbn_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Autoencoder Modeli
def create_autoencoder(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli Eğitme ve Test Etme
def train_and_predict_kfold(features, labels, model_name, input_dim, input_shape=None, n_splits=5):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    train_times = []
    test_times = []

    for train_index, test_index in skf.split(features, labels):
        X_train, X_test = features[train_index], features[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        y_train = to_categorical(y_train)
        y_test = to_categorical(y_test)

        if model_name == "MLP":
            model = create_mlp_model(input_dim, y_train.shape[1])
        elif model_name == "RNN":
            model = create_rnn_model((X_train.shape[1], 1), y_train.shape[1])
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        elif model_name == "CNN":
            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
            model = create_cnn_model((X_train.shape[1], 1), y_train.shape[1])
        elif model_name == "DBN":
            model = create_dbn_model(input_dim, y_train.shape[1])
        elif model_name == "Autoencoder":
            model = create_autoencoder(input_dim, y_train.shape[1])

        # Eğitim Süresi
        start_train = time.time()
        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)
        end_train = time.time()
        train_times.append(end_train - start_train)

        # Test Süresi
        start_test = time.time()
        model.evaluate(X_test, y_test, verbose=0)
        end_test = time.time()
        test_times.append(end_test - start_test)

    # Ortalama Eğitim ve Test Süreleri
    avg_train_time = np.mean(train_times)
    avg_test_time = np.mean(test_times)

    return avg_train_time, avg_test_time

def main():
    # Veri yolları ve öznitelik çıkarma yöntemleri
    paths = [
        ("/content/drive/MyDrive/data_with_lda_train.csv", "LDA"),
        ("/content/drive/MyDrive/data_with_pca_normalized.csv", "PCA"),
        ("/content/drive/MyDrive/5_data_balanced.csv", "Balanced"),
        ("/content/drive/MyDrive/data_with_lda_test.csv", "LDA Test")
    ]

    # Her model için sürelerin toplanacağı sözlük
    train_times_dict = {model: [] for model in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]}
    test_times_dict = {model: [] for model in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]}

    # Her veri seti ile işlem yapılacak
    for path, method in paths:
        data = pd.read_csv(path)

        # Etiketler ve Özellikler
        labels = data.iloc[:, -1].values
        features = data.iloc[:, :-1].values

        print(f"Veri seti: {path} - Öznitelik Yöntemi: {method}")
        print(f"Etiketlerin ilk 5 değeri: {labels[:5]}")
        print(f"Özelliklerin boyutu: {features.shape}")

        # Her model için eğitim ve test sürelerini toplama
        for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
            avg_train_time, avg_test_time = train_and_predict_kfold(features, labels, model_name, features.shape[1])
            train_times_dict[model_name].append(avg_train_time)
            test_times_dict[model_name].append(avg_test_time)

    # Eğitim ve Test Sürelerinin Yazdırılması
    for model_name in ["MLP", "RNN", "CNN", "DBN", "Autoencoder"]:
        print(f"\n{model_name} Modeli İçin Ortalama Eğitim ve Test Süreleri:")
        for idx, (method, _) in enumerate(paths):
            print(f"  {method} - Eğitim Süresi: {train_times_dict[model_name][idx]:.4f} saniye")
            print(f"  {method} - Test Süresi: {test_times_dict[model_name][idx]:.4f} saniye")

if __name__ == "__main__":
    main()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Flatten, Conv3D
from keras.utils import to_categorical
import time

# 3D CNN+LSTM Modeli
def create_3d_cnn_lstm_model(input_shape, num_classes):
    model = Sequential()
    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape))
    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))
    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same'))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Modeli Eğitme ve Test Etme
def train_and_predict(features, labels, model_name):
    # Eğitim ve test verilerini ayırma
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

    # Veriyi Standartlaştırma
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Kategorik hale getirme
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    # Modelin giriş şekli için reshape işlemi
    time_steps = 1
    n_features = X_train.shape[1]
    X_train = X_train.reshape(X_train.shape[0], time_steps, n_features // time_steps, 1, 1)
    X_test = X_test.reshape(X_test.shape[0], time_steps, n_features // time_steps, 1, 1)

    model = create_3d_cnn_lstm_model((time_steps, X_train.shape[2], 1, 1), y_train.shape[1])

    # Eğitim Süresi
    start_time = time.time()
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)
    training_time = time.time() - start_time

    # Test Süresi
    start_time = time.time()
    y_pred = model.predict(X_test)
    prediction_time = time.time() - start_time

    # Sonuçları döndürme
    return training_time, prediction_time

# Eğitim ve Tahmin Sürelerini Toplama ve Görselleştirme
def summarize_training_times(paths):
    results = []

    for path in paths:
        data = pd.read_csv(path)

        labels = data.iloc[:, -1].values
        features = data.iloc[:, :-1].values

        # Dosya adı üzerinden PCA veya LDA etiketini belirleme
        if 'lda' in path.lower():
            dataset_label = 'LDA'
        elif 'pca' in path.lower():
            dataset_label = 'PCA'
        else:
            dataset_label = 'Original'  # Orijinal veri

        # Modeli orijinal veri ile çalıştır
        train_time, pred_time = train_and_predict(features, labels, "3D_CNN_LSTM")

        # Sonuçları toplama
        results.append({
            "Model": "3D_CNN_LSTM",
            "Dataset": f"{dataset_label} - {path.split('/')[-1]}",
            "Training Time (s)": train_time,
            "Prediction Time (s)": pred_time
        })

    # Sonuçları DataFrame olarak döndürme
    results_df = pd.DataFrame(results)
    return results_df

def main():
    # Veri yolları
    paths = [
        "/content/drive/MyDrive/data_with_lda_train.csv",
        "/content/drive/MyDrive/data_with_lda_test.csv",
        "/content/drive/MyDrive/5_data_balanced.csv",
        "/content/drive/MyDrive/data_with_pca_normalized.csv"
    ]

    # Eğitim ve tahmin sürelerini hesapla
    results_df = summarize_training_times(paths)

    # Sonuçları yazdır
    print(results_df)

    # Sonuçları CSV olarak kaydetme
    results_df.to_csv('/content/drive/MyDrive/model_training_results.csv', index=False)

if __name__ == "__main__":
    main()

"""# MODEL GENELLEMESİ VE K-FOLD GEÇERLEME SONUÇLARI
Hibrit model için PCA-LDA-NO_PCA_LDA dosyalarına uygulanacak her bir fold için değerelndirme sonuçları
> HİBRİT MODEL EĞİTİM VE PERFORMANSI
bölümünde önceden yapılmıştır.

# DOĞRULAMA VERİSİ HAZIRLIĞI
"""

import cv2
import mediapipe as mp
import os
import pandas as pd

# MediaPipe Pose setup
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False)

# Görsel dizinleri (Ana dizin)
image_paths = [
    "/content/drive/MyDrive/validation_veri_seti"  # Görsellerin bulunduğu ana dizin
]

# Çıktı dosyası
output_csv_path = "/content/drive/MyDrive/validation_dataset_skeleton_keypoints.csv"

# Çıktılar için bir liste
all_keypoints = []

# Her görsel dizini için işlemler
for path in image_paths:
    for root, dirs, files in os.walk(path):  # Ana dizin ve alt dizinleri tarar
        for file in files:
            if file.endswith((".jpg", ".jpeg", ".png")):  # Görsel dosyalarını işler
                image_path = os.path.join(root, file)  # Görselin tam yolu
                print(f"Processing image: {image_path}")

                # Görseli oku
                frame = cv2.imread(image_path)

                # BGR'den RGB'ye dönüştür
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # Pose algılama
                results = pose.process(rgb_frame)

                # Eğer poz tespit edilmişse
                if results.pose_landmarks:
                    keypoints = []
                    for lm in results.pose_landmarks.landmark:
                        keypoints.append((lm.x, lm.y, lm.z))
                    all_keypoints.append({
                        "image_path": image_path,
                        "keypoints": keypoints
                    })

# Tüm keypoint verilerini DataFrame'e çevir ve kaydet
df = pd.DataFrame(all_keypoints)
df.to_csv(output_csv_path, index=False)
print(f"Keypoints saved to: {output_csv_path}")

import pandas as pd
import numpy as np
import ast

# CSV dosyasını yükleyelim
file_path = "/content/drive/MyDrive/validation_dataset_skeleton_keypoints.csv"
data = pd.read_csv(file_path)

# 1. 'keypoints' sütunundaki verilerin işlenmesi
def expand_keypoints(row):
    try:
        keypoints = ast.literal_eval(row)  # Keypoints sütununu bir listeye dönüştür
        flat_keypoints = [coord for point in keypoints for coord in point]  # x, y, z'yi birleştir
        return flat_keypoints
    except Exception as e:
        print(f"Hata oluştu: {e} -> {row}")  # Hata mesajını daha ayrıntılı şekilde yazdıralım
        return [np.nan] * (num_keypoints * 3)  # Hata durumunda NaN değerleri döndür

# keypoints sütununun örnek uzunluğunu (keypoint sayısını) bulalım
num_keypoints = len(ast.literal_eval(str(data['keypoints'].iloc[0])))  # ilk satırdan örnek al

# Anahtar noktaları sütunlara ayırmak için yeni sütun isimleri oluşturalım
keypoint_columns = [f"kp{i}_{axis}" for i in range(num_keypoints) for axis in ["x", "y", "z"]]

# 'keypoints' sütununu dönüştürüp yeni sütunlar ekleyelim
expanded_keypoints = data['keypoints'].apply(expand_keypoints)

# Yeni sütunları oluşturup, 'keypoints' sütununu kaldırarak veriyi genişletelim
keypoints_df = pd.DataFrame(expanded_keypoints.tolist(), columns=keypoint_columns)
data_expanded = pd.concat([data.drop(columns=['keypoints']), keypoints_df], axis=1)

# Verinin tamamını kaydedelim
output_full_path = "/content/drive/MyDrive/validation_dataset_skeleton_keypoints_data_expanded_full.csv"
data_expanded.to_csv(output_full_path, index=False)

print(f"Veri başarıyla kaydedildi: {output_full_path}")

import os
import json
import pandas as pd

# Function to parse COCO JSON files and extract relevant data
def parse_coco_json(json_file, folder_path, valid_category_ids):
    data = []
    with open(json_file, 'r') as file:
        annotations = json.load(file)

        # Extract image information
        images = {img['id']: img['file_name'] for img in annotations.get('images', [])}

        # Process annotations
        for annotation in annotations.get('annotations', []):
            image_id = annotation.get('image_id')
            category_id = annotation.get('category_id')
            bbox = annotation.get('bbox')

            # Validate category ID and bbox
            if category_id in valid_category_ids and bbox is not None:
                image_name = images.get(image_id)
                if image_name:
                    image_path = os.path.join(folder_path, image_name)

                    # Check if the image file exists
                    if os.path.exists(image_path):
                        data.append({
                            "image_path": image_path,
                            "category_id": category_id,
                            "bbox": bbox
                        })
                    else:
                        print(f"Warning: Image file {image_path} not found for JSON {json_file}")
    return data

# Define valid category IDs for backhand, forehand, and serve
valid_category_ids = [1, 2, 3]  # Replace with actual IDs

# Define paths to train, valid, and test folders
train_dir = "/content/drive/MyDrive/validation_veri_seti/test"
valid_dir = "/content/drive/MyDrive/validation_veri_seti/valid"
test_dir = "/content/drive/MyDrive/validation_veri_seti/test"

# Parse JSON files from each folder
train_json_file = os.path.join(train_dir, "_annotations.coco.json")
valid_json_file = os.path.join(valid_dir, "_annotations.coco.json")
test_json_file = os.path.join(test_dir, "_annotations.coco.json")

train_data = parse_coco_json(train_json_file, train_dir, valid_category_ids)
valid_data = parse_coco_json(valid_json_file, valid_dir, valid_category_ids)
test_data = parse_coco_json(test_json_file, test_dir, valid_category_ids)

# Convert parsed data to DataFrames
def create_dataframe(data, dataset_type):
    df = pd.DataFrame(data)
    df["dataset_type"] = dataset_type
    return df

train_df = create_dataframe(train_data, "train")
valid_df = create_dataframe(valid_data, "valid")
test_df = create_dataframe(test_data, "test")

# Combine all data into a single DataFrame
combined_df = pd.concat([train_df, valid_df, test_df], ignore_index=True)

# Save the combined dataset to a CSV file
output_csv_path = "/content/drive/MyDrive/test_validprepared_dataset.csv"
combined_df.to_csv(output_csv_path, index=False)

print(f"Dataset prepared and saved at {output_csv_path}")

import os
import json
import pandas as pd

# Function to parse COCO JSON files and extract relevant data
def parse_coco_json(json_file, folder_path, valid_category_ids):
    data = []
    with open(json_file, 'r') as file:
        annotations = json.load(file)

        # Extract image information
        images = {img['id']: img['file_name'] for img in annotations.get('images', [])}

        # Process annotations
        for annotation in annotations.get('annotations', []):
            image_id = annotation.get('image_id')
            category_id = annotation.get('category_id')
            bbox = annotation.get('bbox')

            # Update category ID mapping
            category_mapping = {1: 0, 2: 1, 3: 2}
            if category_id in category_mapping:
                category_id = category_mapping[category_id]

            # Validate category ID and bbox
            if category_id in valid_category_ids and bbox is not None:
                image_name = images.get(image_id)
                if image_name:
                    image_path = os.path.join(folder_path, image_name)

                    # Check if the image file exists
                    if os.path.exists(image_path):
                        data.append({
                            "image_path": image_path,
                            "category_id": category_id,
                            "bbox": bbox
                        })
                    else:
                        print(f"Warning: Image file {image_path} not found for JSON {json_file}")
    return data

# Define valid category IDs for backhand, forehand, and serve
valid_category_ids = [0, 1, 2]  # Updated to reflect new mapping

# Define paths to train, valid, and test folders
train_dir = "/content/drive/MyDrive/validation_veri_seti/test"
valid_dir = "/content/drive/MyDrive/validation_veri_seti/valid"
test_dir = "/content/drive/MyDrive/validation_veri_seti/test"


# Parse JSON files from each folder
train_json_file = os.path.join(train_dir, "_annotations.coco.json")
valid_json_file = os.path.join(valid_dir, "_annotations.coco.json")
test_json_file = os.path.join(test_dir, "_annotations.coco.json")

train_data = parse_coco_json(train_json_file, train_dir, valid_category_ids)
valid_data = parse_coco_json(valid_json_file, valid_dir, valid_category_ids)
test_data = parse_coco_json(test_json_file, test_dir, valid_category_ids)

# Convert parsed data to DataFrames
def create_dataframe(data, dataset_type):
    df = pd.DataFrame(data)
    df["dataset_type"] = dataset_type
    return df

train_df = create_dataframe(train_data, "train")
valid_df = create_dataframe(valid_data, "valid")
test_df = create_dataframe(test_data, "test")

# Combine all data into a single DataFrame
combined_df = pd.concat([train_df, valid_df, test_df], ignore_index=True)

# Save the combined dataset to a CSV file
output_csv_path = "/content/drive/MyDrive/test_validprepared_dataset_labeled.csv"
combined_df.to_csv(output_csv_path, index=False)

print(f"Dataset prepared and saved at {output_csv_path}")

import pandas as pd

# 1. Dosyaları yükleyelim
labeled_data_path = "/content/drive/MyDrive/test_validprepared_dataset_labeled.csv"
keypoints_data_path = "/content/drive/MyDrive/validation_dataset_skeleton_keypoints_data_expanded_full.csv"

labeled_data = pd.read_csv(labeled_data_path)
keypoints_data = pd.read_csv(keypoints_data_path)

# 2. image_path verilerini karşılaştıralım
labeled_image_paths = set(labeled_data['image_path'])
keypoints_image_paths = set(keypoints_data['image_path'])

# 3. Eşleşen image_path'leri bulalım
matching_image_paths = labeled_image_paths.intersection(keypoints_image_paths)

# 4. Eşleşen image_path'leri yazdıralım
print(f"Eşleşen image_path sayısı: {len(matching_image_paths)}")

# 5. Eşleşen image_path'ler için etiketleme işlemi yapalım
merged_data = pd.merge(keypoints_data, labeled_data[['image_path', 'category_id']], on='image_path', how='left')

# 6. Etiketleme işlemi sonrası veriyi kaydedelim
output_file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged.csv"
merged_data.to_csv(output_file_path, index=False)

# 7. Kaydedilen dosyanın içeriğindeki veri sayısını yazdıralım
saved_data = pd.read_csv(output_file_path)
print(f"Kaydedilen dosyada toplam {saved_data.shape[0]} veri bulunuyor.")

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# 1. Dosyayı yükleyelim
file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged.csv"
data = pd.read_csv(file_path)

# 2. Sayısal Veriler Üzerinde Boş Verileri Doldurma
# Sayısal sütunlarda boş verileri medyan ile dolduralım
# 'category_id' ve 'image_path' sütunlarını dışarıda bırakıyoruz
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
numerical_columns = [col for col in numerical_columns if col not in ['category_id', 'image_path']]
data[numerical_columns] = data[numerical_columns].fillna(data[numerical_columns].median())

# 3. Veriyi Ölçeklendirme
# Sayısal sütunları belirleyip, Min-Max ölçeklendirmesi uygulayalım
scaler = MinMaxScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# 4. Tekrarlanan Öğeleri Silme
# Aynı satırların tekrarlayanlarını silelim
data.drop_duplicates(inplace=True)

# 5. category_id ve image_path sütununu dışarıda bırakıp, geri kalan sütunlarla işlemi sürdürdük
output_file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_cleaned_no_balance.csv"
data.to_csv(output_file_path, index=False)

# 6. Kaydedilen dosyanın içeriğindeki veri sayısını yazdıralım
saved_data = pd.read_csv(output_file_path)
print(f"Temizlenmiş dosyada toplam {saved_data.shape[0]} veri bulunuyor.")

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# 1. Dosyayı yükleyelim
file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_cleaned_no_balance.csv"
data = pd.read_csv(file_path)

# 2. Etiketlenmiş ve etiketsiz verileri ayıralım
train_data = data.dropna(subset=['category_id'])  # Etiketlenmiş veriler
test_data = data[data['category_id'].isna()]  # Etiketsiz veriler

# Etiketsiz veri olup olmadığını kontrol edelim
if test_data.empty:
    print("Etiketsiz veri bulunmamaktadır.")
else:
    # Özellikler (features) ve etiketleri ayıralım
    X_train = train_data.drop(columns=['category_id', 'image_path'])  # image_path dışarıda bırakıldı
    y_train = train_data['category_id'].astype(int)  # Kategorik hale getirmek için int türüne dönüştürme

    X_test = test_data.drop(columns=['category_id', 'image_path'])  # image_path dışarıda bırakıldı

    # 3. Modeli başlatalım ve eğitelim
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    # 4. Boş etiketli veriler için tahmin yapalım
    predicted_labels = model.predict(X_test)

    # 5. Test verisindeki boş etiketleri tahminlerle dolduralım
    test_data['category_id'] = predicted_labels

    # 6. Sonuçları birleştirip kaydedelim
    data.loc[data['category_id'].isna(), 'category_id'] = predicted_labels
    output_file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_filled.csv"
    data.to_csv(output_file_path, index=False)

    print(f"Boş etiketler başarıyla tahmin edilip kaydedildi: {output_file_path}")

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import RandomOverSampler

# 1. Dosyayı yükleyelim
file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_filled.csv"
data = pd.read_csv(file_path)

# 2. Sayısal Veriler Üzerinde Boş Verileri Doldurma
# 'category_id' ve 'image_path' sütunlarını dışarıda bırakıyoruz
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
numerical_columns = [col for col in numerical_columns if col not in ['category_id', 'image_path']]
data[numerical_columns] = data[numerical_columns].fillna(data[numerical_columns].median())

# 3. Veriyi Ölçeklendirme
# Sayısal sütunları belirleyip, Min-Max ölçeklendirmesi uygulayalım
scaler = MinMaxScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# 4. Tekrarlanan Öğeleri Silme
# Aynı satırların tekrarlayanlarını silelim
data.drop_duplicates(inplace=True)

# 5. Veriyi Dengeleme (Over-sampling)
# 'category_id' hedef sütunu (label) olarak alıyoruz
X = data.drop(columns=['category_id', 'image_path'])
y = data['category_id']

# RandomOverSampler ile dengeleme işlemi yapıyoruz
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

# Dengeleme sonrası yeni veriyi tekrar birleştiriyoruz
data_resampled = pd.DataFrame(X_resampled, columns=X.columns)
data_resampled['category_id'] = y_resampled
data_resampled['image_path'] = data['image_path'].iloc[:len(data_resampled)].reset_index(drop=True)

# 6. Temizlenmiş ve dengelenmiş veriyi kaydedelim
output_file_path = "/content/drive/MyDrive/validation_dataset_with_labels_merged_cleaned_balanced.csv"
data_resampled.to_csv(output_file_path, index=False)

# 7. Kaydedilen dosyanın içeriğindeki veri sayısını yazdıralım
saved_data = pd.read_csv(output_file_path)
print(f"Dengelenmiş dosyada toplam {saved_data.shape[0]} veri bulunuyor.")

"""# MODEL GENELLEME YETENEĞİ"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Dosyayı yükle
file_path = '/content/drive/MyDrive/results_optimation.csv'
data = pd.read_csv(file_path)

# İlk birkaç satıra göz atalım
data.head(), data.info()

# Grafik boyutlarını ve genel stil ayarlarını belirleyin
import matplotlib.pyplot as plt
import seaborn as sns

# Grafik boyutlarını ve genel stil ayarlarını belirleyin
sns.set_theme(style="whitegrid")
plt.figure(figsize=(16, 10))

# Subplot 1: Epochs'e göre Accuracy ve F1 Score
plt.subplot(2, 2, 1)
sns.lineplot(data=data, x="Epochs", y="Accuracy", label="Accuracy", marker="o", color="blue")
sns.lineplot(data=data, x="Epochs", y="F1 Score", label="F1 Score", marker="o", color="green")
plt.title("Accuracy & F1 Score vs Epochs", fontsize=14)
plt.xlabel("Epochs", fontsize=12)
plt.ylabel("Score", fontsize=12)
plt.legend()
plt.grid(True)

# Subplot 2: Learning Rate'e göre AUC ROC (colorbar düzeltmesiyle)
plt.subplot(2, 2, 2)
scatter = plt.scatter(
    data["Learning Rate"],
    data["AUC ROC"],
    c=data["Batch Size"],
    s=data["Epochs"] * 10,  # Boyutu Epochs ile ilişkilendir
    cmap="viridis",
    edgecolor="k"
)
plt.title("AUC ROC vs Learning Rate", fontsize=14)
plt.xlabel("Learning Rate", fontsize=12)
plt.ylabel("AUC ROC", fontsize=12)
cbar = plt.colorbar(scatter)
cbar.set_label("Batch Size")
plt.grid(True)

# Subplot 3: Sensitivity ve Precision karşılaştırması
plt.subplot(2, 2, 3)
sns.barplot(data=data, x="Batch Size", y="Sensitivity", hue="Epochs", palette="pastel", edgecolor="k")
plt.title("Sensitivity vs Batch Size", fontsize=14)
plt.xlabel("Batch Size", fontsize=12)
plt.ylabel("Sensitivity", fontsize=12)
plt.legend(title="Epochs")
plt.grid(True)

# Subplot 4: Precision ve Accuracy arasındaki ilişki
plt.subplot(2, 2, 4)
sns.regplot(data=data, x="Precision", y="Accuracy", scatter_kws={'color': 'purple'}, line_kws={'color': 'red'})
plt.title("Precision vs Accuracy", fontsize=14)
plt.xlabel("Precision", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)
plt.grid(True)

# Genel başlık ve gösterim
plt.suptitle("Results Optimization Visualization", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Dosyayı yükle
file_path = '/content/drive/MyDrive/results_optimation.csv'
data = pd.read_csv(file_path)

# Genel stil ayarları
sns.set_theme(style="whitegrid")

# 1. Grafik: Epochs'e göre Accuracy ve F1 Score
plt.figure(figsize=(6, 4))
sns.lineplot(data=data, x="Epochs", y="Accuracy", label="Accuracy", marker="o", color="blue")
sns.lineplot(data=data, x="Epochs", y="F1 Score", label="F1 Score", marker="o", color="green")
plt.title("Accuracy & F1 Score vs Epochs")
plt.xlabel("Epochs")
plt.ylabel("Score")
plt.legend()
plt.grid(True)
plt.savefig("accuracy_f1_vs_epochs.png")  # Kaydet
plt.show()

# 2. Grafik: Learning Rate'e göre AUC ROC
plt.figure(figsize=(6, 4))
scatter = plt.scatter(
    data["Learning Rate"],
    data["AUC ROC"],
    c=data["Batch Size"],
    s=data["Epochs"] * 10,
    cmap="viridis",
    edgecolor="k"
)
plt.title("AUC ROC vs Learning Rate")
plt.xlabel("Learning Rate")
plt.ylabel("AUC ROC")
cbar = plt.colorbar(scatter)
cbar.set_label("Batch Size")
plt.grid(True)
plt.savefig("auc_vs_learning_rate.png")  # Kaydet
plt.show()

# 3. Grafik: Sensitivity vs Batch Size
plt.figure(figsize=(6, 4))
sns.barplot(data=data, x="Batch Size", y="Sensitivity", hue="Epochs", palette="pastel", edgecolor="k")
plt.title("Sensitivity vs Batch Size")
plt.xlabel("Batch Size")
plt.ylabel("Sensitivity")
plt.legend(title="Epochs")
plt.grid(True)
plt.savefig("sensitivity_vs_batch.png")  # Kaydet
plt.show()

# 4. Grafik: Precision vs Accuracy
plt.figure(figsize=(6, 4))
sns.regplot(data=data, x="Precision", y="Accuracy", scatter_kws={'color': 'purple'}, line_kws={'color': 'red'})
plt.title("Precision vs Accuracy")
plt.xlabel("Precision")
plt.ylabel("Accuracy")
plt.grid(True)
plt.savefig("precision_vs_accuracy.png")  # Kaydet
plt.show()